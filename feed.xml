<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pascalknoll.github.io/blockpost_flashMD/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pascalknoll.github.io/blockpost_flashMD/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-16T11:32:48+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Trade-off Between Parallel Environments and Steps in PPO</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/ppo-batch-size/" rel="alternate" type="text/html" title="The Trade-off Between Parallel Environments and Steps in PPO"/><published>2026-11-13T00:00:00+00:00</published><updated>2026-11-13T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/ppo-batch-size</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/ppo-batch-size/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>In this post, we are going to explore a common dilemma we face when tuning PPO <d-cite key="schulman2017ppo"></d-cite> hyperparameters: constructing the batch size.</p> <p>It’s easy to think of batch size as just a single number, but in PPO, it is actually the product of two distinct levers we can pull:</p> <ul> <li>The number of parallel environments ($N$).</li> <li>The number of steps collected per environment ($T$).</li> </ul> <p>Does it matter if we reach a batch size of 2,048 by running 2 environments for 1,024 steps, or by running 1,024 environments for 2 steps?</p> <p>Recent work has shown that data collection strategy is not a trivial detail. Multiple studies—including <d-cite key="mayor2025the"></d-cite> and <d-cite key="sapg2024"></d-cite>—highlight that how we distribute experience across environments and rollout lengths can meaningfully influence the stability and effectiveness of on-policy RL methods like PPO. As simulation becomes faster and large-scale parallelism more accessible, understanding these choices is becoming increasingly important.</p> <p>In this post, we’ll dig into why the structure of the batch matters at all. In particular, we’ll look at how increasing $N$ and increasing $T$ affect the bias and variance of PPO’s gradient estimates, theoretically and empirically. Code for the empirical analysis is provided <a href="https://drive.google.com/drive/folders/1z8w_T0Ree9XwfBjK6yvL3zk4SlJZ_flG?usp=sharing">here</a></p> <h3 id="clarifying-terminology-batch-vs-mini-batch">Clarifying Terminology: Batch vs. Mini-Batch</h3> <p>When reading PPO implementations across different libraries, you may notice that the term batch size is used in slightly different ways. To keep things consistent in this post, we’ll use the following terminology as is used in the original PPO paper <d-cite key="schulman2017ppo"></d-cite>:</p> <ul> <li>Rollout Buffer (Total Batch Size): The full dataset collected before a policy update. It is the product of the number of parallel environments ($N$) and the number of steps collected per environment ($T$).</li> </ul> \[\text{Total Batch Size} = N \times T\] <ul> <li>Mini-Batch: A subset of the Rollout Buffer used for a single gradient descent step. The Rollout Buffer is shuffled and divided into these smaller chunks during training.</li> </ul> <p><strong>Source of Confusion.</strong> Different RL libraries sometimes use the word batch to refer to what we’re calling a mini-batch. For example, Stable Baselines3 <d-cite key="stable-baselines3"></d-cite>, Dopamine <d-cite key="castro18dopamine"></d-cite>, OpenAI Baselines (PPO1) <d-cite key="baselines"></d-cite>, and Ray RLlib <d-cite key="liang2018rllib,liang2021rllib"></d-cite> commonly adopt this convention. This can lead to ambiguity, especially when tuning PPO’s data collection parameters.</p> <p>As noted by <d-cite key="shengyi2022the37implementation"></d-cite>:</p> <blockquote style="font-size:0.9em; margin:6px 0; padding-left:12px; border-left:3px solid #ccc;">Some common mis-implementations include (1) always using the whole batch for the update, and (2) implementing mini-batches by randomly fetching from the training data, which does not guarantee all training data points are fetched.</blockquote> <p>Being precise about terminology makes it clear that when we adjust hyperparameters like $N$ and $T$, we’re modifying the amount of experience collected instead of just changing how much data goes into each optimization step (which is determined by the size of each mini-batch).</p> <h3 id="background---ppo-gradient-computation">Background - PPO Gradient Computation</h3> <p>Before diving into bias and variance, let’s write down the gradient PPO computes during policy updates. Ignoring value and entropy terms, the per-sample PPO objective is:</p> \[L_t^{\text{PPO}}(\theta) = -\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t\right),\] <p>with probability ratio:</p> \[r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}.\] <p>During a training epoch, PPO updates the policy using mini-batches sampled from the rollout buffer of size $N \times T$. For a mini-batch $M$, the gradient estimate is:</p> \[G_{mb} = \frac{1}{|M|} \sum_{t \in M} \nabla_\theta L_t^{\text{PPO}}(\theta).\] <p>Because PPO performs multiple mini-batch updates per iteration, the policy parameters $\theta$ change between these updates. As a result, later mini-batches observe a policy that differs from the one that generated the data, meaning $r_t(\theta) \neq 1$ for most mini-batches.</p> <details> <summary><strong>Additional Info on Clipping</strong></summary> The clipping term prevents the update from moving too far from the behavior policy that produced the data, improving stability during multiple gradient steps by ensuring the new policy stays sufficiently close to the old one for the importance-sampling ratio $r_\theta(t)$ to remain reliable. </details> <h3 id="bias-and-variance-due-to-batch-size">Bias and Variance Due to Batch Size</h3> <div style="border-left: 4px solid #335c67; padding: 0.5em 1em; background: #d7cade94;"> <strong>Note:</strong><br/> In this section, we assume that the gradient is computed using the <strong>full batch</strong> of collected data. This allows us to isolate and analyze the inherent sources of bias and variance that arise purely from the sampled batch itself. <br/> As mentioned in previous section, PPO uses <strong>mini-batch</strong> stochastic gradient descent, which introduces additional sources of bias and variance due to sub-sampling. These effects will be discussed in the later section. </div> <p><br/></p> <h4 id="-simplified-gradient-equation"><span style="color:#2ca6a4;">▶</span> Simplified Gradient Equation</h4> <p>If we assume that the gradient is computed using the <strong>full batch</strong> of collected data, we can isolate and analyze the inherent sources of bias and variance that arise purely from the sampled batch itself. Under this assumption, the gradient estimator takes the same form as the standard (vanilla) policy gradient:</p> <div style="border: 1px solid #4a79bc01; padding: 0.2em 0.3em; border-radius: 4px; background: #b1d5bbac;"> $$ G_B = - \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} A^{\theta_{\text{old}}}(s_t, a_t)\, \nabla_\theta \log \pi_\theta(a_t \mid s_t) $$ </div> <p>Here, the policy gradient estimate $G_B$ is simply the average of the per-sample policy-gradient contributions $g_i$.</p> <p>Although the PPO gradient is ultimately derived from the standard (vanilla) policy gradient in a way that supports mini-batch sampling and multiple gradient updates per iteration, it is helpful to “backtrack’’ from the PPO gradient to this full-batch (vanilla) version for clarity.</p> <p>The policy $\pi_{\theta_{old}}$ is the one used to collect the data, while $\pi_\theta$ is the policy being updated. For the very first full-batch gradient computation of an iteration, we have $\pi_\theta = \pi_{\theta_{old}}$, which implies that the probability ratio satisfies $r = 1$. Because of this, the clipping term does not affect the gradient at this stage. The PPO loss therefore reduces to:</p> \[L = -\frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}}_{r} \, A^{\theta_{\text{old}}}(s_t, a_t)\] <p>Taking the gradient of this loss yields:</p> \[G_B = \nabla_\theta L = -\frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \nabla_\theta \left[ r \cdot A^{\theta_{\text{old}}}(s_t, a_t) \right]\] <p>Since the advantage $A$ is constant with respect to the new policy $\theta$, and knowing that $\nabla_\theta r = r \cdot \nabla_\theta \log \pi_\theta(a_t \mid s_t)$, the gradient simplifies to:</p> \[\nabla_\theta L = -\frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} r \, A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\] <p>Since $r=1$ in the full-batch case, the equation becomes:</p> \[G_B = - \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{ A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t) }_{g_i}\] <h4 id="-sources-of-noise-in-gradient-estimation"><span style="color:#2ca6a4;">▶</span> Sources of Noise in Gradient Estimation</h4> <p>Let us define $G_T$ as the ideal gradient we would obtain with infinite data, or full distribution of trajectories. When PPO estimates a policy gradient, it computes ${G_B}$ from a finite set of sampled trajectories rather than the true gradient $G_T$. This estimator ${G_B}$ is noisy, and it is helpful to describe this noise in terms of two main contributions:</p> \[G_B \approx G_T + \color{brown}{\text{sampling noise}} + \color{blue}{\text{advantage estimation noise}},\] <p>where $G_T$ denotes the ideal gradient under the policy’s trajectory distribution.</p> <ul> <li>The first term, $\color{brown}{\text{sampling noise}}$, is the noise that arises from working with a finite number of samples.</li> <li>The second term, $\color{blue}{\text{advantage estimation noise}}$, appears because each gradient $g_i$ is scaled by an advantage estimate $A^{\theta_{\text{old}}}(s_t, a_t)$, which itself is noisy. Since advantage estimation depends on multi-step returns and bootstrapping, two noise sources arise: <ul> <li>$\color{blue}{\text{Trajectory Variability Noise}}$ comes from randomness in environment transitions and policy stochasticity. Even small random differences early in a trajectory can lead to large differences in later outcomes and returns.</li> <li>$\color{blue}{\text{Credit Assignment Noise}}$ reflects how difficult it is to determine which early actions contributed to rewards that appear much later. This makes advantage estimates inherently noisy, especially in sparse or delayed reward environments.</li> </ul> </li> </ul> <p>Overall, these noise sources appear in the gradient estimator in two ways:</p> <ul> <li>Variance: how much ${G_B}$ would fluctuate if we collected a different batch from the <em>same</em> policy.</li> <li>Bias: the systematic deviation between the expected estimate $\mathbb{E}[{G_B}]$ and the true gradient.</li> </ul> <div align="center" style="display:flex; justify-content:center; gap:16px;"> <div style="flex:1; max-width:550px;"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/data_distribution.png" alt="data distribution" style="width:100%; border-radius:6px;"/> <div class="explain-box" style="margin-top:8px;"> <strong>Figure</strong> Policy Update: Updating a policy alters the trajectory distribution. A policy update from iteration <i>i</i> to <i>i+1</i> shifts the probability mass over state–action trajectories, which is what the gradient aims to achieve. </div> </div> </div> <div align="center" style="display:flex; justify-content:center; gap:16px;"> <div style="flex:1; max-width:400px;"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/batch_gradient.png" alt="PPO bias-variance and policy update illustration" style="width:85%; border-radius:6px;"/> <div class="explain-box" style="max-width:700px; margin-top:8px;"> <strong>Figure</strong> Bias and variance: The estimated gradient <i>Ĝ</i> differs from the true gradient <i>G<sub>T</sub></i> due to sampling variability (variance) and systematic estimation error (bias). Both influence how the policy moves from iteration <i>i</i> to <i>i+1</i>. </div> </div> </div> <h4 id="-effect-of-batch-size-on-variance"><span style="color:#2ca6a4;">▶</span> Effect of Batch Size on Variance</h4> <p>The variance of a mean is critical because it tells us how noisy $G_B$ is. High variance means we can’t trust the update.</p> <p>The total variance of our gradient estimate $G_B$ can be mathematically decomposed into two parts:</p> \[\text{Var}(G_B) = \text{Var}\Bigg(\frac{1}{N T} \sum_i g_i \Bigg) = \color{blue}{\underbrace{\frac{1}{(N T)^2} \sum_i \text{Var}(g_i)}_{\text{Term 1: Individual Variance}}} + \color{brown}{\underbrace{\frac{1}{(N T)^2} \sum_{i \neq j} \text{Cov}(g_i, g_j)}_{\text{Term 2: Covariance (Correlation)}}}\] <p>This decomposition connects directly to the noise sources discussed earlier and holds the key to the $N$ vs. $T$ trade-off:</p> <ul> <li> <p>$\color{blue}{\text{Term 1: Individual Variance}}$</p> <p>This term reflects the variability of individual gradient contributions. In practice, this variability arises from both the stochastic policy $\pi_\theta(a_t \mid s_t)$ as well as noise in the advantage estimates, since each $g_i$ is scaled by a noisy advantage value. Importantly, higher individual variance does not necessarily translate into higher variance of the batch-averaged gradient, as gradient cancellation effects and covariance between samples also play a crucial role.</p> </li> <li> <p>$\color{brown}{\text{Term 2: Covariance (The Correlation Problem)}}$</p> <p>This is the term that makes RL different from supervised learning. The covariance $\mathrm{Cov}(g_i, g_j)$ need not be zero because $g_i$ and $g_{j}$ may originate from the same episode. Since states within a single episode are temporally correlated, the policy gradients computed from those states are also correlated.</p> </li> </ul> <p><strong>Impact:</strong><br/> When $T$ is <strong>large</strong>, the batch contains many highly correlated steps, leading to either large positive or large negative covariance term. This effectively reduces the <em>effective sample size</em> of the batch. This $\color{brown}{\text{increases the sampling noise}}$ because the effective sample size has been reduced. Therefore, a long horizon $T$ can keep $\mathrm{Var}(G_B)$ high. In addition, when $T$ is large, the $\color{blue}{\text{individual variance term also increases}}$, since advantage estimation has high variance over long horizons. (The role of $\lambda$ in GAE will be discussed later.)</p> <p>When $T$ is small, advantage estimates have lower variance, and the effective sample size is less severely reduced than in the long-horizon case, since fewer temporally correlated states are included. Consequently, the overall variance of the gradient estimator is much lower.</p> <p><strong>Take-away:</strong><br/> To aggressively reduce variance, we must minimize the covariance term. This requires breaking temporal correlations, which means collecting more independent trajectories—that is, increasing the number of parallel environments, $\mathbf{N}$.</p> <p><strong>Some Pendulum-v1 Environment Experiments:</strong><br/> To investigate whether policy gradients are correlated along a trajectory, we use the Pendulum-v1 gym environment. PPO is trained with <code style="background:#f6f2f8ff; padding:2px 4px; border-radius:4px;">num_envs = 4</code> and <code style="background:#f6f2f8ff; padding:2px 4px; border-radius:4px;">num_steps = 256</code>, which produces a total batch size of <code style="background:#f6f2f8ff; padding:2px 4px; border-radius:4px;">batch_size = 4 × 256 = 1024</code> samples per update.</p> <p>In the pendulum environment, episodes do not terminate due to task completion or failure; instead, they are truncated after a fixed horizon of 200 time steps <d-cite key="towers2024gymnasium"></d-cite>. Consequently, all trajectories have a fixed length of 200 steps.</p> <p>To quantify the correlation between gradients computed from two data points, let us use the cosine similarity between gradients $g_i$ and $g_j$, defined as: \(\cos(g_i, g_j) = \frac{g_i \cdot g_j} {\lVert g_i \rVert \, \lVert g_j \rVert }.\)</p> <p>At a certain training update, we analyze gradient correlation at two levels: For a particular step in training, we compute following:</p> <ul> <li><strong>Within-trajectory correlation</strong>: cosine similarity between gradients $g_t$ and $g_{t+k}$ sampled from the same trajectory at temporal lag $k$.</li> <li><strong>Across-trajectory correlation:</strong> cosine similarity between gradients sampled from two different trajectories, each selected at random. For within trajectory cosine similarity we test it for different values of lag k.</li> </ul> <p>Gradients within the same trajectory are expected to be more strongly correlated, while gradients sampled across trajectories provide a baseline that serves as approximately independent samples.</p> <p>The below figure compares the cosine similarity of gradients within a trajectory to that of gradients across different trajectories at two different points during training.</p> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/gradient_similarity_399360.png" alt="gradient similarity distribution for step# 399360" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Comparison of within-trajectory and across-trajectory gradient cosine similarity at global step 399360 for lag values \(k = 1, 3, 5, 7\). </div> </div> <div style="height:3rem;"></div> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/gradient_similarity_450560.png" alt="gradient similarity distribution for step# 450560" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Comparison of within-trajectory and across-trajectory gradient cosine similarity at global step 450560 for lag values \(k = 1, 5, 10, 40\). </div> </div> <p>The strong correlation we observed within a trajectory suggests that <code style="background:#f6f2f8ff; padding:2px 4px; border-radius:4px;">num_steps</code> play a significant role in gradient variance. To study this effect more systematically, let us analyze two settings: <code class="language-plaintext highlighter-rouge">num_envs = 4</code>, <code class="language-plaintext highlighter-rouge">num_steps = 256</code> (long trajectories) and <code class="language-plaintext highlighter-rouge">num_envs = 16</code>, <code class="language-plaintext highlighter-rouge">num_steps = 64</code> (short trajectories). Let us focus on training step 450560 now, as the correlation effects remain visible even at large lags (i.e. $k$=40).</p> <p>For each setting, we randomly sample five batches from the rollout buffer (<code class="language-plaintext highlighter-rouge">batch0</code> … <code class="language-plaintext highlighter-rouge">batch4</code>). From each batch, we compute the <strong>mean gradient vector</strong>, and then compute pairwise cosine similarities:</p> \[\cos(\bar{g}^{(i)}, \bar{g}^{(j)}) = \frac{\bar{g}^{(i)} \cdot \bar{g}^{(j)}}{\lVert \bar{g}^{(i)} \rVert \, \lVert \bar{g}^{(j)} \rVert }.\] <p>This allows us to visualize whether the mean gradient from one batch tends to point in the same direction as the mean gradient from another batch.</p> <p>Importantly, the differences we observe between batches represent the overall variance of the gradient estimator. This includes both the per-sample variance (noise in each $(g_i)$) and the additional covariance created by temporal correlations within trajectories.</p> <p>In the <strong>long-horizon case (4×256)</strong>, the off-diagonal cells of the cosine similarity heatmap shows that mean gradients from different batches can be strongly <strong>positively</strong> or <strong>negatively</strong> correlated. Negative correlation indicates that the gradient estimate from one batch points in the <em>opposite</em> direction of another batch’s mean gradient, revealing substantial variance in the gradient estimator. In contrast, in the <strong>short-horizon case (16x64)</strong>, while negatively correlated batch mean gradients do occur, they are noticeably less severe than in the long-horizon setting, indicating reduced gradient variance.</p> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/batch_grad_var.png" alt="analysis of variance in gradient for step# 450560" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Cosine similarity between batch mean gradients for short-horizon and long-horizon settings. The long-horizon case (4×256) on the right exhibits stronger positive and negative off-diagonal correlations, suggesting increased variability in gradient estimates. </div> </div> <style>.fixed-row{display:grid;grid-template-columns:repeat(2,1fr);gap:10px;margin-top:1rem;margin-bottom:1rem}.scroll-window{max-height:300px;overflow-y:auto;border:1px solid #ccc;padding:5px;border-radius:6px}.scroll-grid{display:grid;grid-template-columns:repeat(2,1fr);gap:10px}img{width:100%;height:auto}</style> <style>.row-header{font-weight:bold;margin-top:1.2rem;margin-bottom:.3rem;font-size:1.05rem;text-align:left;padding-left:4px;border-left:4px solid #444}</style> <style>.explain-box{border:1px solid #ffffffff;border-left:4px solid #335c67;padding:12px 16px;background:#b1d5bbac;border-radius:6px;margin-top:1.5rem;margin-bottom:1.5rem;font-size:.95rem;line-height:1.45}.explain-box code{background:#eee;padding:2px 4px;border-radius:4px;font-size:.95em}</style> <style>.column-headers{display:grid;grid-template-columns:repeat(2,1fr);margin-top:1.5rem;margin-bottom:.5rem;font-weight:bold;text-align:center;font-size:1.2rem}.header{padding-bottom:4px;border-bottom:2px solid #444}</style> <h4 id="-effect-of-batch-size-on-bias"><span style="color:#2ca6a4;">▶</span> Effect of Batch Size on Bias</h4> <p>Bias describes consistent displacement between the estimated gradient and the true gradient:</p> \[\text{Bias} = \mathbb{E}[{G_B}] - G_T.\] <p>In PPO, bias arises mainly through how returns or advantages are estimated. Generalized Advantage Estimation rely on bootstrapping via Value function estimates, and can introduce bias if the value function is inaccurate, but provides lower variance.</p> <p>The effect of the rollout length $T$ on bias is as follows:</p> <ul> <li> <p>When $T$ is small, advantage estimates rely more heavily on bootstrapping, which increases bias. This shows up in practice as noisier advantage estimates.</p> </li> <li> <p>When $T$ is large, advantage estimates use more actual returns and rely less on the value function, so bias becomes smaller.</p> </li> </ul> <p>Importantly, correlation between samples (the covariance term) affects variance but does not affect bias as it is determined by how advantages are computed. </p> <h3 id="bias-and-variance-due-to-mini-batches">Bias and Variance Due to Mini-batches</h3> <p>Mini-batches are used because the rollout buffer is typically large, and multiple updates per iteration improve computational efficiency. However, mini-batching also introduces sub-sampling variance, since each mini-batch represents only a portion of the available data. Clipped objectives modify the effective update direction, and introduce another form of bias in the gradient estimate.<br/> </p> <p></p> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/mini_batch.png" alt="PPO bias-variance and policy update illustration" style="width:75%; max-width:500px;"/> <div class="explain-box" style="max-width:500px; margin-top:12px;"> <strong>Figure:</strong> Mini-batch gradients: Mini-batch updates compute gradient estimates from subsets of the full batch. These estimates vary across mini-batches and accumulate over an update epoch, introducing sub-sampling variance and policy drift related bias. </div> </div> <h3 id="what-role-does-gae-lambda-play-here">What role does GAE-$\lambda$ play here?</h3> \[A^{\text{GAE}}(\gamma, \lambda)_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}\] <p>where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ and corresponds to the one-step temporal-difference error.</p> <p>For $\lambda = 0$, this reduces to \(A^{\text{GAE}}(\gamma, 0)_t = \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\) which only considers immediate rewards and bootstrapped values. This has low variance (the environment stochasiticity only affects a single step) but can be biased due to the bootstrapping from the value function, which is still being learned.</p> <p>For $\lambda = 1$, it becomes the Monte Carlo return: \(A^{\text{GAE}}(\gamma, 1)_t = \sum_{l=0}^{\infty} \gamma^l \delta_{t+l} = \sum_{l=0}^{\infty} \gamma^l r_{t+l} - V(s_t)\) which incorporates the full sequence of rewards until the end of the episode. This is a accurate (unbiased) estimate of the advantage, as it is based on actual observed returns, opposed to boostrapped values from value estimates. However, it has high variance because the returns can be highly variable, especially in stochastic and sensitive environments.</p> <p>$\lambda$ thus controls how much we rely on immediate rewards vs long-term returns. Now, the advantages are computed for the collected batch of size $NT$, and “how far into the future” is limited by $T$, the rollout steps. When $T$ is much smaller than the episode length (especially for recurrent environments <d-cite key="schulman2017ppo"></d-cite>), the advantage estimates will be truncated, effectively creating the same effect as a lower $\lambda$. For $T &gt; $ episode length, the advantage estimates is unaffected.</p> <p>Based on this reasoning, one might expect the impact of $\lambda$ on gradient variance to depend strongly on the rollout horizon $T$. However, empirical results do not show a clean separation between short-horizon and long-horizon settings. Instead, varying $\lambda$ produces qualitatively similar effects across both regimes.</p> <p>In particular, changing $\lambda$ does not eliminate temporal correlation between gradients sampled along a trajectory. Even for $\lambda = 0$, gradients remain correlated across time due to shared state visitation and common policy. What $\lambda$ primarily influences is directional coherence: as $\lambda$ increases, per-sample gradient directions become more variable, which weakens alignment within a batch.</p> <p>Across both short-horizon and long-horizon settings, the following patterns emerge:</p> <ul> <li> <p>Temporal correlation between gradients persists for all values of $\lambda$.</p> </li> <li> <p>Increasing $\lambda$ increases directional variability in per-sample gradients.</p> </li> <li> <p>As a consequence, batch mean gradients become less directionally aligned, even though individual gradient samples remain temporally correlated.</p> </li> </ul> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/lambda_similarity.png" alt="lambda similarity" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Within-trajectory and across-trajectory cosine similarity for the 4×256 setup. Even for $\lambda = 0$ and $\lambda = 1$, gradients remain correlated at large temporal lags (here, $k = 40$). </div> </div> <div style="height:2rem;"></div> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/lambda_mean_similarity.png" alt="lambda mean similarity" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Cosine similarity between each per-sample gradient and the batch mean gradient, computed as $ \cos(g_i, \bar{g}^{(i)}) = \frac{g_i \cdot \bar{g}^{(i)}}{\lVert g_i \rVert \, \lVert \bar{g}^{(i)} \rVert}. $ As $\lambda$ increases, the distribution becomes more concentrated around zero, indicating increased variability in per-sample gradient directions. </div> </div> <p>Varying the GAE parameter $\lambda$ therefore primarily affects the directional variability of per-sample gradients within a batch rather than removing temporal correlation altogether. For larger values of $\lambda$, gradients tend to point in more diverse directions, leading to partial cancellation when averaged. As a result, batch mean gradients may appear less extreme and, in some cases, more consistently aligned across batches.</p> <p>For smaller values of $\lambda$, per-sample gradients are often more directionally consistent within a batch. This can produce strongly aligned batch mean gradients, which may result in either low or high batch-to-batch variance depending on the rollout and trajectory correlations.</p> <div align="center"> <img src="/blockpost_flashMD//assets/img/2026-04-27-ppo-batch-size/batch_mean_lambda.png" alt="batch mean gradient" style="width:100%; max-width:700px;"/> <div class="explain-box" style="max-width:700px; margin-top:12px;"> <strong>Figure:</strong> Pairwise cosine similarity between batch mean gradients. For $\lambda = 0$, batch-to-batch variance can be either low or high depending on the rollout. For larger $\lambda$, cancellation within batches can reduce extreme batch-to-batch variance. </div> </div> <h3 id="wrapping-up">Wrapping-up</h3> <p>This post examined how the rollout length $T$ and the number of parallel environments $N$ affects the variance of gradient estimates. Longer rollouts introduce strong temporal correlations between gradients computed from neighboring time steps, which can substantially increase variance despite a fixed total batch size. Increasing the number of parallel environments instead reduces these correlations by collecting more independent trajectories.</p> <p>In practice, PPO implementations further mitigate correlation effects by shuffling the rollout buffer and performing updates using mini-batches. This randomization breaks up temporally adjacent samples and partially reduces gradient correlation. Nevertheless, the choice of $N$ and $T$ directly influences the stability of the learning dynamics.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This blog post explores batch size in PPO-what happens when we increase the number of parallel environments versus the number of rollout steps, while keeping the total samples per update fixed. We discuss how this affects bias and variance in gradient estimation.]]></summary></entry><entry><title type="html">Beyond Attention as a Graph</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/beyond-attention-as-graph/" rel="alternate" type="text/html" title="Beyond Attention as a Graph"/><published>2026-04-29T00:00:00+00:00</published><updated>2026-04-29T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/beyond-attention-as-graph</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/beyond-attention-as-graph/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/hero-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/hero-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of increasing sample-efficiency.</p> <p>In my previous blogpost I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as 2-simplicial Attention <d-cite key="roy2025fastsimplex"></d-cite> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the Universal Approximation Theorem guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the curse of dimensionality <d-cite key="poggio2017deepnotshallow_journal"></d-cite>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What Lies Beyond Graphs</h2> <p>As we know, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the Weisfeiler-Lehman test <d-cite key="huang2022wl"></d-cite> (also referred to as the WL-test). I won’t go in the details of what the test is, and will gladly refer the interested reader to Federico Barbero’s excellent video explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is at most as expressive at graph isomorphism as the WL-test itself <d-cite key="xu2018powerful"></d-cite>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = {1, 2, \cdots, n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where</p> \[\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\] <p>Intuitively, this represents <em>$k$-sized tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, it can be shown <d-cite key="bodnar2021weisfeiler"></d-cite> that the networks with <strong>order $k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like Simplicial Complexes, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h2 id="2-simplicial-attention">2-Simplicial Attention</h2> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to the 2019 work by Clift et al. <d-cite key="clift2020simplicialtransformer"></d-cite>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as Representational Strengths and Limitations of Transformers <d-cite key="sanford2023representational"></d-cite>, Tensor attention <d-cite key="liang2024tensorattentiontraining"></d-cite>, The Cellular Transformer <d-cite key="ballester2024cellulartransformer"></d-cite>, AlphaFold 2 <d-cite key="jumper2021alphafold"></d-cite>, and TransNAR <d-cite key="bounsi2024transformersmeetnar"></d-cite>. Even I, since last year, have been obsessed with the idea, proposing it in public a couple of times.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, Aurko, Rohan and their colleagues delivered well beyond that: a novel implementation <d-cite key="roy2025fastsimplex"></d-cite> of a Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 1: Scaling law results from the Fast and Simplex: 2-Simplicial Attention in Triton paper. </div> <h3 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h3> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 2: An attention matrix encodes a graph. </div> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 3: Left: central node (i) weighs via attention neighboring nodes; Right: central node aggregates via attention weights the value function defined on neighboring nodes. </div> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K’$ matrix, resulting from a $W_{K’}$ projection, such that we would have a 3D tensor</p> \[T_{ijk} = \sum_{l} Q_{il} K_{jl} K'_{kl}\] <p>Which can also be seen as taking a multilinear product, if viewed per query:</p> \[T_{ijk} = \langle q_i, k_j, k'_s \rangle\] <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.</p> <p>Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 4: 2-simplicial attention's tensor T, in each of its entries, represents a (directed) 2-simplex (triangle). </div> <p>Now that we’ve found a formulation to represent 2-simplices (or simplexes, one day I’ll have to decide which version of the plural I prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attention (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes:</p> \[\alpha^{(2)}_{ijk} = \text{softmax}(T)^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk} e^{T_{ijk}}}\] <p>Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structures: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 5: Left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node's representation. </div> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V’$, that we use analogously to $K’$.</p> <p>What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 6: v and v' from each triangle are aggregated and used to update the central node's embedding. </div> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n \times d}$ shape:</p> \[\text{attention}(x)_{il} = \sum_{jk} \frac{\alpha^{(2)}_{ijk} V^{(2)}_{jkl}}{\sqrt{d}}\] <p>Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial:</p> <p>For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i, j_1, \ldots, j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1 \cdots j_n} = \sum_{\ell} Q_{i\ell} \prod_{m=1}^n K^{(m)}_{j_m \ell} = \langle q_i, k^{(1)}_{j_1}, \ldots, k^{(n)}_{j_n} \rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1 \cdots j_n} = \frac{\exp T_{i\,j_1 \cdots j_n}}{\sum_{(j_1, \ldots, j_n)} \exp T_{i\,j_1 \cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1 \cdots j_n} = \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$:</p> \[\text{attn}(X)_{i\ell} = \frac{1}{\sqrt{d}} \sum_{j_1, \ldots, j_n} \alpha^{(n)}_{i\,j_1 \cdots j_n} \left[V^{(n)}_{j_1 \cdots j_n}\right]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of sparsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When I first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with DeepSeek 3.2 <d-cite key="deepseek2025v32exp"></d-cite>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{ijh} = \text{ReLU}(\langle q^I_{ih}, k^I_{j} \rangle) \cdot w^I_{ih},\] <p>where $q^I_{ih}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{ih}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} = \sum_{h=1}^{H_I} s_{ijh}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i = \text{TopK}_j(S_{ij}, k),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 7: Intuitively representing full attention, SWA and DSA in the regular case. </div> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 8: Losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention. Total token horizon is of around 60M tokens. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/speedup-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/speedup-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 9: speedup across steps of DSA and SWA vs baseline </div> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 9: Reported performance comparison between transformers and 2-simplicial attention in the original paper. </div> <h3 id="what-does-n-simplicial-attention-mean-for-depth">What Does n-Simplicial Attention Mean for Depth</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity.]]></summary></entry><entry><title type="html">A Hitchhiker’s Guide to Agent Evaluation</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/agent-evaluation/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Agent Evaluation"/><published>2026-04-28T00:00:00+00:00</published><updated>2026-04-28T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/agent-evaluation</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/agent-evaluation/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As Large Language Models (LLMs) evolve from standalone text generators into <strong>autonomous agents</strong> capable of taking actions in the real world, the way we evaluate them must fundamentally change. Traditional benchmarks that measure text quality or accuracy are no longer sufficient. We need evaluation frameworks that assess whether agents can perform multi-step tasks in dynamic environments to achieve goals in a reliable and safe way.</p> <p>This blog provides a hitchhiker’s guide to the emerging field of agent evaluation. It begins by detailing the key distinctions from traditional LLM evaluation, and then describes how these differences affect evaluation solutions. We organize the paper around the main questions that can allow easy entrance to newcomers to the field.</p> <hr/> <h2 id="how-do-llm-and-agent-evaluation-differ">How Do LLM and Agent Evaluation Differ?</h2> <p>The shift from LLMs to agents introduces three fundamental changes in evaluation philosophy:</p> <h3 id="single-step-vs-multi-step">Single-step vs. Multi-step</h3> <p>LLM benchmarks mostly assess one-step tasks. Agents handle <strong>long-horizon tasks</strong> requiring planning and multiple steps. For example, the SWE-bench coding tasks require editing multiple functions and files to fix a bug, going far beyond single-line code generation <d-cite key="jimenez2023swebench"></d-cite>. Similarly, τ-bench tasks involve multi-turn dialogues with tools and database queries, which standard LLM evaluation would not capture <d-cite key="yao2024tbench"></d-cite>.</p> <blockquote> To make an analogy, LLM evaluation is like examining the performance of an engine. In contrast, agent evaluation assesses a car’s performance comprehensively, as well as under various driving conditions. </blockquote> <h3 id="output-vs-outcome">Output vs. Outcome</h3> <p>Traditional LLM evaluation focuses on <strong>text-generation quality</strong>, accuracy on a benchmark, likelihood scores, or fluency metrics. Agent evaluation, by contrast, focuses on <strong>task completion</strong>. We care about whether a goal is achieved (e.g., a flight booked, GitHub issue solved), not just the plausibility of generated text <d-cite key="mohammadi2025survey"></d-cite>.</p> <h3 id="passive-vs-interactive">Passive vs. Interactive</h3> <p>Agents operate in <strong>dynamic environments</strong>, interacting with users or APIs. This means evaluation must account for interactivity and adherence to external rules. For instance, τ-bench highlights that an agent must gather user information, call backend APIs, and follow domain-specific policy rules during a conversation. Safety also becomes critical: unlike pure LLM tasks, an agent evaluation must check for policy compliance and avoidance of unsafe actions (e.g. deleting the code base) <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph LLM["Traditional LLM Evaluation"]
        A[Input Prompt] --&gt; B[Text Output]
        B --&gt; C[Quality Metrics]
    end
    
    subgraph Agent["Agent Evaluation"]
        D[Goal/Task] --&gt; E[Multi-step Actions]
        E --&gt; F[Environment Interaction]
        F --&gt; G[Outcome Assessment]
        G --&gt; H[Safety &amp; Policy Check]
    end
</code></pre> <hr/> <h2 id="what-does-agent-evaluation-actually-measure">What Does Agent Evaluation Actually Measure?</h2> <h3 id="model-vs-system-performance">Model vs. System Performance</h3> <p>When evaluating a fixed agent framework, performance reflects the underlying LLM’s capability (e.g., tool-calling accuracy, reasoning). In this case, we are effectively measuring the model’s problem-solving ability on multi-step agentic tasks.</p> <p>By contrast, when comparing different agent architectures or “scaffolds,” the evaluation measures the full agent architecture. Leaderboards like the ones for SWE-bench and AppWorld have more focus on evaluating the different scaffolds and not only the model itself. More broadly, the <strong>Holistic Agent Leaderboard (HAL)</strong> conducts standardized trials across many tasks and frameworks to isolate architectural effects <d-cite key="kapoor2025hal"></d-cite>. For example, HAL ran 21,730 rollouts over 9 LLMs, 9 different agent “scaffolds,” and multiple benchmarks (coding, web navigation, etc.), revealing how agent design (e.g., planning algorithm, memory use) affects success.</p> <h3 id="primary-metrics">Primary Metrics</h3> <p>Most agent benchmarks report <strong>success rates</strong> or <strong>task completion percentages</strong> as the main metric (analogous to accuracy). These metrics test whether the agent was able to achieve the task, but other auxiliary measures are needed for evaluating the multi-step agent actions (trajectory) quality and efficiency of the agent, as pointed out by the AI-Agent that matters paper. Auxiliary measures include:</p> <table> <thead> <tr> <th>Metric Type</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>Primary</td> <td>Success rate, task completion %</td> </tr> <tr> <td>Efficiency</td> <td>Latency, token cost, number of steps</td> </tr> <tr> <td>Partial Credit</td> <td>Subtask completion, milestone-based accuracy</td> </tr> <tr> <td>Trajectory Quality</td> <td>Action sequence correctness, tool usage accuracy</td> </tr> </tbody> </table> <p>Other metrics from traditional NLP (perplexity, F1) are rarely appropriate for agents because the “text output” is just one small part of the process. Instead, agent evaluation often includes metrics for action sequences, tool usage, and end-state correctness. This change reflects a higher focus on semantic evaluation than on syntactic one.</p> <hr/> <h2 id="how-to-evaluate-agent-reliability-and-safety">How to Evaluate Agent Reliability and Safety?</h2> <p>Beyond raw performance, agents must demonstrate <strong>reliability</strong> and <strong>safety</strong>. This section covers three critical dimensions.</p> <h3 id="consistency-metrics">Consistency Metrics</h3> <p>Because LLM agents are nondeterministic, it is not sufficient to only measure the agent success rate; we also need to measure how reliably an agent performs a task over multiple runs. Common metrics are <strong>pass@k</strong> and <strong>pass^k</strong> rates <d-cite key="yao2024tbench"></d-cite>:</p> \[\text{pass@}k = \text{Success in one of } k \text{ attempts}\] \[\text{pass}^k = \text{Success in all } k \text{ attempts}\] <p>Where:</p> <ul> <li>Success is defined by the task completion metric</li> <li>$k$ is the number of attempts</li> <li><strong>pass@k</strong> represents the agent’s ability to succeed <em>at least once</em> in $k$ attempts</li> <li><strong>pass^k</strong> represents the agent’s ability to succeed on <em>every one</em> of $k$ trials</li> </ul> <d-footnote>The pass@k metric is useful for scenarios where you can retry, while pass^k is crucial for production systems where consistent performance is required.</d-footnote> <p>For example, τ-bench explicitly introduced pass^k to quantify agent consistency. In practice, modern agents often have high pass@1 but rapidly falling pass^k. Yao et al. report that <strong>GPT-4’s success on τ-bench drops from ~61% (pass@1) to only ~25% for pass^8</strong>, underscoring that a good agent must not only succeed sometimes, but succeed consistently.</p> <h3 id="policy-adherence">Policy Adherence</h3> <p>In interactive or enterprise settings, agents must obey rules or policies. Benchmarks now include safety constraints as part of the task. For instance, <strong>ST-WebAgentBench</strong> explicitly provides a hierarchy of organizational policies and measures whether the agent completes the task under those policies <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <p>A proposed metric is <strong>Completion under Policy (CuP)</strong>, which gives credit only if no policy is violated. Studies find that state-of-the-art agents often fail on these criteria—for example, many succeed at completing a web task but ignore critical safety rules. These metrics are especially important for high-risk organizational agents.</p> <h3 id="adversarial-safety-tests">Adversarial Safety Tests</h3> <p>Additional evaluations probe harmful or unsafe behaviors by design. For example, the <strong>CoSafe benchmark</strong> feeds agents adversarial prompts (e.g., requests for illicit instructions) and measures the rate of unsafe completions <d-cite key="pan2024cosafe"></d-cite>. Other tests like <strong>AgentHarm</strong> quantify the agent’s tendency to produce disallowed content.</p> <p>In practice, one might report the percentage of trials in which the agent violates a safety rule (akin to a “failure rate” under adversarial stress). These measures complement success metrics, ensuring an agent is not only effective but also aligned with ethical and safety standards.</p> <hr/> <h2 id="how-to-evaluate-agent-trajectories">How to Evaluate Agent Trajectories?</h2> <p>Beyond final outcomes, understanding <em>how</em> an agent arrives at its solution is crucial. This section covers trajectory-level evaluation.</p> <h3 id="milestones-and-subgoals">Milestones and Subgoals</h3> <p>Many agent tasks are naturally hierarchical. Benchmarks often define intermediate checkpoints or key subgoals along the trajectory. For instance, <strong>TheAgentCompany</strong> benchmark explicitly designs tasks that require many consecutive steps and provides partial credit for completing subtasks <d-cite key="xu2024theagentcompany"></d-cite>. Likewise, <strong>WebCanvas</strong> measures success rates at “key nodes” in the workflow <d-cite key="zhou2024webcanvas"></d-cite>.</p> <p>By breaking a task into milestones, evaluators can compute metrics like:</p> <ul> <li>Fraction of subtasks achieved</li> <li>Milestone-based accuracy</li> <li>Progress score (even for failed tasks)</li> </ul> <p>This provides a finer-grained view of progress than a single binary outcome.</p> <h3 id="agent-as-a-judge">Agent-as-a-Judge</h3> <p>A recent trend is to use LLMs (or agents) themselves to evaluate trajectories. The <strong>LLM-as-a-Judge</strong> paradigm employs a large model to score or critique an agent’s multi-step output <d-cite key="zhuge2024agentjudge"></d-cite>.</p> <p>For example, Zhuge et al. (2024) propose an <strong>Agent-as-a-Judge</strong> framework: multiple AI agents read an execution trace and vote on success. Such approaches can automatically assess factors like:</p> <ul> <li>Logical consistency</li> <li>Goal alignment</li> <li>Efficiency of the solution path</li> </ul> <p>They remain experimental, but they show promise for scalable, subjective evaluations.</p> <h3 id="tool-call-analysis">Tool-Call Analysis</h3> <p>Since agent interaction with its environment is based on tool calling, evaluating the sequence of tool invocations is critical. Key questions are:</p> <ul> <li>Did the agent call the <strong>right tools</strong>?</li> <li>Were they called in the <strong>right order</strong>?</li> </ul> <pre><code class="language-mermaid">flowchart TD
    A[Agent Action] --&gt; B{Tool Call?}
    B --&gt;|Yes| C[Invocation Accuracy]
    B --&gt;|No| D[Text Response]
    C --&gt; E[Tool Selection Accuracy]
    E --&gt; F[Sequence Analysis]
    F --&gt; G[Graph-based Metrics]
    G --&gt; H[Node F1: Correct tools]
    G --&gt; I[Edge F1: Correct ordering]
    G --&gt; J[Edit Distance: Path similarity]
</code></pre> <p>Metrics include <d-cite key="mohammadi2025survey"></d-cite>:</p> <table> <thead> <tr> <th>Metric</th> <th>What it Measures</th> </tr> </thead> <tbody> <tr> <td>Invocation Accuracy</td> <td>Was a tool call needed at each step?</td> </tr> <tr> <td>Tool Selection Accuracy</td> <td>Was the correct tool chosen?</td> </tr> <tr> <td>MRR/NDCG</td> <td>Ranking quality of tool selection</td> </tr> <tr> <td>Node F1</td> <td>Correct tools chosen (graph-based)</td> </tr> <tr> <td>Edge F1</td> <td>Correct ordering of tools</td> </tr> <tr> <td>Normalized Edit Distance</td> <td>Similarity to reference trajectory</td> </tr> </tbody> </table> <p>In addition, <strong>execution-based evaluation</strong> runs the tool calls to verify they produce the right output. For instance, <strong>GorillaBench</strong> executes each proposed function call to verify it produces the right output <d-cite key="patil2023gorilla"></d-cite>. Similarly, τ-bench applies the tools to measure if the agent achieves the desired database state.</p> <hr/> <h2 id="what-are-the-big-open-problems--research-questions-in-agent-evaluation">What Are the Big Open Problems &amp; Research Questions in Agent Evaluation?</h2> <p>Despite rapid progress, several fundamental challenges remain in agent evaluatio<d-cite key="yehudai2025survey"></d-cite>.</p> <h3 id="scalability">Scalability</h3> <p>Current agent evaluations are <strong>resource-intensive</strong>. Running complex tasks with many trials (especially using large models) can cost thousands of dollars. HAL’s evaluation harness reduced wall-clock time, but still required 21,730 agent rollouts across 9 benchmarks at a cost of about <strong>$40,000</strong> <d-cite key="kapoor2025hal"></d-cite>. Moreover, creating evaluation data is time and cost intensive, making it hard to evaluate agents on new domains.</p> <p>Future work must improve:</p> <ul> <li>Efficient evaluation: achieving comparable evaluation signal with fewer resources</li> <li>Automation: automate synthetic benchmark creation with LLM-based pipelines</li> </ul> <h3 id="cost-efficiency">Cost-Efficiency</h3> <p>As model inference is expensive, a key question is how to balance performance against computational cost. HAL, for example, emphasizes <strong>Pareto frontiers of accuracy vs. inference cost</strong> <d-cite key="kapoor2025hal"></d-cite>.</p> <p>Such multi-objective evaluation is still nascent: we need standardized ways to report cost (token usage, latency, cloud bills) alongside success rates. Without this, improvements may hide astronomical costs.</p> <h3 id="long-term-autonomy">Long-term Autonomy</h3> <p>Evaluating agents over extended interactions remains challenging. Most benchmarks cover <strong>minutes-long tasks</strong>; long-term autonomy would involve days or continuous deployment.</p> <p>Some recent efforts study simulated multi-day environments, or tasks with long-horizon goals. However, metrics for tracking sustained goal achievement or adaptation over time are still underdeveloped. How do we measure an agent’s ability to pursue a goal if the task evolves over hours or days? This “life-long” evaluation is an open frontier.</p> <h3 id="generalist-agents">Generalist Agents</h3> <p>Many benchmarks focus on narrow domains, but we aspire to agents that generalize across tasks and environments. Evaluating such <strong>generalist agents</strong> requires broad, heterogeneous test suites.</p> <p>TheAgentCompany attempted this by mixing coding, management, and finance tasks; even so, the best agent solved only <strong>~24% of tasks</strong> <d-cite key="xu2024theagentcompany"></d-cite>, highlighting the difficulty.</p> <p>Open questions include:</p> <ul> <li>How to aggregate performance across diverse tasks (weighted averages, worst-case)?</li> <li>How to design benchmarks that fairly test versatility?</li> <li>How to maintain agent and environment generality when producing a general agent evaluation framework?</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Agent evaluation is at an exciting inflection point. As LLMs become autonomous actors in the world, we need evaluation paradigms that go beyond text quality to assess:</p> <ol> <li><strong>Task completion</strong> over long horizons</li> <li><strong>Safety and policy compliance</strong> in interactive settings</li> <li><strong>Consistency and reliability</strong> across multiple runs</li> <li><strong>Trajectory quality</strong> including tool usage and intermediate steps</li> <li><strong>Cost-efficiency</strong> and scalability of the evaluation itself</li> </ol> <p>The benchmarks and metrics described here—from SWE-bench and τ-bench to HAL and ST-WebAgentBench—represent important first steps. But as agents become more capable and are deployed in higher-stakes domains, the evaluation frameworks must continue to evolve.</p> <p>For practitioners, the key takeaway is: <strong>don’t just measure if your agent works, measure if it works safely, consistently, and efficiently</strong>. For researchers, the open problems in scalability, long-term autonomy, and generalist evaluation offer rich opportunities for contribution.</p> <p>The hitchhiker’s guide to agent evaluation is still being written—and there’s plenty of galaxy left to explore.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[An introductory guide to LLM-based agents' evaluation. We explore what makes agent evaluation different from traditional LLM benchmarks, how to measure success, safety, and trajectory quality, and highlight open challenges in the field.]]></summary></entry><entry><title type="html">Attention Sinks from the Graph Perspective</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/attention-sinks-graph-perspective/" rel="alternate" type="text/html" title="Attention Sinks from the Graph Perspective"/><published>2026-04-28T00:00:00+00:00</published><updated>2026-04-28T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/attention-sinks-graph-perspective</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/attention-sinks-graph-perspective/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like Softpick <d-cite key="softpick2025"></d-cite>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at a first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias to the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as Message-Passing</h2> <p>Recent work by Chaitanya K. Joshi <d-cite key="joshi2025"></d-cite> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the original Transformer paper <d-cite key="vaswani2017attention"></d-cite>. Despite this being generally a good practice in my opinion, it generally directs attention to being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither being wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multiheaded attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$, we first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\text{attention}(X) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Let’s take a look at $\alpha = QK^T$.</p> <p>If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{l=1}^{d_k} Q_{il}(K^T)_{lj} = \sum_{l=1}^{d_k} Q_{il}K_{jl}\] <p>and if we note that $Q$ and $K$’s rows, respectively $q_i$ and $k_i$, we see that</p> \[\alpha_{ij} = q_i k_j^T = \langle q_i, k_j \rangle\] <p>The attention matrix $\alpha$’s entries are thus simply speaking the euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective?</p> <p>Let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\text{attention}(x)_i = \sum_l A_{il} V_l\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V, E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$.</p> <p>Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_{j}\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix’ entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of Transformer models employ attention bidirectionally, LLMs, our Large Model protagonists, are usually causally masking attention to leverage parallelism for their Next Token Prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (<em>DAG</em>), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1-4: We simulate an attention matrix being composed with itself several times, across three different configurations: bidirectional, masked pre-softmax, softmax. As we can see, the combination of masking (making the matrix nilpotent) and softmax (forcing row-wise mass to sum to one) rapidly recovers the familiar attention pattern we see for attention sinks. </div> <p>These plots (Fig.1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \left(\prod_{\ell=1}^{L}(I + A^{\ell} B^{\ell})\right) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\top} {A^{\ell+1}}^{\top}) \cdots (I + {B^{L}}^{\top} {A^{L}}^{\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for LLaMA-2 <d-cite key="xiao2023streamingllm"></d-cite>, or in Sun et al. <d-cite key="sun2024massive"></d-cite> and Cancedda et al. <d-cite key="cancedda-2024-spectral"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Attention patterns across layers showing the accumulation effect in later layers. </div> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>So, to recap, what does this mean? We individuated a possible mechanism that may bias Causal Transformers to accumulate attention on its first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon.]]></summary></entry><entry><title type="html">Square Peg, Round Hole: Plugging Non-Sequential Data into Sequential Language Models</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/autoregressive-tokenization/" rel="alternate" type="text/html" title="Square Peg, Round Hole: Plugging Non-Sequential Data into Sequential Language Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/autoregressive-tokenization</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/autoregressive-tokenization/"><![CDATA[<div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 300px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/square_peg-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/square_peg-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/square_peg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/square_peg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="introduction">Introduction</h1> <p>Autoregressive sequence models sit at the center of modern generative AI, excelling in settings like natural language where data arrive in a well-defined sequence. However, many important modalities do not immediately offer such a linear structure. Images, graphs, point clouds, and sets lack an intrinsic notion of “the next token.”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/intro-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/intro-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/intro-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/intro.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> How can one apply sequential models to non-sequential (e.g. non-language) data? (Note that we will use “autoregressive model” and “sequential model” interchangeably.) </div> <p>Despite this apparent mismatch between modeling assumption and data structure, autoregressive (AR) models have been repeatedly applied in such non-lingual settings <d-cite key="Antunes2024,10.1609/aaai.v39i24.34804,sun2024autoregressivemodelbeatsdiffusion"></d-cite>. There are good reasons: AR models offer variable-length generation, precise likelihoods, flexible conditioning, and step-by-step controllability <d-cite key="wang2024diverse,chen2024diffusion"></d-cite>. Moreover, from a practical perspective, autoregressive models have been engineered and scaled to perfection, with well-established scaling laws, training recipes, and ready-to-use open source libraries.</p> <p>This blog post explores the emerging landscape of techniques for turning non-sequential data into discrete 1D sequences, which autoregressive models can effectively process. It is intended for a diverse audience, including anyone who wishes to design machine learning systems for non-sequential data (images, molecules, point clouds, etc).</p> <p>We start with a primer on autoregressive modeling, including tokenization and positional encodings. Readers familiar with these concepts already should skip ahead to the following <a href="#what-exactly-are-tokens">section</a>, which defines “non-sequential data”. We then categorize recent research into two distinct kinds of approaches: <strong>model-level methods</strong>, which optimize the generation order for a fixed set of tokens, and <strong>tokenization-level methods</strong>, which redesign the discrete input representation itself to align with a sequential prior. In the case of tokenization-level methods, we highlight the inherent tradeoff between compressibility and modelability. Although these methods often originate in different communities and target different modalities, they are instances of the same underlying challenge. Our aim is to draw out these connections, map the shared structure across approaches, and sketch a broader landscape of possibilities for modeling non-sequential data with sequential architectures.</p> <h1 id="primer-autoregressive-modeling">Primer: Autoregressive Modeling</h1> <p>From the early days of recursive neural networks to the current transformer revolution, <strong>Autoregressive Models (ARMs)</strong> have emerged as a central paradigm for sequence-based generative modeling. By treating data as a series of discrete tokens (drawn from some finite vocabulary) and modeling their joint distribution through next-token prediction, machine learning systems achieve strong performance on tasks ranging from fluent text generation to complex program synthesis <d-cite key="chen2021evaluatinglargelanguagemodels,openai2024gpt4technicalreport"></d-cite>.</p> <p>A central assumption behind ARMs is that data can be meaningfully factorized into a sequence of tokens, \(x = (x_1,\dots,x_n)\) where each token depends on those who came before it. Although <strong>any</strong> sequence of data can be factorized as</p> \[p(x_1,\dots,x_n)=p(x_1)p(x_2\mid x_1)p(x_3 \mid x_1,x_2)\dots\] <p>via the chain rule, by “meaningfully” we refer to how easy it is to model each of the individual factors \(p(x_i \mid x_1,\dots,x_{i-1})\) — more on this in the next section.</p> <p>Under this factorization, the model is trained to predict the next token $x_i$ given the context of preceding tokens $x_1,\dots,x_{i-1}$. In transformers, this is implemented via causal masking, where the self-attention mechanism prevents any position from attending to “future” tokens.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Autoregressive models predict next-token distributions over tokens based only on the preceding tokens. </div> </div> </div> <h2 id="what-exactly-are-tokens">What exactly are tokens?</h2> <p>So, an autoregressive model operates on a sequence of discrete tokens representing a piece of data. But what exactly are tokens, and how are they computed? At first, tokens might seem unnecessary. For example, one could simply input the raw byte sequence (e.g. UTF-8 values for text) into an autoregressive model. However, byte sequences can become very long, making long-range dependencies harder to learn and obscuring meaningful linguistic structure that the model could otherwise exploit. As a result, byte-level models often require more computation and struggle to match the efficiency and performance of systems that use more semantically informed units <d-cite key="10.1609/aaai.v33i01.33013159"></d-cite>.</p> <p>This motivated the use of <strong>tokenization</strong>: the process of mapping raw data into a sequence of discrete symbols drawn from a finite vocabulary<sup id="fnref:soft"><a href="#fn:soft" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. For text data, the most commonly used tokenization schemes such as Byte-Pair Encoding (BPE) <d-cite key="sennrich-etal-2016-neural"></d-cite>, WordPiece <d-cite key="devlin-etal-2019-bert"></d-cite>, or unigram tokenization segment strings into subword units:</p> <ul> <li> <p>“tokenization” → [‘token’, ‘ization’]</p> </li> <li> <p>“codebook” → [‘code’, ‘book’]</p> </li> </ul> <p>Subword methods strike a balance between vocabulary size and sequence length, whereas byte-level tokenization uses a near-minimal vocabulary that results in no information loss but produces considerably longer sequences. These choices reflect a fundamental tension in tokenizer design: <strong>a tokenizer optimized purely for compression (i.e. the best reconstruction per bit budget) is not necessarily the one that is easiest for a generative model to predict</strong> (as evidenced by e.g. <d-cite key="lester2024training"></d-cite>, who demonstrate that a naive compression-based tokenizer works poorly for language modeling). Later sections will revisit this reconstruction-generation tradeoff from multiple angles.</p> <p>Each token is moreover associated with a positional encoding, which encodes that token’s position in the sequence and breaks the native permutational invariance of the attention mechanism. However, even without positional encodings, the causal attention mask of autoregressive models still forces token generation to occur in a specific order. Thus, simply removing positional encodings does not fundamentally change the sequential nature of causal attention, as works like NoPos <d-cite key="haviv-etal-2022-transformer"></d-cite> have demonstrated.</p> <h2 id="tokenization-free-methods">Tokenization-free methods</h2> <p>A few recent “tokenization-free” approaches have moved away from the tokenization paradigm, which suffers from various idiosyncrasies and challenges for multilingual data <d-cite key="neitemeier2025hierarchical"></d-cite>. Instead of committing to a predefined vocabulary or a fixed sequence structure, approaches such as the Byte Latent Transformer <d-cite key="pagnoni-etal-2025-byte"></d-cite> and H-Net <d-cite key="hwang2025dynamicchunkingendtoendhierarchical"></d-cite> let the representational units evolve during generation. If the model can decide how to construct these building blocks as it trains, then the tokenization becomes an emergent property of the model’s inference dynamics, rather than something constructed ahead of time. While promising as methods for transcending hand-designed, modality-specific tokenizations, both of these models <strong>remain autoregressive</strong>. In other words, they both work with fixed sequences of bytes. Thus, although we will focus on the more widespread tokenization paradigm in the rest of this blog post, the mismatch between sequential models and non-sequential data prevails for tokenization-free methods, too.</p> <h2 id="advantages">Advantages</h2> <p>Autoregressive models have several useful properties that make them appealing across a wide range of generative settings <d-cite key="chen2024diffusion"></d-cite>:</p> <ul> <li><strong>Variable-length generation</strong>: The model can decide dynamically when to stop generating, for example by emitting an end-of-sequence symbol. This is especially important in settings where the desired output length is unknown or input-dependent.</li> <li><strong>Flexible conditioning</strong>: ARMs allow conditioning on a prefix of any length, which enables a broad class of conditional generation and editing tasks.</li> <li><strong>Efficient sampling</strong>: In large transformers, once a token has been generated, its key-value representations can be cached and reused for a dramatic speed-up at inference time.</li> <li><strong>Compatibility with search and planning algorithms</strong>: Because AR models assign a decomposable likelihood to each partial sequence, they can be combined with search procedures that explore multiple continuations in parallel (e.g., beam search).</li> <li><strong>Online feedback control</strong>: Since generation proceeds step-by-step, external signals can intervene during generation. Intermediate states can be adjusted to guide the next token, enabling closed-loop interaction and real-time control <d-cite key="Hafner2020Dream"></d-cite>.</li> </ul> <p>However, these advantages come at the cost of a rigid dependency structure: the model must commit to a specific, one-token-at-a-time generation order.</p> <h1 id="non-sequential-data">Non-sequential data</h1> <p>We’ve alluded to the idea that autoregressive models rely on a <em>meaningful</em> factorization of the data into a sequence, but that certain data is “non-sequential”. What does this mean exactly? Let’s start with some examples. Spoken and written language are clearly “sequential” in a meaningful way: they are both generated in, and meant to be consumed in, a certain temporal sequence. But if someone asked you to order the pixels from an image into a sequence, what would you choose? Raster order? Top-to-bottom, or bottom-to-top? How about the atoms in a molecule?</p> <p>Perhaps you would answer that it depends on what you want to <em>use</em> the sequence for. Otherwise, how can you choose between many seemingly equivalent orderings? Images, molecular graphs, and 3D point clouds are defined by spatial relationships and symmetries, but there is no single, canonical ordering of elements on which all readers of this post would agree. To distinguish between orderings, we require the notion of <strong>modelability for autoregressive models</strong>.</p> <p>At a high-level, modelability is a general and ubiquitous idea in representation learning: simply put, some representations are easier for models to learn from than others. This perspective echoes e.g. Xu et al.’s <d-cite key="Xu2020A"></d-cite> notion of usable information, which highlights that two representations can encode exactly the same information yet differ dramatically in how easy they are for a model to approximate <d-cite key="dieleman2025latents"></d-cite>. A similar view appears in the rate-distortion-usefulness tradeoff<sup id="fnref:tradeoff"><a href="#fn:tradeoff" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> of Tschannen et al. <d-cite key="tschannen2018recentadvancesautoencoderbasedrepresentation"></d-cite>, where the “usefulness” of a representation depends not only on information content, but also on how that information is organized.</p> <p>We focus here on the specific notion of modelability for autoregressive models. The data representation is now not a single vector per datapoint, but a sequence of discrete tokens $x_1,\dots,x_n$ per datapoint. Since autoregressive models factor the data distribution as $\prod_{i=1}^n p(x_i \mid x_{&lt;i})$, modelability asks: are the induced conditional distributions $p(x_i \mid x_{&lt;i})$ <strong>easily learnable by your model class</strong>? Formally, we can write this as the expected binary cross-entropy (BCE) loss (denoted by $\ell(\text{distribution}, \text{true label})$) of the best next-token prediction model $p_{\theta^*}$ from the model class:</p> \[\mathbb{E}_{x_1,\dots,x_n} \ell \left(p_\theta(\cdot \mid x_{&lt; i}), x_i \right)\] <p>Here, by the “best” model we mean the model produced by a training procedure (usually, optimizing for the same BCE loss) over a finite training set.</p> <p>In words, the autoregressive modelability of a tokenization is simply the test perplexity of the best next-token prediction model. Language (under any standard tokenizer) is highly modelable because next-token models do a good job at, well, predicting the next tokens. Note that modelability is a property of a specific tokenization of a data distribution, not the modality itself. For any data distribution and model class, we can ask what tokenization yields the optimal modelability score (the equation above). Thus, this notion of modelability implicitly depends on the inductive biases and computational limitations (e.g., finite context length and recency bias) of the model class.<sup id="fnref:tokenization"><a href="#fn:tokenization" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p>In this sense, the dichotomy of “sequential” and “non-sequential” is overly simplistic, since one can arbitrarily pick a sequence for any input data. What we really mean is, based on domain knowledge or just common sense, is there an <strong>obviously modelable</strong> sequence? If not, we call the modality “non-sequential”, and assert that more complex methods are needed to identify a modelable tokenization (more on this later).</p> <h2 id="examples">Examples</h2> <p>The difference in modelability between different tokens orders is especially clear in domains where different prediction orders induce subproblems of highly varying difficulty. For example, consider training a model to solve Sudoku puzzles. At a given current state, some cells might be nearly forced, while others are highly ambiguous – so, the difficulty of the prediction subproblem depends strongly on which cell is predicted first. As explored by Kim and Shah et al. <d-cite key="kim2025train"></d-cite>, changing the prediction order of the unfilled Sudoku tiles can shift the model from easy, highly-constrained cases to much harder, underdetermined ones.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/sudoku-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/sudoku-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/sudoku-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/sudoku.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Example of a 4x4 Sudoku board where filling in a highly constrained square first forces the remaining moves, while starting elsewhere would be more ambiguous. Figure taken from <d-cite key="woojrSudokuKids"></d-cite>. </div> </div> </div> <p>A similar example appears in arithmetic tasks, where models have been observed to perform better when generating blocks of digits right-to-left than left-to-right, perhaps reflecting how carries propagate in the computation <d-cite key="singh2024tokenizationcountsimpacttokenization,lee2024digitstodecisions"></d-cite>. Across these settings, a consistent pattern emerges: prediction orders that better align with a task’s underlying structure tend to be more effective.</p> <h2 id="caution-semantic-meaning-of-data-ordering">Caution: semantic meaning of data ordering</h2> <p>In this blogpost, we talk a lot about permuting data tokens. However, naively permuting data tokens clearly doesn’t make sense for domains like language or vision: the sentence “Work is more important than family” has the opposite meaning from its permutation, “Family is more important than work”<sup id="fnref:poem"><a href="#fn:poem" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. Similarly, permuting the pixels of an image can create an entirely new image. In contrast, permuting the points in a point cloud or nodes in a graph preserves the underlying object, and therefore doesn’t lose any information. (It’s possible to formalize this using the concept of equivalence classes, but we focus on modelability, since it is the most relevant concept for autoregressive modeling.) Thus, when we talk about permuting data tokens, what we really mean is changing the generation order – while retaining the information describing the input object itself.</p> <p>Applying standard language modeling objectives to non-sequential data requires us to force a square peg into a round hole: we must linearize the intrinsic geometry of the data into a flat sequence. This raises a challenge: <strong>how should sequential models operate when there is no intrinsic order to exploit?</strong></p> <h1 id="can-we-just-use-non-sequential-models">Can we just use non-sequential models?</h1> <p>Given that many modalities lack a natural left-to-right structure, one might reasonably ask: why use autoregressive models at all? Indeed, some of the most popular alternative frameworks – like generative masked language models and diffusion models – avoid one-token-at-a-time prediction, and operate on entire sequences at once. A brief overview of these approaches is provided in the <a href="#appendix">Appendix</a>.</p> <p>However, compared to autoregressive models, these non-sequential architectures sacrifice several desirable properties discussed <a href="#primer-autoregressive-modeling">earlier</a>, including variable-length outputs, efficient sampling with the KV cache, and step-wise guidance. Thus, there remain many settings in which applying autoregressive methods is still desirable. We should note that there is a growing line of work aiming to combine the strengths of autoregressive and diffusion models <d-cite key="hoogeboom2022autoregressive,chen2024diffusion,arriola2025block"></d-cite>. We set these aside in this post, as our focus is specifically on autoregressive models.</p> <p>A well-studied domain where the lack of an inherent ordering becomes especially salient is vision. Images do not come with a built-in sequence structure, yet significant effort has been devoted to making them autoregressively modelable. We start with this domain as a case study, before diving into more recent trends of tokenization-model alignment.</p> <h1 id="example-images">Example: Images</h1> <p>Images have no inherent traversal order, yet to apply autoregressive models, we must linearize them into a 1D sequence. The earliest attempts, such as PixelRNN <d-cite key="10.5555/3045390.3045575"></d-cite>, flattened the image into a sequence of raw pixels and predicted them one by one in raster scan order (top-to-bottom, left-to-right). While these models achieved strong likelihood scores, they struggled to generate high-quality samples compared to diffusion models <d-cite key="theis2016noteevaluationgenerativemodels"></d-cite>. One limiting factor of pixel-level flattening was that it would result in very long sequences that are difficult to model (e.g., a 256 x 256 image results in a sequence of length 65,536). In fact, this is the same reason that tokenization arose for language <d-cite key="sennrich-etal-2016-neural"></d-cite>!</p> <p>To solve the sequence length problem, the fundamental unit of computation shifted from pixels to patches. This strategy was standardized by Vision Transformers (ViTs) <d-cite key="dosovitskiy2021an"></d-cite>: divide the image into fixed-size squares (e.g., 16 x 16), embed each patch as a token with positional encodings, and arrange them in a sequence. Crucially, ViTs retained the raster scan order, as shown at the bottom of the following Figure from Dosovitsky et al. <d-cite key="dosovitskiy2021an"></d-cite>.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/vit-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/vit-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/vit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/vit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> The Vision Transformer architecture, where the input image is converted into a sequence by flattering the patch grid according to a raster scan order. </div> </div> </div> <p>The modern paradigm of autoregressive models for images adopts this patch-based approach via the following two-stage architecture:</p> <ol> <li> <p><strong>Stage 1 - Tokenization</strong>: Models first compress the image into a codebook drawn from a learned visual dictionary, e.g. using a VQGAN <d-cite key="esser2020taming"></d-cite> or VQ-VAE <d-cite key="vandenOord2017"></d-cite>. The tokenizer is trained once, typically with reconstruction or perceptual losses, and then frozen.</p> </li> <li> <p><strong>Stage 2 - Modeling</strong>: A separate autoregressive (AR) Transformer is trained on the learned tokens in raster order. By offloading low-level reconstruction to the tokenizer, the AR model can devote its computational capacity to modeling global structure and long-range interactions.</p> </li> </ol> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/two_stage-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/two_stage-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/two_stage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/two_stage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Autoregressive image modeling tends to follow a two-step approach. In the first stage, discrete image tokens are trained using a reconstruction loss. In the second stage, the now-fixed tokens are fed into a transformer for generation. </div> </div> </div> <p>Although VQ-VAE and VQGAN tokenizers learn a visual “vocabulary”, they do not remove the need for a fixed ordering, as the tokens are still ordered for input to the transformer. Thus, the <strong>inherent mismatch between the prediction order and the causal structure of natural images remains</strong>: predicting a patch from only previously raster-ordered predecessors might force the model to commit to global structural decisions (e.g., “this is a dog”) based on ambiguous local evidence (e.g., a patch of fur in the top-left corner). This is just one example of how a representation can induce conditional prediction tasks as subproblems that are poorly aligned with the modality’s underlying structure. With this as motivation, we now turn to the general problem of aligning sequential models with non-sequential data.</p> <h1 id="aligning-sequential-models-and-non-sequential-data">Aligning sequential models and non-sequential data</h1> <p>Earlier, we noted a basic tension: representations that preserve all the information in the data tend to make generation more difficult, while representations that simplify prediction inevitably compress or bias the underlying signal, to the detriment of generation <d-cite key="lester2024training"></d-cite>. The goal, then, is to make sequential modeling easier without giving up too much reconstruction quality. We organize the emerging methods for navigating this tradeoff into two distinct categories. One approach keeps the tokenization, and therefore the reconstruction quality, fixed, but modifies the model so that the existing sequence becomes easier to generate (still autoregressively). The other approach keeps the model class fixed but changes the tokenization itself, aiming to find representations that are jointly good for reconstruction and sequential prediction. In other words, the two overarching categories of approaches we identify are:</p> <ul> <li><strong>Model-level</strong>: Given a fixed tokenization, adjust or learn an optimal ordering.</li> <li><strong>Tokenization-level</strong>: Given a desired ordering, adjust or learn the tokenization itself so that the resulting tokens are better aligned with it.</li> </ul> <p>We further subdivide these categories according to the following flowchart, which serves as a roadmap for the remainder of the post.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/roadmap-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/roadmap-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/roadmap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/roadmap.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> What we term model-level vs token-level approaches for applying autoregressive models to data without clear sequential structure. </div> <h2 id="model-level-alignment">Model-level alignment</h2> <p>When a modality lacks an intrinsic traversal order, the challenge is to decide (or discover) the prediction order that leads to the easiest, most structurally coherent subproblems for a model to learn. Several lines of work explore increasingly sophisticated ways of doing this.</p> <h3 id="marginalizing-over-orderings">Marginalizing over orderings</h3> <p>What if we simply train the model to be robust to any sequence using data augmentation? This is precisely the method behind <strong>Any-Order Autoregressive Models (AO-ARMs)</strong>, where the model is trained under random orderings drawn uniformly from all permutations of the input sequence <d-cite key="wang2025learningorder"></d-cite>. Given a permutation $\sigma$ of indices ${1,\dots,n}$, the learned distribution factorizes as \(p(\mathbf{x} \mid \sigma) = \Pi_{i=1}^n p(x_{\sigma_i} \mid \mathbf{x}_{\sigma_{&lt;i}})\) where $\sigma_{&lt;i}$ corresponds to indices ${1, \ldots, i-1}$ under the permutation σ. In this formulation, the permutation can be interpreted as a latent variable that specifies which conditional subproblem the model solves at each step.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Any-order autoregressive models predict next-token distributions over randomly selected token orderings. </div> </div> </div> <p>Moreover, each permutation effectively defines a masking pattern: at step \(i\), the model observes the variables in $\sigma_{&lt;i}$ and treats all variables in $\sigma_{&gt;i}$ as unobserved. Training across many permutations therefore exposes the model to a wide variety of partially observed inputs and forces it to learn conditional distributions of the form \(p(x_i \mid \mathbf{x}_{-i}),\) which is the same family of conditionals targeted by masked language models. As noted by <d-cite key="kim2025train"></d-cite>, <strong>AO-ARMs can be viewed as an autoregressive reformulation of the masked language modeling objective</strong>, differing only in whether the observed subset is chosen by a permutation or by an explicit mask.</p> <p>A related approach is <strong>σ-GPT</strong> <d-cite key="10.1007/978-3-031-70368-3_9"></d-cite>, which also trains on random permutations but does so with explicit positional encodings for both the true index and the permutation order. This enables generation and conditioning in any order.</p> <h3 id="heuristically-choosing-the-order">Heuristically choosing the order</h3> <p>AO-ARMs already introduce the view that, once tokenization is fixed, the remaining structural choice is the permutation that determines the order of prediction. The default any-order training treats this latent variable as uniformly distributed over all permutations, which forces the model to learn a wide family of conditionals. The methods4 in this section build directly on this perspective, exploring how the choice of latent permutation influences the difficulty of the induced prediction tasks and downstream performance.</p> <p>Kim, Shah et al. <d-cite key="kim2025train"></d-cite> examine this sensitivity in Masked Diffusion Models (MDMs) trained under the same uniform distribution over permutations used in AO-ARMs. Instead of sampling tokens in a random or left-to-right order, they propose <strong>adaptive MDM inference</strong>, greedily selecting the position with the lowest predictive entropy. This “most confident first” rule is shown to dramatically improve performance on tasks such as Sudoku by allowing the model to avoid the hard, ambiguous subproblems. Notably, when this learned ordering from the MDM model is included in the training of an ARM, it also improves the performance compared to an ARM trained on the default (left-to-right) ordering.</p> <p>Pramanik et al. <d-cite key="pramanik2025distillingsemanticallyawareorders"></d-cite> adopt a related idea for images in Ordered AR. After training on random patch permutations, their model evaluates all unfilled positions in parallel and selects the next patch using a top-k scoring heuristic. Fine-tuning on these adaptively chosen paths yields a canonical semantic order that leads to improved Frèchet Inception Distance (FID) metrics compared to the raster order. Taken together, both Ordered AR and adaptive MDM inference treat order selection as a heuristic search problem, showing that even simple rules for choosing the next token can substantially improve autoregressive generation.</p> <h3 id="learning-the-order">Learning the order</h3> <p>Rather than using a heuristic, Wang et al. <d-cite key="wang2025learningorder"></d-cite> directly parametrize the ordering via a neural network. In <strong>Learning Order Autoregressive Models (LO-ARMs)</strong>, the ordering is treated explicitly as a latent variable: the model learns an order-policy \(p_x(\sigma) = \Pi_i p(\sigma_i \mid \sigma_{&lt;i}, x_{\sigma_{&lt;i}})\) that, given a partially masked input, chooses a position to unmask next. At the same time, a shared model (UNet for images, Graph Transformer for molecules) produces value logits for every position. Once the order-policy selects a position \(\sigma_i\), the model applies a softmax to choose <strong>what value to place there</strong>. The resulting orderings were found to reflect coherent structural patterns, such as placing border pixels in images or bond structures in molecular graphs first in the ordering!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/lo-arm.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> The LO-ARM sampling process, in which a shared model jointly predicts which position to unmask next and what value to assign. </div> <p>A closely related learned-ordering approach is <strong>REOrder</strong> <d-cite key="kutscher2025reorderingpatchesimprovesvision"></d-cite>, which also trains a policy network to select the next position, and then trains an autoregressive model to follow the discovered task-optimal ordering.</p> <p>The model-level methods show that reordering can meaningfully reduce the difficulty of the generative task, but only within the limits imposed by the underlying tokenization. Since the representation itself is fixed, these approaches cannot introduce coarse-to-fine structure or reshape the information content of the tokens; they can only choose a more favorable sequence in which to model them. As a result, <strong>model-level alignment can improve modelability, but it cannot fully explore the reconstruction-generation tradeoff</strong>.</p> <h2 id="tokenization-level-alignment">Tokenization-level alignment</h2> <p>Tokenization-level methods are motivated by the following question: how can we provide sequential models with the “most sequential,” i.e. most modelable, representation of the input data? If the model generates autoregressively, then the tokenizer can be designed with autoregressive generation in mind from the start!. Thus, they address the reconstruction-generation tradeoff directly at the representation level, rather than merely reordering the prediction path.</p> <h3 id="heuristically-encouraging-ar-modelability">Heuristically encouraging AR modelability</h3> <p>A natural starting point is to impose an ordering that we have good reason to believe (e.g. based on domain intuition) will be easier for an autoregressive model to learn. Instead of relying on the model to discover a useful sequence structure on its own, we can choose an ordering that reflects how information in the modality is organized. A prominent example is <strong>Visual Autoregressive Modeling</strong> <d-cite key="tian2024visual"></d-cite>, which aims to align the prediction order with the hierarchical structure of images. VAR follows a <strong>next-scale</strong> prediction strategy, predicting coarse global structure first and then refining with higher-resolution tokens<sup id="fnref:diffusion"><a href="#fn:diffusion" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>. By replacing a rigid raster order with a more semantically coherent prediction schedule, VAR closed much of the historical gap between AR and diffusion models in metrics such as image quality, inference speed, data efficiency, and scalability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/var-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/var-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/var-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/var.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> Figure from <d-cite key="tian2024visual"></d-cite>. AR and VAR both perform sequential generation, but AR generates one patch at a time, whereas VAR generates a (globally) better-resolved image with each timestep. </div> <p>Note that if prefixes of the token sequence already encode global structure, then shorter sequences essentially give a coarse depiction of the image, while longer sequences progressively increase fidelity. Several works have leveraged this property of coarse-to-fine tokenization to allow for a <strong>variable number of tokens per image</strong>. In doing so, they also tackle a core limitation of the fixed-size patch grid in standard tokenizers, where every image is forced into the same number of tokens regardless of its complexity. A plain sky and a dense texture both consume the same token budget, creating inefficiency for simple images and information loss for complex ones. Instead, sequence length can track the information density of the image. Some examples of variable-length tokenization methods include:</p> <ul> <li> <p><strong>FlexTok</strong> <d-cite key="bachmann2025flextok"></d-cite> uses nested dropout, repeatedly chopping off the tail during training so that high-level content is forced into the early positions.</p> </li> <li> <p><strong>Matryoshka Multimodal Models</strong> <d-cite key="cai2025matryoshka"></d-cite> learn nested token subsets where each prefix is already a valid representation and additional tokens just add finer details.</p> </li> <li> <p><strong>One-D-Piece</strong> <d-cite key="miwa2025onedpieceimagetokenizermeets"></d-cite>introduces a “Tail Token Drop” regularization that removes later tokens on the fly and pushes essential global semantics into the head of the sequence.</p> </li> </ul> <p>This shift from “next-patch’’ to “next-scale’’ prediction reframes what it means for images to be “non-sequential”, and illustrates the flexibility that comes with modifying the data representation directly (rather than just the ordering of predefined tokens). While there is no obvious choice of order in the spatial dimension, VAR suggests that the ordering along the resolution dimension is highly modelable in an autoregressive manner.</p> <p>This principle likely extends far beyond computer vision. Whether generating molecular graphs (e.g., defining the scaffold before functional groups) or 3D geometry (e.g., blocking out shapes before refining surface details), the more modelable sequence for complex data might be a trajectory from low to high complexity or resolution, rather than a linear path through spatial components. Moreover, the optimal generation path need not be semantically interpretable. Beyond explicit hierarchies like resolution, we expect that many high-dimensional datasets possess hidden latent structures that define a natural generation order, even if it is abstract and invisible to human observers<sup id="fnref:language"><a href="#fn:language" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Identifying the most modelable order may require deep domain knowledge in most settings. This, in turn, motivates methods that dispense with predefined heuristics and instead build autoregressive-friendly structure directly into the learned tokenization.</p> <h3 id="autoregressive-priors">Autoregressive priors</h3> <p>In the standard two-stage tokenization-generation paradigm, Stage 1 and Stage 2 are often treated as independent problems. The tokenizer minimizes reconstruction loss, while the generator minimizes prediction loss. However, a tokenizer that is optimal for reconstruction with a global, bidirectional decoder is often suboptimal for autoregressive generation. To bridge this gap, recent works have introduced an autoregressive prior directly into the Stage 1 training process. By enforcing causal constraints during tokenization, these methods ensure the resulting codebook aligns with the sequential nature of the downstream generator. We visualize three representative approaches to this alignment in the figure below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/token_level-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/token_level-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/token_level-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-autoregressive-tokenization/token_level.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> Three approaches to incorporating autoregressive priors during tokenization. CRT <d-cite key="ramanujan2025when"></d-cite> adds next-token prediction on continuous latents, AliTok <d-cite key="wu2025sequencemodelingalignmenttokenizer"></d-cite> imposes causal decoding during Stage 1 but relaxes it during Stage 2, and LARP <d-cite key="wang2025larp"></d-cite> applies an AR prior only to global query tokens produced by a stochastic quantizer. </div> <p><strong>Causally Regularized Tokenization</strong> (CRT). Ramanujan et al. <d-cite key="ramanujan2025when"></d-cite> keep the standard encoder-quantizer-decoder architecture, but modify the objective function. CRT adds a next-token prediction loss on the pre-quantized continuous latents, encouraging tokens to be predictable from their predecessors. This explicitly trades off reconstruction quality for AR predictability and yields better downstream generative performance as well as computational efficiency.</p> <p><strong>Aligned Tokenizer</strong> (AliTok). Wu et al. <d-cite key="wu2025sequencemodelingalignmenttokenizer"></d-cite> propose AliTok, which directly constrains the decoder to be autoregressive. While the causal decoder provides a mechanism for enforcing sequential structure in the tokens, it also limits reconstruction quality. During the second stage, this limitation is mitigated by jointly training a high-fidelity bidirectional decoder while retaining the causal structure from the first stage. This approach offers a practical means of reconciling the two objectives, albeit through a multi-step process.</p> <p><strong>Learned AutoRegressive generative Prior</strong> (LARP): Instead of forcing autoregressive constraints onto all patch tokens, LARP <d-cite key="wang2025larp"></d-cite> adds a set of learned “holistic” query tokens that summarize high-level video semantics. An AR prior is trained only on these (de-quantized) query vectors, giving them a coherent causal structure without imposing constraints on the low-level patch tokens. Since they opt to use a stochastic vector quantization scheme (sampling from the codebook similarity distribution), the AR prior is trained to predict the next-token distribution. Despite the surface-level methodological differences between these methods, they all build sequentiality directly into the learned tokenizer, using general learning methods rather than heuristics.</p> <h1 id="conclusion">Conclusion</h1> <p>In sum, autoregressive models offer a flexible, efficient method for generative modeling tasks, but they require tokens to be input in some ordering. However, many modalities of interest outside language lack a clear, natural ordering for tokenization. Although one can simply choose an arbitrary ordering convention, it may not optimize the resultant model’s generation quality. This is the notion of “modelability,” which we apply specifically to autoregressive models. To improve the autoregressive modelability of arbitrary modalities, model-based approaches broadly attempt to find the most modelable ordering of some fixed tokenization. In contrast, tokenization-based approaches take sequential generation into account when constructing the tokenization itself. This encourages ordered tokenizations that are easy to incrementally predict. In a scattered landscape of diverse tokenization and ordering strategies, we hope to have provided not just a methodological survey, but a unifying perspective.</p> <p>For researchers who work with boutique architectures or non-language data modalities, we want to highlight tokenization and “sequential-ization” as promising directions for future research – in particular, modality-specific tokenization methods that anticipate the sequential nature of their downstream model and align with it. Notably, most existing alignment strategies have been explored primarily in the image domain, leaving substantial room for discovering analogous structures in other forms of data.</p> <p>There are many possible routes for the future of non-sequential data. Perhaps specialized architectures for each modality will win out in the end, and the mismatch we expound upon in this blogpost won’t be relevant! But at this point, it seems highly unlikely that the application of large, generalist sequential models for non-sequential data will disappear entirely. After all, even agents calling specialized models as tools must be able to describe the objects of interest with a sequence of tokens. Thus, the square peg for the round hole remains.</p> <h1 id="appendix">Appendix</h1> <h2 id="examples-of-non-sequential-generative-models">Examples of non-sequential generative models</h2> <p>Masked Language Models (MLMs) depart from the autoregressive likelihood factorization by adopting a denoising objective that permits bidirectional context. The model learns to reconstruct missing tokens from a global view of the input, rather than relying only on past context. This approach was popularized at scale by BERT, which showed that masked language modeling can produce highly transferable representations for a wide range of downstream tasks <d-cite key="devlin-etal-2019-bert"></d-cite>.</p> <p>Given an input sequence $\mathbf{x}$, a random subset of tokens at positions $M \subset {1,\dots,n}$ are replaced with a special [MASK] token. The model receives the masked sequence $\tilde{\mathbf{x}}$, where $\tilde{\mathbf{x}}_i = \text[MASK]$ for $i \in M$, and is optimized to estimate the conditional distributions, \(p(x_i \mid \tilde{\mathbf{x}}), \quad \text{for all } i \in M.\)</p> <p>In expectation, this objective is repeated over many different random masks \(M\), exposing the model to a rich family of partially observed subproblems. In this sense, <strong>autoregressive models can be seen as a specific subproblem of MLMs</strong>, where the masking is always applied to the final position in the input sequence and the prediction is conditioned only on past tokens. Note that unlike ARMs, MLMs do not yield a tractable likelihood over complete sequences. The loss is a sum of cross-entropies over the randomly masked positions, which is not equal to the log-likelihood of the whole sequence (as was the case with ARMs): \(\mathcal{L_{\text{MLM}}}(\theta) = -\sum_{i \in M} \log p_\theta(x_i \mid \tilde{\mathbf{x}}).\)</p> <p>Despite relaxing the left-to-right prediction order, MLMs still rely on the underlying positional structure of the sequence (as captured by the positional encodings) to determine which tokens constitute the context for each prediction. Thus, both ARMs and MLMs fundamentally assume an ordered sequence of tokens.</p> <p>If we view masking as a type of corruption process, then iterating the reconstruction step naturally gives rise to <strong>diffusion models</strong> <d-cite key="Ho2020"></d-cite>. Diffusion models define a forward process that starts from the data $x_0$ and progressively adds noise until the sample becomes nearly Gaussian. The reverse process refines the entire sample at once rather than predicting one symbol at a time, meaning that generation is defined without any notion of token order. While diffusion models have historically achieved better generative performance than autoregressive approaches in continuous domains, recent work suggests that this gap may be narrowing as better latent parameterizations become available <d-cite key="tian2024visual"></d-cite>.</p> <p>While diffusion was originally formulated using continuous Gaussian noise, several lines of work show that the same iterative denoising idea extends naturally to discrete domains. Discrete denoising diffusion probabilistic models (D3PM) replace Gaussian noise with a categorical corruption process such as random token replacement <d-cite key="austin2021"></d-cite>. Alternatively, <strong>Masked Diffusion Models (MDMs)</strong> use masking rather than categorical replacement as the corruption operator <d-cite key="lou2024discrete"></d-cite>, where each diffusion step applies a random masking pattern and the model is trained to reconstruct the missing content. As highlighted by Zheng et al. <d-cite key="zheng2025masked"></d-cite>, this makes the <strong>learning problem of MDMs equivalent to MLMs</strong>.</p> <h1 id="footnotes">Footnotes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:soft"> <p>Recent work has explored the use of continuous or “soft” tokens with an infinite vocabulary to allow for a more semantically rich latent space <d-cite key="tschannen2025jetformer"></d-cite>. <a href="#fnref:soft" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:tradeoff"> <p>The tradeoff between modelability and reconstruction is not strict (“if A increases, B necessarily decreases”): indeed, model-level approaches to computing the optimal ordering of tokens preserve reconstruction while improving modelability. However, the two are generally competing objectives. For example, as noted in CRT <d-cite key="ramanujan2025when"></d-cite>, the optimally modelable tokenization is a single constant token, which fails at reconstruction. <a href="#fnref:tradeoff" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:tokenization"> <p>This naturally dovetails with the theory of tokenization discussed by Rajaraman et al. <d-cite key="rajaraman2025theorytokenizationllms"></d-cite>: there, the advantages of tokenization were related to the limitations of the transformer, which tended to learn unigram models in their setting. <a href="#fnref:tokenization" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:poem"> <p>As is famously capitalized upon in Jonathan Reed’s reversible poem “The Lost Generation” – a clever work that can be read forwards and backwards, with diametrically opposed meanings <a href="#fnref:poem" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:diffusion"> <p>This loosely parallels the way diffusion models reconstruct signals by first resolving low-frequency components and then adding higher-frequency detail. Dieleman <d-cite key="dieleman2025latents"></d-cite> gives an intuitive explanation of the viewpoint that “diffusion is spectral autoregression”, and Falck <d-cite key="falck2025fourier"></d-cite> offers a complementary analysis that clarifies the conditions under which the connection holds. <a href="#fnref:diffusion" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:language"> <p>Even when there is a canonical ordering for the input, it might not be the most faithful to the underlying generative process. For example, one could imagine that the “best” ordering for certain language modeling tasks might be structured according to some hierarchy of abstract concepts or reasoning steps rather than the default left-to-right sequence. There is thus a growing body of work on Transformers that operate in a latent token space <d-cite key="sun2025enhancinglatentcomputationtransformers"></d-cite>. <a href="#fnref:language" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Autoregressive (AR) models are central to modern generative AI systems, yet their sequential inductive bias clashes with modalities that lack an obvious ordering, such as images, graphs, and point clouds. Despite this mismatch, AR models are widely used beyond language, owing to their scalability and controllability. This post highlights the growing set of techniques that make non-sequential data amenable to autoregressive modeling. There are two broad directions: approaches that choose or optimize a generation order for a fixed tokenization, and approaches that redesign the tokenization itself to simplify each next-token prediction step. We emphasize the tradeoffs these methods face, particularly between compression and autoregressive &quot;modelability&quot;. By drawing these connections, we aim to motivate future work on tokenizations tailored to the needs of autoregressive models for arbitrary datatypes.]]></summary></entry><entry><title type="html">Hypes and Hopes for Causal Inference for Brain Dynamics</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/causal-ts/" rel="alternate" type="text/html" title="Hypes and Hopes for Causal Inference for Brain Dynamics"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/causal-ts</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/causal-ts/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The problem of identifying latent sources from sensor-level time series appears in many scientific domains, including neuroscience. A central question is whether it is possible to recover independent underlying sources from sensor mixtures, especially when the mixing process is nonlinear. We examine this question in the context of EEG recordings and evaluate whether Time-Contrastive Learning (TCL), a nonlinear ICA method, can recover meaningful source-level representations.</p> <blockquote> <p>In short: we find that while TCL can exploit nonstationarity, it does not outperform PCA in recovering components aligned with ground-truth cortical sources.</p> </blockquote> <hr/> <h2 id="background">Background</h2> <p>A classical formulation considers a vector of sources ( s = (s_1, …, s_n) ) and a mixing process:</p> \[x = A s ,\] <p>where ( x ) is observed sensor activity. ICA methods aim to recover sources by assuming statistical independence and non-Gaussianity.</p> <p>Standard ICA succeeds for linear mixtures but fails in general nonlinear settings. This motivated nonlinear ICA approaches such as the method of Hyvärinen &amp; Morioka (2016), which introduces <strong>nonstationarity</strong> as additional information for identifiability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/mixing-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/mixing-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/mixing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/mixing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="what-is-identifiability">What Is Identifiability?</h2> <p>Identifiability formalizes whether the original sources can be uniquely recovered given observations. For nonlinear mixing ( x = f(s) ), the mapping is typically non-identifiable: many combinations of ( f ) and ( s ) produce the same observed distribution.</p> <p>Nonlinear ICA becomes identifiable only when extra structure—such as temporal nonstationarity—is introduced.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/identifiability-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/identifiability-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/identifiability-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/identifiability.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="time-contrastive-learning">Time-Contrastive Learning</h2> <p>TCL exploits the idea that if latent sources change their distributions differently across <em>time segments</em>, then a classifier trained to predict segment identity must extract features informative about those sources.</p> <p>The steps are:</p> <ol> <li><strong>Segment</strong> the multivariate time series ( x_t ) into windows indexed by ( \tau ).</li> <li><strong>Label</strong> points using their segment index.</li> <li><strong>Train</strong> a neural network encoder ( h(x_t; \theta) ) and a linear classifier:</li> </ol> \[w_\tau^\top h(x_t) + b_\tau.\] <p>The classifier approximates log-density differences</p> \[w_\tau^\top h(x) + b_\tau \approx \log p_\tau(x) - \log p_1(x),\] <p>and the learned representation satisfies:</p> \[h(x) \approx A q(s) + d ,\] <p>under assumptions on nonstationarity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/tcl-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/tcl-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/tcl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/tcl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="experiments">Experiments</h2> <p>We evaluate TCL on both simulated nonlinear mixtures and EEG recordings. For EEG, we also compute source reconstructions to serve as proxy ground truth.</p> <h3 id="workflow-overview">Workflow Overview</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/workflow-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/workflow-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/workflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/workflow.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We obtain:</p> <ul> <li><strong>TCL embeddings</strong> from sensor data</li> <li><strong>PCA embeddings</strong> as a baseline</li> <li><strong>Source reconstruction embeddings</strong> as the evaluation target</li> </ul> <h3 id="evaluation">Evaluation</h3> <p>For simulated data, sources are nonstationary Laplacian processes with nonlinear mixing.</p> <p>For EEG, source-level signals correspond to 116 anatomical regions. We compare learned representations ( h_{\mathrm{tcl}} ) and PCA representations ( h_{\mathrm{pca}} ) against these reconstructed sources.</p> <h3 id="sparse-prediction-analysis">Sparse Prediction Analysis</h3> <p>We perform canonical correlation-style linear prediction:</p> <ul> <li>Train a linear model to map ( h_{\mathrm{tcl}} \rightarrow h_r )</li> <li>Train a baseline model ( h_{\mathrm{pca}} \rightarrow h_r )</li> </ul> <p>Ideal recovery would produce a near-permutation matrix. Instead we observe:</p> <ul> <li>PCA: sharp diagonal structure</li> <li>TCL: weak, sparse, non-diagonal mappings</li> </ul> <hr/> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/results-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/results-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-causal-ts/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-causal-ts/results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Regression coefficients comparing learned components with source-level components:</p> <table> <thead> <tr> <th>method</th> <th>unseen_size</th> <th>MDD</th> <th>ODER</th> </tr> </thead> <tbody> <tr> <td>PCA</td> <td>4</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>10</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>4</td> <td>0.0003</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>10</td> <td>0.0004</td> <td>0.9997</td> </tr> <tr> <td>TCL</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> </tbody> </table> <p>TCL did not surpass PCA in alignment with source components. PCA maintained strong diagonal structure, while TCL representations produced weaker and more diffuse correlations.</p> <hr/> <h2 id="discussion">Discussion</h2> <h3 id="why-causal-inference-style-identifiability-fails-on-realistic-eeg">Why causal-inference-style identifiability fails on realistic EEG?</h3> <p>Real EEG includes strong 1/f background activity from many neural populations. The nonstationarity signal needed by TCL is overshadowed. Thus TCL learns features reflecting global variability rather than source-specific fluctuations.</p> <h3 id="are-causal-effects-recoverable-at-the-scalp">Are causal effects recoverable at the scalp?</h3> <p>Some directed-dependency methods (e.g., Granger-style approaches) can reveal predictive relationships, but these differ from structural causal models. Identifiability for nonlinear SCMs would require:</p> <ul> <li>known interventions,</li> <li>control over confounded pathways,</li> <li>or guaranteed invertibility of the generative process,</li> </ul> <p>none of which hold for EEG sensors.</p> <hr/> <h2 id="appendix">Appendix</h2> <p>Dataset details, implementation notes, and source reconstruction parameters follow those in the accompanying project documentation.</p> <p>Additional figures (e.g., replication checks) verify that our TCL implementation behaves as expected.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We study whether modern identifiability-based nonlinear ICA methods, in particular Time-Contrastive Learning (TCL), can recover meaningful sources from realistic scalp-level brain recordings such as EEG. Using simulated data, EEG sensor data, and source-reconstructed cortical activity, we evaluate whether TCL provides representations aligned with the underlying sources.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/distill-example</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/9-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/9-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/8-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/8-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/10-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/10-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/11-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/11-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/12-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/12-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/blockpost_flashMD/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <a-cite key="gregor2015draw">&lt;/d-cite&gt; (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</a-cite></p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">dLLM - Rethinking Generation Beyond Autoregressive Models</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dllm/" rel="alternate" type="text/html" title="dLLM - Rethinking Generation Beyond Autoregressive Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dllm</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dllm/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Despite the existence of a plethora of architectures and learning objectives, most language models in the current pantheon follow a time-tested recipe: a Transformer backbone trained using a next-token prediction objective, with model outputs generated autoregressively. This recipe, while simple, has proved surprisingly resilient and is still the predominant paradigm for training language models.</p> <p>To recap, autoregressive models factorize the joint probability of a token sequence of length \(T\), where \(x_{1:T}\) is a product of next-step conditionals:</p> \[p(x_{1:T}) = \prod_{t=1}^{T} p(x_t \mid x_{\&lt; t})\] \[x_{\&lt; t} = (x_1, x_2, \ldots, x_{t-1})\] <p>Here, \(p(x_{1:T})\) denotes the joint probability of the entire sequence and \(\(p(x_t \mid x_{\&lt; t})\)\) represents the probability of the next token given all previous tokens, and \(\(x_{\&lt; t}\)\) refers to the prefix of the sequence up to time step \(t-1\). This decomposition expresses a high-dimensional distribution as a product of simpler conditional distributions, which is the defining property of autoregressive models.</p> <p>These models generate sequences one token at a time. This formulation comes with some inherent disadvantages:</p> <ol> <li>Generating tokens one at a time imposes a sequential bottleneck. This means that generation latency scales with output length. While methods like KV caching make subsequent decoding faster, they are not able to eliminate the sequential dependency.</li> <li>Errors can rapidly accumulate and snowball, as the generation of one incorrect token causes it to be added as context for all subsequent tokens to rely on.</li> <li>Once a token is generated, there is no possibility for editing or revision in-place; the generated token remains part of the context for the rest of the sequence.</li> <li>ARMs optimize for token-level likelihood, which does not always correlate with sequence-level goals. The resulting tunnel vision can impede the ability of the model to plan over long-horizons or maintain coherence over longer outputs</li> <li>Hallucinations get exacerbated with ARMs, because once a token is generated, it is used as context for subsequent generations, therefore leading the model to generate coherent narratives over incorrect facts.</li> <li>Theoretically, certain distributions can be represented more efficiently by models that marginalize over latent variables; representing the same distributions via a purely autoregressive formulation can require scaling up the model parameter size super-polynomially with input length.</li> </ol> <p>As the bitter lesson <d-cite key="sutton2019bitter"></d-cite> shows, working towards learning more general capabilities and scaling them has proven more fruitful than injecting tasks or domain-specific rules into a model. In a similar vein, we can challenge the inductive biases of autoregressive models (i.e. that language sequences are generated left-to-right), thus providing the model with more freedom.</p> <p>One such alternative to autoregressive models is the diffusion paradigm. Diffusion models have seen great success in computer vision, but just like many other techniques that have found success in computer vision, adapting them to text has been hard, primarily due to the discrete nature of language. Therefore, more focus has been placed on discrete diffusion models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/MaskedDiffusion-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/MaskedDiffusion-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/MaskedDiffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/MaskedDiffusion.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In this blog, we focus specifically on masked discrete diffusion models, termed as dLLMs. Masked diffusion models have garnered a lot of interest recently, with a rapidly growing body of work in this paradigm.</p> <p>Masked diffusion works by:</p> <ol> <li>Defining a forward noising process that gradually replaces tokens with a special token (typically <code class="language-plaintext highlighter-rouge">[MASK]</code>), and</li> <li>Learning a reverse denoising process that iteratively predicts and “locks in” tokens until the sequence is fully unmasked.</li> </ol> <p>The masked diffusion learning objective looks similar to masked language modeling (e.g. BERT), or denoising autoencoding (e.g. BART). However, there is a key difference differentiating diffusion from these other objectives. Diffusion models are trained across a range of corruption levels and use an iterative sampling process that starts from a fully or almost fully noisy sequence and progressively denoises, rather than doing a single reconstruction from a fixed corruption scheme like BERT or BART.</p> <p>Diffusion models are still generative models. Unlike masked language modeling where the masking rate is fixed through the training process, diffusion models randomly sample a masking rate (or “time”) between 0 and 1 for each example <d-cite key="nie2025largelldm"></d-cite>.</p> <p>A typical masked diffusion training objective can be expressed as a time-weighted cross entropy calculated over masked tokens:</p> \[t \sim \mathcal{U}(0,1), \qquad x_t \sim q(x_t \mid x_0, t)\] \[\mathcal{L}(\theta) = \mathbb{E}_{x_0,\, t,\, x_t} \Big[ w(t)\, \sum_{i \in \mathcal{M}(x_t)} -\log p_\theta(x_{0,i} \mid x_t, t) \Big]\] <p>In this formulation, we first sample a time variable</p> \[t \sim \mathcal{U}(0,1)\] <p>representing a point along a continuous noising schedule. Given the original data \(x_0\), we then generate a partially corrupted version</p> \[x_t \sim q(x_t \mid x_0, t)\] <p>where the corruption is controlled by the sampled time \(t\).</p> <p>The model is trained to reverse this corruption using the loss function:</p> \[\mathcal{L}(\theta) = \mathbb{E}_{x_0, t, x_t} \Big[ w(t) \sum_{i \in \mathcal{M}(x_t)} -\log p_\theta(x_{0,i} \mid x_t, t) \Big]\] <p>Here, \({M}(x_t)\) indicates the positions in \(x_t\) that have been masked or noised, and \(w(t)\) is a weighting term that ensures heavily corrupted examples do not dominate the learning signal.</p> <p>Intuitively, the loss encourages the model to predict the original tokens \(x_{0,i}\) at the masked positions, given the noisy input \(x_t\) and the noise level \(t\). By learning to undo the corruption at every point along the noise schedule, the model effectively learns a denoising process that can reconstruct clean data from partially corrupted inputs. This principle is central to diffusion-based generative modeling and related reconstruction tasks.</p> <p>Autoregressive models optimize the maximum likelihood objective directly. Diffusion models are derived from a variational formulation (ELBO / NELBO), though many practical implementations use a weighted mixture of masked-token cross-entropies as shown above.</p> <h1 id="characteristics-of-diffusion-models">Characteristics of Diffusion Models</h1> <p>From a user standpoint, diffusion models are said to generate by infilling (iterative refinement of a partially completed sequence). This is especially suitable for tasks like coding or reasoning, which are often non-linear. Infilling also provides opportunities for personalization and enhances structured generation. The decoding order is also configurable.</p> <p>Some obvious benefits of diffusion models include the ability to perform any-order modeling, in-place context modification, and parallel token prediction.</p> <p>Let’s now explore the mechanics of masked diffusion in detail.</p> <h2 id="masked-diffusion-explained">Masked Diffusion Explained</h2> <p>Masked diffusion can be implemented independent of the architecture. For example, masked diffusion can use state-space models as the backbone.</p> <h3 id="forward-process">Forward Process</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessA-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessA-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessA.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessB-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessB-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessB-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ForwardProcessB.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Consider an example sequence x in the training dataset:</p> <p><code class="language-plaintext highlighter-rouge">‘He invented the parallelogram as a means to exact vengeance upon his detractors’</code></p> <p>A number \(t\) sampled randomly between 0 and 1 (often chosen from a discrete mask schedule in practice), is chosen to be the mask strength. Each token in the given sequence is replaced by a <code class="language-plaintext highlighter-rouge">[MASK]</code> token with probability \(t\).</p> <p>For example, if \(t=0.2\),</p> <p><code class="language-plaintext highlighter-rouge">‘He invented the [MASK] as a means to [MASK] vengeance upon [MASK] detractors’</code></p> <p>If \(t=0.8\),</p> <p><code class="language-plaintext highlighter-rouge">‘[MASK] invented [MASK] [MASK] [MASK] means [MASK] exact vengeance [MASK] [MASK] [MASK]</code></p> <p>Let’s refer to the masked sequence as \(xt\).</p> <p>The objective of the model is to predict the masked tokens in \(xt\). The training loss is typically the cross entropy over the masked tokens, with a normalization/weighting such that examples with higher masking rates do not contribute disproportionately to the training signal. One common way of normalization is to divide the loss by the masking strength \(t\). Equivalently, some implementations instead normalize by the number of masked tokens.</p> <p>The key difference between masked language models like BERT and diffusion models is that in BERT the corruption policy (masking rate) is fixed throughout training, while in masked diffusion models the masking rate \(t\) varies per example across a range of masking rates.</p> <h3 id="reverse-process">Reverse Process</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ReverseProcess-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ReverseProcess-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ReverseProcess-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ReverseProcess.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Prompt: ‘Is Socotra a real place?’</code> <code class="language-plaintext highlighter-rouge">Response: ‘Yes, Socotra is an island in Yemen.’</code></p> <p>A typical reverse process proceeds like this:</p> <p>For a given prompt \(p\), an initial answer is generated consisting entirely of <code class="language-plaintext highlighter-rouge">[MASK]</code> tokens. The response length is typically a hyperparameter.</p> <p>The reverse process (called denoising) runs for K steps, which is typically a hyperparameter.</p> <p>At each step, the model predicts tokens for all the <code class="language-plaintext highlighter-rouge">[MASK]</code> positions at once, conditioned on the prompt and the currently unmasked tokens. It then commits some tokens (unmasks them) and remasks a portion of tokens (often low-confidence ones), either randomly or via heuristics. Generation stops after all denoising steps are completed. If an <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is present in the final output, then the tokens after it are discarded.</p> <p>In practice, there is a discrepancy between training and inference; during inference, the whole output often starts off as masked and is gradually de-masked.</p> <h2 id="training">Training</h2> <p>Diffusion language models can continue using the same Transformer backbones that underpin today’s language models. The primary change is in the learning objective, where instead of predicting the next token in a sequence as autoregressive models do, diffusion models are taught to predict all the masked tokens in a sequence simultaneously. This also means that dLLMs can be built using non-Transformer backbones (e.g., state-space models), as long as the architecture supports conditioning on a partially observed sequence.</p> <h3 id="typical-training-pipeline">Typical Training Pipeline</h3> <ol> <li>Pre-train from scratch OR Continued pre-training</li> <li>Midtraining/annealing</li> <li>Instruction tuning</li> <li>Reinforcement learning</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/TrainingPipeline-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/TrainingPipeline-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/TrainingPipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/TrainingPipeline.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Note that dLLMs can either be trained from scratch or can be adapted from a base ARM.</p> <h3 id="pre-training-from-scratch">Pre-training From Scratch</h3> <p>While training from scratch, training with next-token prediction objective is more sample and compute efficient than diffusion in practice. This is because in dLLMs, the loss is typically calculated only over the masked tokens, so each forward pass in dLLMs supervises fewer target positions than an AR pass . As a result, given the same architecture, compute, and data, AR baselines typically train faster and reach higher quality, though the exact gap depends on masking schedules, weighting, and decoding strategies used.</p> <h3 id="adapting-ar-models-to-dllms">Adapting AR Models to dLLMs</h3> <p>Pre-training from scratch is not the only option; one can also adapt existing autoregressive models to support diffusion. The adaptation is typically carried out using continual pre-training. In this technique, we take a stable checkpoint of an autoregressive LLM, replace the causal mask with a bidirectional mask and then continue pre-training it with the diffusion learning objective.</p> <p>Chandrasegaran et al. <d-cite key="chandrasegaran2025rnd1"></d-cite> propose that self-attention weights are trained with a relatively higher learning rate to help adapt the model to the diffusion paradigm. The feed forward layers are trained at a relatively lower learning rate so that world knowledge and other capabilities learned during the AR pre-training stage are retained. This helps mitigate catastrophic forgetting. They also observe that dLLMs benefit from larger batch sizes during continual pre-training. Other techniques for adaptation from ARMs include</p> <ul> <li>Grafting, where the architecture is edited by swapping causal attention blocks for bidirectional attention blocks.</li> <li>Attention mask annealing, where the causal mask is gradually converted into a bidirectional one during training.</li> </ul> <p>Masked language models like BERT can also be converted into diffusion models using the continued pre-training approach.</p> <h2 id="inference">Inference</h2> <p>During inference, the model starts from a masked output sequence and generates tokens through a series of denoising steps. Within a denoising step, the masked positions are typically predicted in parallel, followed by an accept or remask decision.</p> <p>A basic denoise-and-remask procedure works as follows. We first initialize the output sequence to be entirely masked:</p> \[y^{(0)} = [\text{MASK}]^L\] <p>where \(L\) is the sequence length, typically chosen as a hyperparameter. Then, for a fixed number of iterations \(k = 1, \dots, K\), we perform the following steps:</p> <ol> <li><strong>Predict masked token distributions:</strong> Using the model, we estimate the probability distribution over tokens at the currently masked positions:</li> </ol> \[p_\theta(\cdot \mid p, y^{(k-1)}, k)\] <p>where \(p\) may represent any conditioning information (e.g., a prompt or context), \(y^{(k-1)}\) is the sequence from the previous iteration, and \(k\) indicates the current step.</p> <ol> <li> <p><strong>Commit a subset of positions:</strong> We select a subset of tokens to “commit” to the output sequence. This is usually based on a confidence criterion, such as selecting the highest-probability tokens or those with the lowest entropy.</p> </li> <li> <p><strong>Optional remasking:</strong> To refine the sequence, a heuristic or schedule may remask a subset of previously committed tokens that are considered low-confidence. This allows the model to revisit uncertain predictions in subsequent iterations.</p> </li> <li> <p><strong>Update the sequence:</strong> The newly committed tokens replace the previous masked positions to form the updated sequence \(y^{(k)}\).</p> </li> </ol> <p>After completing all \(K\) iterations, the final sequence \(y^{(K)}\) is returned. If an end-of-sequence token <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> appears, any tokens following it are discarded.</p> <p>This iterative denoising procedure gradually replaces masks with high-confidence predictions, while optionally revisiting uncertain tokens. Over multiple steps, the sequence converges toward a coherent output that reflects both the learned model distribution and any conditioning context.</p> <p>The initial output sequence can be fully masked or it can contain parts of the output we already know; delegating the model to perform infilling for the tokens we do not know. This can be operationalized in a few ways, such as constrained endings or structured infilling.</p> <h3 id="structured-infilling">Structured Infilling</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/StructuredInfilling-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/StructuredInfilling-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/StructuredInfilling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/StructuredInfilling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Instead of asking the model to generate output in a specific structured format (like JSON), we use a structured format template and let the dLLM fill in the blanks.</p> <p>For a given structured format, we have:</p> <ul> <li>Invariant tokens (syntax, labels, brackets etc), which stay unmasked</li> <li>Variable tokens (value, content) which are masked</li> </ul> <p>An advantage with infilling templates is that it shrinks the search space during the generation, now that it need only choose content tokens and not tokens related to the syntax. Another advantage is that it implicitly ensures the structured format is adhered to during generation.</p> <p>The tricky part of this technique is in deciding how many masked tokens to allocate for the variables. If the masked tokens added are inadequate, the output has to be truncated. If too many masked tokens are added, then the model tries to fill in the extraneous masked tokens with content, leading to unpredictable outcomes.</p> <p>Self-adaptive schema scaffolding (S3) addresses this issue by allowing the model to output a special null token upon which generation for that variable block stops, leaving the remaining slots empty.</p> <h2 id="decoding-strategies">Decoding Strategies</h2> <h3 id="random">Random</h3> <p>A simple baseline is random unmasking, where the positions to commit/unmask at each step are chosen uniformly at random. However in practice, heuristics tend to be more efficient and higher quality.</p> <h3 id="confidence-based-sampling">Confidence-Based Sampling</h3> <p>Confidence-based sampling is a common strategy in iterative denoising or masked sequence generation. In this approach, tokens with high confidence are “locked in,” while low-confidence tokens may be remasked for further refinement.</p> <p>However, this strategy is not always optimal. High-confidence tokens are often syntactic or structurally predictable, which can cause the model to commit to the surface structure of the sequence too early, potentially limiting the flexibility of subsequent generation.</p> <p>A typical way to quantify the confidence of a token at position (i) is:</p> \[c_i = \max_v p_\theta(v \mid \text{context})\] <p>where \(p_\theta(v \mid \text{context})\) is the predicted probability of token \(v\) given the current context, and \(c_i\) represents the confidence score for position \(i\). Tokens with higher \(c_i\) are more likely to be committed, while those with lower confidence can be remasked and reconsidered in future iterations.</p> <p>This method provides a simple and interpretable heuristic for guiding which positions to finalize versus which to refine, balancing stability and flexibility in the generated sequence.</p> <h3 id="entropy-based-sampling">Entropy-Based Sampling</h3> <p>This technique uses entropy as a confidence measure, where lower entropy implies higher confidence. This is often more robust than raw probability thresholds. A common way to calculate the entropy at position \(i\) is:</p> \[H_i = - \sum_v p_i(v) \log p_i(v)\] <p>where \(p_i(v)\) is the probability assigned to token \(v\) at position \(i\). Here, \(H_i\) measures the uncertainty of the model’s prediction: positions with low entropy correspond to confident predictions, while positions with high entropy indicate ambiguity.</p> <h3 id="margin-based-sampling">Margin-Based Sampling</h3> <p>Margin-based sampling uses a second-order confidence measure: we take the difference between the confidence of the most probable token and the second most probable token as the margin, and select only tokens that have a high enough margin.</p> <p>Formally, let \(v_1\) and \(v_2\) be the most probable and second-most probable tokens at position \(i\). The margin is defined as:</p> \[m_i = p_i(v_1) - p_i(v_2)\] <p>where \(p_i(v_1)\) and \(p_i(v_2)\) are the probabilities assigned to these tokens.</p> <p>A higher margin \(m_i\) indicates that the model is strongly favoring the top token over the runner-up, while a small margin suggests uncertainty. During iterative generation, margin-based sampling allows the model to commit tokens with high certainty while deferring those with ambiguous predictions for further refinement.</p> <h3 id="eb-sampler">EB Sampler</h3> <p>Entropy-bounded (EB) sampling typically commits tokens until an entropy budget/constraint is met (e.g. keep committing the lowest-entropy positions until the remaining masked positions have entropy above a target, or until a step-wise budget is exhausted).</p> <h3 id="pc-sampler">PC Sampler</h3> <p>Position-calibrated (PC) samplers add a position-aware calibration term to avoid pathological early commitments to “easy” regions (e.g. always unmasking near the prefix first). Without calibration, models may tend to unmask or commit tokens near the beginning of a sequence first, which can reduce diversity and flexibility in later steps.</p> <p>One way to implement this is to adjust the confidence score of each position with a position-dependent bias:</p> \[\tilde{s}_i = s_i + b(i)\] <p>where \(s_i\) is a base confidence score—such as the negative entropy \(-H_i\) or the raw probability \(c_i\) and \(b(i)\) is a bias or penalty term that depends on the position \(i\).</p> <p>By adding \(b(i)\), positions that are typically “easy” to commit (like the prefix) can be down-weighted, encouraging the sampler to consider less obvious positions first. The calibrated score \(\tilde{s}_i\) is then used to select which positions to commit or remask in the current iteration, promoting a more balanced and robust sequence generation process.</p> <h2 id="unmaskingremasking-strategies">Unmasking/Remasking Strategies</h2> <h3 id="static-low-confidence-remasking">Static Low-Confidence Remasking</h3> <p>In this masking regime, the denoising occurs over K steps. At each step, a fixed number of tokens N/K, where N is the size of the output, are unmasked, usually chosen by a confidence criterion. The low-confidence tokens are remasked.</p> <h3 id="dynamic-low-confidence-remasking">Dynamic Low-Confidence Remasking</h3> <p>A confidence threshold t is set. At a given denoising step, each token is unmasked only if it crosses the threshold t. If too few positions cross the threshold, then a minimum number of highest confidence tokens are unmasked.</p> <h3 id="dilute-unmasking-schedule">Dilute Unmasking Schedule</h3> <p>Instead of committing aggressively every step, the schedule “dilates” commits, by committing fewer tokens early, more in the middle, and fewer near the end, so that more global context can settle before locking in too many tokens.</p> <h2 id="speculative-decoding">Speculative Decoding</h2> <p>Speculative decoding in diffusion models is more challenging than in autoregressive models because generation can happen in parallel, and some models use block-based decoding. Gao et al. <d-cite key="gao2025selfspec"></d-cite> propose Self-Speculative Decoding (SSD) to address these challenges. SSD consists of two main phases: self-drafting and verification.</p> <h3 id="self-drafting">Self-Drafting</h3> <p>In this step, we construct a partial sequence that includes the prompt tokens, the tokens already committed in previous steps, and the currently masked positions. We then perform a single denoising step on this sequence to produce predictions for all masked tokens. This initial prediction step is referred to as self-drafting.</p> <p>If the model uses block-based decoding, the self-drafting procedure is applied within the current block as follows:</p> <ul> <li>Sort the positions in the block by a confidence measure.</li> <li>Select the top-k positions as candidates for verification.</li> <li>If there are fewer than \(k\) positions in the block, extend the selection into subsequent blocks until \(k\) positions are chosen.</li> </ul> <h3 id="verification">Verification</h3> <p>The \(k\) drafted tokens are then verified using a <strong>verification tree</strong>, which efficiently checks multiple token proposals at once. Tokens that pass verification are committed to the sequence, while tokens that fail are remasked and will be reconsidered in later iterations.</p> <p>This two-phase procedure allows speculative decoding to leverage parallel generation while maintaining reliability, committing only those tokens for which the model demonstrates sufficient confidence.</p> <h2 id="hyperparameters">Hyperparameters</h2> <h3 id="generation-length">Generation length</h3> <p>In autoregressive models, the generation length is dynamic for a given query, and generation stops when the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is generated. However, in diffusion models, tokens are predicted in parallel, so we typically allocate a maximum output length in advance, and discard tokens after the first <EOS> in the final output. This means that the generation length becomes a hyperparameter. Large generation lengths will be expensive due to the quadratic nature of self-attention.</EOS></p> <h3 id="number-of-denoising-steps">Number of denoising steps</h3> <p>The number of denoising steps K is also a hyperparameter. More steps give the model more chances to revise tokens. If optimizing for latency, K can be lower. If optimizing for task performance, K can be larger.</p> <h3 id="block-length">Block length</h3> <p>In practice, large output sequences are not amenable to being generated in one go. Hence semi-autoregressive (blockwise) diffusion is used. The output sequence is divided into blocks, and each block is generated sequentially. Within each block, tokens are generated through diffusion. Using this method, it becomes possible to perform diffusion across very long horizons.</p> <h3 id="temperature">Temperature</h3> <p>Similar to ARMs, temperature is a key hyperparameter in the dllm paradigm. Increasing temperature not only increases token diversity but also diversity in generation order (Gong et al., 2025) <d-cite key="gong2025diffucoder"></d-cite>.</p> <h3 id="classifier-free-guidance-cfg">Classifier-Free Guidance (CFG)</h3> <p>Classifier-free guidance (CFG) for discrete diffusion combines a conditional prediction with an “unconditional” prediction (often a null prompt or dropped conditioning).</p> <p>A common formulation is:</p> \[z_{\text{cfg}} = z_{\text{uncond}} + s \cdot (z_{\text{cond}} - z_{\text{uncond}})\] <p>where \(z_{\text{uncond}}\) is the unconditional prediction, \(z_{\text{cond}}\) is the conditional prediction, and \(s\) is the guidance scale.Increasing \(s\) strengthens adherence to conditioning, while smaller values typically yield more diverse outputs.</p> <h1 id="pitfalls--solutions">Pitfalls &amp; Solutions</h1> <h2 id="output-length-is-a-hyperparameter">Output Length is a Hyperparameter</h2> <p>Unlike autoregressive models, where the model continues generating output until an end of sequence (<code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>) token is seen, the output length in dLLMs is typically a hyperparameter that is chosen before generation.</p> <p>If the preset output length is too short for a given query, the model may skip steps, be very terse, or just fail entirely. If the preset output length is too long, neural degeneration may occur and the performance may drop. Longer output lengths also results in significantly more computation due to the quadratic nature of self-attention.</p> <p>These problems can be resolved if the model can dynamically adapt its output length for each query. To this end, Li et al. introduce the inventively named DAEDAL (<strong>D</strong>ynamic <strong>A</strong>daptive Length <strong>E</strong>xpansion for <strong>D</strong>iffusion L<strong>a</strong>rge <strong>L</strong>anguage Models), a training-free decoding technique that leverages <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token probabilities to dynamically adjust output length.</p> <p>DAEDAL has 2 stages, an initial global estimate and iterative local mask insertions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/DAEDAL-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/DAEDAL-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/DAEDAL-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/DAEDAL.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="initial-length-adjustment">Initial Length Adjustment</h3> <p>A short initial length (say 64 or 128 tokens) is assigned for generation. The model then goes through a single denoising step to produce its initial predictions. For the last few tokens of the sequence, the probabilities of the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token are taken and averaged. If the probability exceeds a threshold \(T\), then the current length is likely to be sufficient, otherwise the length is extended by a predetermined amount.</p> <p>This step is repeated until the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token confidence exceeds the threshold \(T\) or the global maximum length \(L\) is reached.</p> <h3 id="iterative-mask-insertion">Iterative Mask Insertion</h3> <p>During the denoising process, there might be local regions in the output where more elaboration is merited. An example would be a tricky code block where it might serve well to reserve more lines of code for it. To facilitate this, after each denoising step, the lowest-confidence masked positions are taken as expansion points. At these expansion points, multiple <code class="language-plaintext highlighter-rouge">[MASK]</code> tokens are inserted, thus dynamically allowing the model to expand generation in that output region.</p> <p>The authors show that DAEDAL leads to a massive jump in effective token ratio (proportion of tokens in the output sequence actually used for the output) compared to fixed length baselines.</p> <h2 id="denosing-steps-is-a-hyperparameter">Denosing Steps is a Hyperparameter</h2> <p>In most contemporary dLLMs, the number of denoising steps is also a hyperparameter. However, the optimal number of denoising steps depends on the query. The number of denoising steps is analogous to test-time compute in reasoning models, and leads to similar issues that scaling test-time compute encounters: (1) if the number of denoising steps is too low, then the model might not have arrived at the answer yet, and (2) if it is too high, the model may have overshot the answer. This phenomenon of drifting away from the correct answer that was generated during an intermediate denoising step is called temporal oscillation.</p> <p>In order to quantify temporal oscillation, we can use the <em>ever pass</em> rate metric. The ever pass rate is the accuracy of the model as measured across all denoising steps.</p> <p>Let \(N\) be the number of evaluation queries, \(K\) the number of denoising steps, and let<br/> \(\mathrm{Correct}(i,k) \in \{0,1\}\)<br/> indicate whether query \(i\) is solved correctly at step \(k\). Then:</p> \[\mathrm{EverPass} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}\left[\max_{1 \le k \le K} \mathrm{Correct}(i,k) = 1\right].\] <p>A query is counted as correct under this metric if the model produces the right answer at least once during the denoising trajectory. This is contrasted with the full pass rate, which measures accuracy only at the final denoising step:</p> \[\mathrm{FullPass} = \frac{1}{N} \sum_{i=1}^{N} \mathrm{Correct}(i, K).\] <p>Temporal oscillation can then be summarized as:</p> \[\mathrm{Oscillation} = \mathrm{EverPass} - \mathrm{FullPass}.\] <p>Ideally, the model would adaptively choose the number of denoising steps based on query difficulty, similar to thought budgeting in reasoning models. Wang et al. <d-cite key="wang2025timeisafeature"></d-cite> propose an interim approach that leverages intermediate outputs via the concept of temporal semantic entropy.</p> <h3 id="temporal-semantic-entropy">Temporal semantic entropy</h3> <p>For a given query \(q\), the model produces a sequence of intermediate answers across the \(K\) denoising steps. These answers are grouped into clusters based on semantic equivalence. Temporal semantic entropy (TSE) is defined as the entropy of the resulting cluster distribution:</p> \[\mathrm{TSE}(q) = - \sum_{c \in \mathcal{C}} p_c \log p_c,\] <p>where \(p_c\) is the proportion of intermediate answers assigned to cluster \(c\).</p> <p>If all intermediate answers fall into a single cluster, TSE is low. If the model oscillates between semantically distinct answers, TSE increases. Empirically, datasets on which the model performs poorly tend to exhibit higher mean TSE. Individual queries answered correctly typically have lower TSE than those answered incorrectly.</p> <p>TSE can be used to mitigate temporal oscillation and improve final accuracy. Wang et al. also introduce Temporal Self-Consistency Voting, a strategy that selects the final answer via majority vote across all denoising steps.</p> <h3 id="temporal-self-consistency-voting">Temporal Self-Consistency Voting</h3> <p>The model performs majority voting over the solution space. The output from each denoising step is normalized to a semantic form. Outputs are weighted based on which denoising step it came from. The weighting scheme can be:</p> <ol> <li>Fixed - all denoising steps carry the same weight</li> <li>Linear - the weight increases along with the number of steps, i.e the later steps are weighted more</li> <li>Exponential - similar to linear, except that the weight increases exponentially at the end of the sequence</li> </ol> <p>The authors observed that empirically, the exponential weighting scheme is the best performing.</p> <p>If the model overshot the answer due to a longer-than-needed denoising process, then temporal self-consistency can ensure that intermediate solutions still stand a chance of being picked up as the final answer.</p> <h2 id="block-length-is-a-hyperparameter">Block Length is a Hyperparameter</h2> <p>Diffusion over very large output sequences is suboptimal due to both latency issues and inability or difficulty in using KV-caches. Therefore in practice, it is customary to divide the output sequence into fixed-length blocks, where each block is generated sequentially but the tokens inside each block are generated via diffusion. Typically, the number of denoising steps is divided equally among each block.</p> <p>However, having a fixed block size comes with pitfalls. In their paper, Lu et al. <d-cite key="lu2025adablock"></d-cite> showcase two common inefficiencies: (1) late decoding overhead and (2) premature decoding error.</p> <h3 id="late-decoding-overhead">Late Decoding Overhead</h3> <p>Consider an output sequence broken down into 3 blocks. Let’s say the tokens in the second block are high confidence and easy to predict. However they are not predicted until all tokens in the first block have been predicted. If the tokens in the first block are also high confidence, then the model will wastefully perform denoising steps even when there are high confidence tokens outside the block boundary waiting to be unmasked. The authors term this as <em>late decoding overhead</em>.</p> <h3 id="premature-decoding-error">Premature decoding error</h3> <p>On the flip side, with block diffusion, all the tokens in the current block need to be predicted before moving on to the next block. This means that there is a chance of low confidence tokens being locked in prematurely. These tokens can cause the errors to propagate, as they will be used as context for generation of tokens in subsequent blocks. The authors term this as <em>premature decoding error</em>.</p> <p>In order to mitigate these issues, the model should ideally have dynamic block lengths. Lu et al. <d-cite key="lu2025adablock"></d-cite> introduce AdaBlock, an adaptive block size scheduler that leverages token confidence dynamics to draw semantically aware block boundaries.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceLandscape-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceLandscape-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceLandscape-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceLandscape.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The authors observed that the confidence dynamics of the output tokens vary across token positions. The physical landscape of output tokens can be divided into three types of regions:</p> <ol> <li>High-confidence plateau - this includes the already decoded tokens and mask tokens in their vicinity.</li> <li>Low-confidence floor - this includes token positions at the end of the output sequence</li> <li>Volatility bands - a band of positions where the confidence is fluctuating.</li> </ol> <p>Active decoding happens in the volatility band. The volatility band encodes local semantic structure. With this in mind, the block size can be made adaptive by dividing blocks based on semantic steps. A semantic step can be a span of tokens that are potentially self-contained, like a reasoning step, a line of code, or a statement.</p> <p>In order to identify semantic step boundaries, we can perform the following:</p> <ol> <li>Identify a set of delimiter tokens (newline, period, etc.) that can convey the end of a semantic unit</li> <li>Slide a window forward from the current generation position</li> <li>Pick the delimiter in the window with the highest confidence</li> <li>If the delimiter with the highest confidence surpasses a threshold \(T\), it is treated as the end of the semantic step, and the block size is adapted such that the block ends at the delimiter</li> <li>If no delimiters exist or none of them have confidence surpassing \(T\), then the default block size is used for the current generation</li> </ol> <p>Because each block corresponds to a self-contained semantic unit, the KV cache representations age more gracefully. This method also helps mitigate the late decoding overhead and premature decoding error issues.</p> <h2 id="confidence-threshold-is-a-hyperparameter">Confidence Threshold is a Hyperparameter</h2> <p>With confidence based decoding, typically the same threshold is employed throughout the generation process regardless if (1) it is an earlier denoising step or a later denoising step or (2) if it is an easy prompt or a difficult prompt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceCurves-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceCurves-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceCurves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ConfidenceCurves.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In practice, we could use a dynamic confidence threshold, because confidence dynamics vary through the diffusion phase. It has been observed that mean confidence is low during earlier denoising steps, peaks in the middle, and then becomes low during the final steps again, forming an up-side-down U-shape <d-cite key="shen2025osdt"></d-cite>. The curve of the U-shape depends on the type of task being performed. Shen et al. note that GSM8K’s curve is different from GPQA’s curve, but the confidence dynamics are similar for problems within each dataset. This indicates we can treat the confidence trajectory over denoising steps as a signature for a task type.</p> <p>The confidence signature for a given task can now be calculated by taking into account the confidence over all diffusion steps and blocks. The metric could be mean, median, etc.. This metric can then be treated as the confidence threshold for all instances of the given task. To prevent the confidence levels from being too restrictive, an upper bound \(B\) can be set.</p> <h2 id="information-is-not-carried-over-across-denoising-steps">Information is not carried over across denoising steps</h2> <p>Consider the partially unmasked sequence:</p> <p><code class="language-plaintext highlighter-rouge">Michael [MASK] to New York.</code></p> <p>At denoising step \(p\), suppose the top-3 predicted tokens for the masked position are:</p> <ul> <li><code class="language-plaintext highlighter-rouge">went</code>: 0.4</li> <li><code class="language-plaintext highlighter-rouge">moved</code>: 0.3</li> <li><code class="language-plaintext highlighter-rouge">galloped</code>: 0.1</li> </ul> <p>If the model uses confidence-based unmasking, the <code class="language-plaintext highlighter-rouge">[MASK]</code> token is not unmasked at step \(p\) because none of these candidates exceed the confidence threshold. However, at step \(p + 1\), the model restarts the prediction process from scratch. The information that <code class="language-plaintext highlighter-rouge">went</code> and <code class="language-plaintext highlighter-rouge">moved</code> had relatively high probabilities at the previous step is not retained.</p> <p>This leads to redundant and inefficient computation: the model repeatedly re-evaluates similar candidate sets without leveraging prior signals. Ideally, diffusion-style language models would propagate information across denoising steps, allowing the model to refine or reweight earlier hypotheses instead of discarding them at each iteration.</p> <h3 id="soft-masking">Soft Masking</h3> <p>Hersche et al. <d-cite key="hersche2025softmasked"></d-cite> introduce soft masking, which augments the discrete <code class="language-plaintext highlighter-rouge">[MASK]</code> token with continuous feedback. Instead of treating denoising as a binary decision (unmask or keep masked), each masked position is represented as an interpolation between the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding and a weighted combination of the top-k token embeddings.</p> <p>This can be expressed as:</p> \[soft embed = (1 - \alpha) embed([MASK]) + \alpha \sum_{j \in \text{top-}k} \tilde{p}_j embed(j)\] <p>where:</p> <ul> <li>\(\alpha\) is calculated by taking the negative entropy of the token probability vector and passing it through a sigmoid to obtain a weight between 0 and 1</li> <li>\(\tilde{p}_j\) is normalized over the top-k tokens so that the weights sum to 1</li> <li>\(k\) is typically 3–4 tokens</li> </ul> <p>If the token distribution is flat, \(\alpha\) is small and the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding barely changes. If the distribution is peaked, the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding is mostly replaced by the mean embedding of the top-k tokens.</p> <p>Hersche et al. report that applying soft masking on roughly 80% of denoising steps yields the best results, with most benefits occurring if applied during only the first 20% of steps.</p> <h3 id="credit-score">Credit Score</h3> <p>Wang et al. <d-cite key="wang2025creditdecoding"></d-cite> propose CreditDecoding, which maintains a credit value \(C_t(i, v)\) for each token position \(i\), each token \(v\), and each denoising step \(t\). The credit is updated as:</p> \[C_t(i, v) = \begin{cases} \gamma \cdot C_{t-1}(i, v) + p_t(i)^{\beta}, &amp; \text{if } v = v_t(i) \\ \gamma \cdot C_{t-1}(i, v), &amp; \text{otherwise} \end{cases}\] <p>where:</p> <ul> <li>\(v_t(i)\) is the top-1 token at position \(i\)</li> <li>\(p_t(i)\) is its probability</li> <li>\(\gamma\) is a decay factor in (0, 1)</li> <li>\(\beta\) is an exponent that amplifies mid-range probabilities</li> </ul> <p>Credit scores are incorporated into the logits of the next step:</p> \[\tilde{z}_t(i, v) = z_t(i, v) + \lambda \cdot C_t(i, v)\] <p>where \(\lambda\) controls the strength of the credit prior.</p> <h2 id="entropy-sink-phenomenon">Entropy Sink Phenomenon</h2> <p>Many diffusion LLMs still exhibit semi-autoregressive behavior. Gong et al. (2025) introduced the concept of local versus global AR-ness. Models adapted from autoregressive pretraining retain latent left-to-right dependencies, while models trained from scratch can be more flexible.</p> <p>Confidence-based unmasking—selecting the highest probability or lowest entropy tokens—tends to favor positions near the prefix. The first committed token biases the immediate right neighbor, creating an entropy sink. This bias induces a left-to-right autoregressive (AR) pattern.</p> <h3 id="degeneration-to-ar">Degeneration to AR</h3> <p>Although diffusion models can, in principle, generate tokens in any order, in practice generation often degenerates toward AR behavior. Two metrics quantify this:</p> <ul> <li>Local AR-ness: over a sliding window of length \(k\), the proportion of contiguous tokens generated reflects local AR behavior</li> <li>Global AR-ness: for each unmasked token, if it is the left-most unmasked token in the sequence, it is considered autoregressively generated. The proportion of such tokens gives global AR-ness</li> </ul> <p>Models continually pre-trained from AR bases retain higher AR-ness than models trained from scratch. Empirically, math generation shows high local AR-ness, while code generation shows lower AR-ness. This mirrors human behavior: math is typically solved sequentially, while code is edited in a non-linear fashion.</p> <p>Confidence-based remasking reinforces AR behavior because high-confidence tokens are usually near the prefix. Increasing the sampling temperature decreases AR-ness by flattening token distributions and increasing uncertainty in token commitments.</p> <h1 id="future-directions">Future Directions</h1> <h2 id="latency-play">Latency Play</h2> <p>Currently diffusion models are being promoted as a faster alternative to autoregressive models, as they support parallel token prediction. In practice, they need multiple denoising steps and often blockwise decoding. Latency can be reduced with fewer denoising steps, but it comes at the cost of performance</p> <h2 id="data-play">Data Play</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsA-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsA-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsA.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsB-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsB-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsB-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/ScalingLawsB.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If we assume a data-limited world where we run out of unique, quality data much sooner than we do compute, dLLMs pose an obvious advantage over ARMs. In data-constrained settings, dLLMs perform much better than ARMs due to better sample efficiency as dLLMs can be trained using repeated data over many epochs with the epochs remaining effective. Repeating data will become a common necessity in cases where data is constrained; however, in ARMs, repeating training data for more than a few epochs will lead to learning plateau and yield diminishing returns. ARMs will fit to the training data after a few epochs and reach saturation and/or overfitting, leading to degraded performance (Ni et al. 2025) <d-cite key="ni2025diffusion"></d-cite>.</p> <p>Diffusion models can be trained on the same data for way more epochs than AR; Prabhudesai et al. <d-cite key="prabhudesai2025diffusion"></d-cite> show that AR models can benefit from data repetition for only around ~4 epochs, while diffusion models can benefit from data repetition for up to ~100 epochs. They also show that diffusion’s best validation loss can occur at dramatically higher epoch counts (e.g., ~500 epochs vs ~50 for AR), suggesting diffusion keeps extracting signal where AR saturates/overfits.</p> <h2 id="reasoning-play">Reasoning Play</h2> <p>In practice, diffusion and auto-regressive modes are likely to co-exist in the foreseeable future. A plausible way to combine these modes together is to use diffusion for reasoning and AR for answer generation.</p> <p>Diffusion can be used for task decomposition, planning, outlines, where revision is natural. After the reasoning step, the AR model can produce a clean left-to-right final response,</p> <p>This aligns with what diffusion is naturally good at (global revision) while keeping AR’s strengths (fast sequential emission, stable length control, and efficient serving).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dllm/Tweets-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dllm/Tweets-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dllm/Tweets-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dllm/Tweets.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="the-case-against-dllms">The Case Against dLLMs</h1> <p>Several critiques have sprung up recently making the case against diffusion LLMs. We will elaborate a few of them in this section.</p> <h2 id="any-order-model-is-inefficient-and-ineffective">Any-order model is inefficient and ineffective</h2> <p>Masked diffusion can be viewed as an any-order model, as it is trained to predict arbitrary masked subsets in any order. However in practice, left-to-right and right-to-left orders are easier to learn. This is due to the Markovian nature of language <d-cite key="sun2025why"></d-cite> , where conditioning on nearby tokens provides more predictive power. An any-order model wastes model capacity on modeling hard and unnatural orders.</p> <h2 id="models-predict-marginals-not-a-joint-distribution">Models predict marginals, not a joint distribution</h2> <p>The denoiser predicts a distribution for each <code class="language-plaintext highlighter-rouge">[MASK]</code> position, conditioned on the unmasked tokens, but not the joint distribution over all masked positions. As a result, sampling multiple tokens in parallel has no guarantee of joint coherence.</p> <h2 id="training-inference-mismatch">Training-Inference mismatch</h2> <p>During training, the model is taught to predict from arbitrary masking patterns. However, during inference, denoising is typically done via confidence-based measures, which causes tokens closer to already unmasked tokens to be generated first, making it nearly autoregressive. Once inference becomes AR-like, most situations covered during training will not actually occur at inference time.</p> <h1 id="conclusion">Conclusion</h1> <p>In summary, at present diffusion LLMs are best viewed as a complementary modeling paradigm rather than a universal replacement for autoregressive models. They offer clear advantages in certain regimes: the masked diffusion objective naturally supports infilling and in-place revision, and recent results indicate that diffusion-based training can be significantly more data-efficient than autoregressive training when the amount of unique high-quality data is limited but can be repeated many times. At the same time, current masked diffusion formulations face structural limitations. They optimize over many token orders despite language exhibiting strong directional biases, and their decoding procedures often behave in a semi-autoregressive manner in practice, reducing the practical benefits of full any-order generation.</p> <p>Taken together, these observations suggest a more nuanced role for diffusion LLMs. In settings where data is the primary bottleneck and compute is relatively abundant, or where flexible infilling and structured editing are central requirements, dLLMs are a compelling choice. In contrast, for latency-sensitive, streaming, or purely left-to-right generation workloads, autoregressive models remain highly competitive and often preferable. A promising direction for future systems is therefore hybrid: using diffusion-style models for planning, reasoning, or structural refinement, and relying on autoregressive models for efficient, stable surface realization.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Diffusion large language models (dLLMs) provide an alternative to autoregressive Transformers, supporting parallel token generation and flexible infilling. They excel in structured, long-horizon, or data-constrained settings, though challenges remain with output length, denoising, and blockwise generation. Hybrid approaches combining diffusion for reasoning and AR for generation show promise.]]></summary></entry><entry><title type="html">Dynamics of Forgetting</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dynamics-of-forgetting/" rel="alternate" type="text/html" title="Dynamics of Forgetting"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dynamics-of-forgetting</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/dynamics-of-forgetting/"><![CDATA[<h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Most of our understanding of the world is built through a continuous interaction with our surroundings.</p> <p>This marks a very distinct difference between Artificial and Human intelligence: while modern AI models are stuck in a training / inference dichotomy, human intelligence (and, more generally, intelligence found in nature) blurs the line between the two, integrating a subconscious process of accumulation and processing of new information throughout our interaction with the world.</p> <p>This makes continual learning, to us, the most pressing problem in modern Artificial Intelligence.</p> <p>Most existent continual learning methods (<d-cite key="kirkpatrick_overcoming_2017"></d-cite><d-cite key="saha_gradient_2020"></d-cite><d-cite key="wang_training_2021"></d-cite><d-cite key="zenke_continual_2017"></d-cite><d-cite key="aljundi_memory_2018"></d-cite><d-cite key="farajtabar_orthogonal_2020"></d-cite><d-cite key="lopez_paz_gradient_2017"></d-cite><d-cite key="chaudhry_efficient_2019"></d-cite><d-cite key="riemer_learning_2019"></d-cite><d-cite key="shin_continual_2017"></d-cite>) revolve around the idea of making a specific subset of the training data <em>harder to forget.</em> Through either replay buffers, reduced learning rates on the parameters that are more important to interpret <em>a specific</em> subset of data, or orthogonalization of the updates to task-specific directions, the model is strongly penalized whenever relevant information is forgotten.</p> <p>All of these methods might work in specific multi-task scenarios, but the interactions that we, as humans, experience with the surrounding world are rarely ever categorizable in separate tasks.</p> <p>To begin pondering the possibility of human-like lifelong learning, we can’t rely on manually “saving” a small amount of specific subsets of data from catastrophic forgetting: we need a learning paradigm that allows to simply <em>not forget anything,</em> and learn new things without damaging previous knowledge.</p> <p>This blogpost will propose a strongly personal point of view on:</p> <ul> <li>Why do models forget, and whether we can distinguish between different types of forgetting;</li> <li>How does the core phenomenon of catastrophic forgetting look like from a spectral perspective;</li> <li>A possible approach towards more graceful optimization, to preserve learned structure.</li> </ul> <h2 id="why-do-models-forget">Why Do Models Forget</h2> <p>Catastrophic forgetting stems as an intrinsic consequence of how models are optimized. Gradient descent would like to minimize a loss function over a dataset of samples. Since this is usually computationally unfeasible, <em>Stochastic</em> Gradient Descent only concerns itself with a tiny, random portion of the data, and tunes the parameters to minimize the objective function on that subset of data. This means that each iteration has only one goal: optimize the parameters to predict the current batch as accurately as possible, as if no other batch ever existed.</p> <p>This leads to a constant flow of <em>construction</em> and <em>destruction</em> of structure in the weight matrices, that hopefully balances itself over enough training steps with large enough batches.</p> <p>To mitigate the damage of this oversimplification of the learning paradigm, two main practices stuck around over the years:</p> <ul> <li>Learning rate schedulers apply a very simple yet effective heuristic: if the model has been updated a lot of times, then probably it has built a lot of structure in the parameters that is worth preserving. Therefore, it makes sense to lower the magnitude of the updates to avoid destroying that structure.</li> <li>Better optimizers, like Adam, add two improvements: i) stabilize the trajectory of the updates, by accelerating descent through consistent paths, and decelerating through inconsistent ones; ii) optimize more strongly parameters that have been optimized fewer times.</li> </ul> <h2 id="setting-the-stage">Setting the Stage</h2> <p>This kind of techniques can be seen, from a continual learning point of view, as heuristics that try to balance the ability of a model to keep learning from new examples, while mitigating the amount of destruction that a new batch induces on the information that was previously learned. We like to think of this balance as a trade-off between <em>stability</em> and <em>plasticity</em> (<d-cite key="kim_achieving_2023"></d-cite><d-cite key="chen_stabilityplasticity_2023"></d-cite><d-cite key="jung_new_2023"></d-cite><d-cite key="lu_rethinking_2025"></d-cite>). The former represents the ability of a model to preserve learned structure (i.e: circuits, patterns…), the latter stands for the potential to learn new structures when needed (for example, when a change in the distribution of training samples occurs).</p> <p>In the traditional optimization scenario, this trade-off looks like a zero-sum game: stronger optimization (higher learning rate), increases plasticity at the cost of heavier forgetting; more delicate optimization (lower learning rate), increases stability by reducing the model’s ability to learn new information.</p> <p>We think that, to better navigate this tradeoff, a change in the optimization paradigm is needed. We like to think of this in terms of <em>gracefulness</em> of the updates. A graceful update is one that improves performance on the current batch without overwriting or degrading previously accumulated structure in the weights. There is a very fine line that keeps this from being an intrinsically contradictory statement: a good update should, by definition, update the model’s perception of the world (the training data) in a generalizable way; so how could it work without altering and refining previously learned connections and circuits?</p> <h2 id="observing-spectral-training-dynamics">Observing Spectral Training Dynamics</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover2-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover2-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To answer, we need to go deeper into the dynamics of learning and refining knowledge. Each optimization step computes update directions that are entirely specific to the current batch. Intuitively, we can think of an update as a mixture of two macro-types of operations:</p> <ul> <li><em>Refining present structure</em>: existing information encoded in the weights is updated according to its performance on the current batch;</li> <li><em>Building new structure</em>: a new encoding is built ad-hoc for the data in this batch.</li> </ul> <p>The first targets encoded structure that is strongly learned and optimized and tries to tweak it to make it work on the current batch. The second takes a weak, raw part of the network and begins to shape and refine it using the data in the current batch.</p> <p>We can try to observe these behaviors by decomposing with SVD both a weight matrix</p> \[W = U_w \Sigma_w V^T_w\] <p>and the update applied to it at each optimization step,</p> \[\Delta = U_\Delta \Sigma_\Delta V^T_\Delta\] <p>and plot the alignment between the $U_w$ vectors of the weight and the $U_\Delta$ vectors of the update.</p> \[M_{i, j} = \frac{|U_w^i \cdot U_\Delta^j|}{\left\lVert U_w^i \right\rVert \|U_\Delta^j\|}\] <p>An alignment between strong update directions and strong weight directions shows destructive interference of an already highly optimized structure in the weights. Alignment between top update directions and weak weight directions represents the second type of update, in which new structure is being built to encode new information.</p> <p>Let’s now observe an EMA of this matrix while training a 2-layer MLP on MNIST with Adam.</p> <div class="l-page"> <iframe src="/blockpost_flashMD/assets/html/2026-04-27-dynamics-of-forgetting/video1.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <div class="caption"> Left, top: singular values of the weight matrix; Left, bottom: singular values of the update proposed by Adam for the same matrix; Center: heatmap of alignment between left singular vectors of the update and left singular vectors of the weight; Right: Validation Accuracy on MNIST. The high values concentrated on the top left of the heatmap show strong alignment between the most important spectral components of the update and the weight. In other words, the optimizer focuses mostly on updating structures which are already very strong. </div> <p>We clearly see that, after an initial stage of randomness, the training quickly converges to always updating a few directions in weight space.</p> <p>This behavior is even more visible when we reduce the hidden dimension of the linear layer analyzed to 2. The training clearly switches from a <em>structure building</em> state, in which updates are not necessarily aligned with the top directions in the weight matrix, to a <em>refining</em> state, in which the update assigns its focus proportionately to the importance of each direction. This switch also synchronized with the moment in which the validation accuracy’s derivative drops below 1 and the model enters a training phase of much slower improvement.</p> <div class="l-page"> <iframe src="/blockpost_flashMD/assets/html/2026-04-27-dynamics-of-forgetting/video2.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="two-types-of-forgetting">Two Types of Forgetting</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover3-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover3-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/cover3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So what causes forgetting, exactly?</p> <p>Our previous distinction between two macro-types of updates leads us to think about forgetting in terms of two easily distinguishable patterns. As an example, let’s think of the case of training a simple MNIST classifier.</p> <ol> <li> <p><em>The current sample destroys structure that was learned on different data.</em></p> <p>In our example, this could look like the model encountering a new digit, and applying to it a series of weights built to recognize other digits, leading to a wrong prediction. The weight update based on this new sample will interfere with the existing circuits built from the previous digits, overwriting weights that were needed to recognize them.</p> </li> <li> <p><em>The model learns entirely new structure on the current batch of data.</em></p> <p>Here, the model might see a new digit and build ad-hoc structure to encode / recognize it. These new paths might now incorrectly fire for other digits as well, interfering with their prediction.</p> </li> </ol> <p>In both cases, the problem consists in the model learning something on the current data which doesn’t apply to different data, but the approach to mitigate these two phenomena is drastically different. The first can be solved by applying updates that preserve the current structure. The second cannot.</p> <p>Most continual learning methods, as well as the rest of this blogpost, are only concerned with the first <em>(destructive)</em> type of forgetting, which is much easier to address.</p> <p>We believe that analyzing and determining how much of each of these types of forgetting actually occurs at a given time in a continual learning procedure is one of the most interesting open problems in continual learning.</p> <p>To observe spectral behavior in presence of strong forgetting, we can look at the alignment matrix when switching training tasks from MNIST to Fashion-MNIST.</p> <div class="l-page"> <iframe src="/blockpost_flashMD/assets/html/2026-04-27-dynamics-of-forgetting/video3.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <p>It seems that, when switching to the new task of Fashion-MNIST:</p> <ul> <li>The top 3 components of the update switch from targeting already strong structures to building weaker ones (in the first 3 columns of the matrix, the “heat” disappears from the top and shifts downward). This would represent type 2, or <em>constructive</em> forgetting.</li> <li>The remaining important components of the update keep targeting already strong structure (a strong heat spot remains visible in the top-left sector of the matrix). This would seem to indicate type 1, or <em>destructive</em> forgetting.</li> </ul> <p>So, are we able to do something to at least mitigate the <em>destructive</em> forgetting that happens at each optimization step?</p> <h2 id="towards-spectrally-graceful-updates">Towards (Spectrally) Graceful Updates</h2> <p>Given this (very raw and certainly incomplete) analysis, we can try to intervene by observing the spectral characteristic of each weight matrix, and manually tweaking each update to make it more graceful. Ultimately, we want to <em>refuse</em> components of the update that wrongfully penalize weights that were optimized for very different data.</p> <p>Let’s start with an oversimplification and divide the spectral decomposition of a matrix into low-rank (or strong) components and high-rank (or weak) ones. Let us also assume that a low-rank update on a direction changes it strongly, while a high-rank update only refines it slightly.</p> <p>Now we can define a policy that decides which components of an update can be allowed, and which ones need to be refused as potentially destructive, based on their alignment with the corresponding singular vectors of the weight matrix.</p> <p>A very simple example policy could look as follows:</p> <ul> <li>Strong updates on strong directions are clearly destructive of previously accumulated knowledge: we want to refuse them;</li> <li>Strong updates on weak directions are exactly what we want: they build new structure for the new data without destroying old ones;</li> <li>Weak updates on strong directions are also probably ok, as they only slightly refine strongly optimized weights;</li> <li>Weak updates on weak structures are hard to characterize: they might be noise, or very low-importance updates in general. For this example, we refuse them.</li> </ul> <p>We can implement this policy and immediately see the structure we are enforcing in the alignment matrix: our policy divides the matrix into four quadrants and only allows updates belonging to two of the four possible combinations (top-right and bottom-left).</p> <div class="l-page"> <iframe src="/blockpost_flashMD/assets/html/2026-04-27-dynamics-of-forgetting/video4.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="going-deeper">Going Deeper</h2> <p>A matrix-wise intervention on modern deep learning models can be extremely impactful, with the <a href="https://kellerjordan.github.io/posts/muon/">Muon optimizer</a> being the prime example. Yet, we can’t but notice that models will inevitably encode information in the composition of their layers, their <em>circuitry</em>, rather than just atomically in the weights of each. This exposes a clear weak point in our technique so far: it only tries to identify and preserve structure in each matrix individually, but it never looks at their composition.</p> <p>While we’re still far from having a practical solution to this, we have some initial approaches that seem to bring improvements over the previous update conditioning function (or <em>post-conditioner</em>, as we like to call it).</p> <p>Our deeper approach shares the same core idea as the previous one, but it does so taking into account a key element: cross-layer alignment.</p> <p>Let</p> \[W^{(l)} = U^{(l)}\Sigma^{(l)}V^{(l)T} , \qquad \Delta^{(l)} = U_\Delta^{(l)}\Sigma_\Delta^{(l)}V_\Delta^{(l)T}\] <p>be a weight matrix for layer $l$ in a model and the corresponding update proposed by a chosen optimizer, such as AdamW or Muon</p> <p>In a matrix-wise post-conditioner, we use some function $f(W^{(l)},\Delta^{(l)})$ to re-scale the singular sub-spaces spanned by $\Delta^{(l)}$, so that the update is not destructive.</p> <p>For this post-conditioner, we instead re-scale them based on a function</p> \[f_{comp}(W^{(l-1)},\Delta^{(l-1)}, W^{(l)},\Delta^{(l)}, W^{(l+1)},\Delta^{(l+1)}).\] <p>Of course, using $l+1$ and $l-1$ implies a notion of ordering of the weight matrices, which is definitely not trivial to define in a more complex network, such as an LLM.</p> <p>The intuition of $f_{comp}$ is that the subspaces of $\Delta^{(l)}$ are now also rescaled based on the alignment matrices</p> \[T^{(l+1)} = V^{(l+1)T}U^{(l)} , \qquad T^{(l-1)} = V^{(l)T}U^{(l-1)}.\] <p>The core insight is as follows: if incoming and outgoing spectral spaces align, then there is sign of structure in the weights, and we don’t want to be updating it too aggressively, so we reduce the magnitude of the update based on how aligned they are.</p> <p>So, can these style of approaches actually mitigate destructive updates and help in the continual learning setting?</p> <h2 id="experimental-results">Experimental Results</h2> <p>Truth is, we don’t really know yet. In some specific scenarios, our preliminary results seemed extremely encouraging; in others, lackluster.</p> <p>We present here a small collection of results, to be taken with a nice pinch of salt, which highlight the intrinsic difficulty of a proper navigation of the Stability-Plasticity trade-off.</p> <p>We experiment with our post-conditioning technique across two distinct scenarios:</p> <ul> <li> <p><strong>Toy model: 2-layer MLP trained on MNIST</strong></p> <p>We train a tiny 2-layer MLP from scratch on two subsequent classification tasks. This lets us study how our method’s hyperparameters behave across settings when training from scratch.</p> </li> <li> <p><strong>LLMs: sequential SFT across 3 datasets</strong></p> <p>We fine-tune a Transformer-based Large Language Model with Supervised Fine-Tuning on three different datasets in sequence. We evaluate accuracy on all three datasets throughout training, and measure forgetting and learning metrics for all checkpoints. This setting lets us monitor how the technique scales and how it interacts with sparsity in large models.</p> </li> </ul> <p>In our experiments, we call <em>softmask</em> the matrix-wise post-conditioner, and <em>compsoft</em> its circuit-aware counterpart.</p> <h3 id="toy-model---mlp-on-mnist">Toy Model - MLP on MNIST</h3> <p>We train on subsets of MNIST, where each task is binary classification over two digits. The first task is classifying digits $1$ vs. $2$, and the second task is classifying digits $3$ vs. $4$. Once accuracy reaches $0.95$ on the first task, the second task is activated and the model is trained on the new digit pair. We train with a constant learning rate.</p> <p>Here we report <strong>AccΣ := Acc1 + Acc2</strong> , representing the sum of the accuracies of the two tasks at the end of the training steps.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig1-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig1-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="llms---sequential-sft-on-qa-tasks">LLMs - Sequential SFT on Q&amp;A Tasks</h3> <p>We perform Supervised Fine-Tuning on <strong>Qwen3 0.6B</strong> (<d-cite key="yang2025qwen3"></d-cite>) and <strong>Llama 3.2 1B</strong> (<d-cite key="grattafiori2024llama"></d-cite>) on a sequence of three distinct datasets. As training progresses, we evaluate the model’s capabilities on all tasks for every checkpoint. The resulting accuracies let us gauge how well training navigates the robustness–plasticity tradeoff: an ideal model learns strongly while retaining high accuracy on previous tasks.</p> <p>We train sequentially on three Q&amp;A datasets (inspired by <d-cite key="shenfeld2025rl"></d-cite>):</p> <ul> <li>Open Reasoner-zero (<d-cite key="hu2025open"></d-cite>)</li> <li>SciKnowEval (<d-cite key="feng2024sciknoweval"></d-cite>)</li> <li>HellaSwag (<d-cite key="zellers2019hellaswag"></d-cite>)</li> </ul> <p>Here, we plot the validation accuracy of the model on each of the three datasets throughout the training, when using a fixed learning rate of $4 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And the same with a learning rate of $1 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig2b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It is crucial to notice how the post-conditioning method strongly outperforms Adam, when used with a higher learning rate, but it starts losing when the learning rate is tuned to a level that is optimal for the task.</p> <p>We can get an idea of the model’s ability to navigate the stability-plasticity tradeoff by plotting the last task’s accuracy (plasticity) vs an average of the accuracies for the previous tasks (stability) at the end of the training.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-dynamics-of-forgetting/fig3b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="final-notes">Final Notes</h2> <p>We are publishing this blogpost with the main intent of conveying our point of view on continual learning: how it is a core distinguishing factor between human and artificial intelligence, how catastrophic forgetting plays a central role in the optimization procedure of a model, even outside of traditional sequential multi-task scenarios, and how we believe that forgetting is made of two very different dynamics, one of which is commonly addressed by most CL methods, the other still largely unexplored.</p> <p>We argue that modern continual learning methods cannot rely on knowing what knowledge must be retained, and should instead be capable of preserving existing structure in the weights.</p> <p>We test a couple early implementations in two distinct settings, where we find some mixed results, which we hope will stimulate discussion in the community and help us develop on this first step towards producing gracefully updating, continually learning models. Overall, we think the point stands: we must find a new way to build non-destructive updates that respect the model’s learned structure. We can’t wait to know what you think about it.</p> ]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We analyze catastrophic forgetting through spectral decompositions of weights and updates, revealing when optimization refines existing circuits versus builds interfering new ones. Leveraging this, we design spectral techniques that suppress destructive update components while preserving structure.]]></summary></entry><entry><title type="html">OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics</title><link href="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/flashmd_old/" rel="alternate" type="text/html" title="OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://pascalknoll.github.io/blockpost_flashMD/blog/2026/flashmd_old</id><content type="html" xml:base="https://pascalknoll.github.io/blockpost_flashMD/blog/2026/flashmd_old/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Molecular Dynamics (MD) is often called the “computational microscope” of modern science. By simulating atoms obeying Newton’s laws, we can watch proteins fold, batteries charge, and materials fracture—all at the atomic scale. If we can simulate the atoms, we can predict the material.</p> <p>But there’s a brutal trade-off: <strong>accuracy versus time</strong>.</p> <p>Machine Learning Interatomic Potentials (MLIPs) recently solved the first bottleneck—calculating forces—by replacing expensive quantum calculations with learned approximations. But a second, more stubborn barrier remains: the <strong>femtosecond prison</strong>. Traditional integrators must take tiny time steps ($\sim10^{-15}$ s) to remain stable<d-cite key="leach2001timestep"></d-cite>. To observe a biological process lasting milliseconds requires <strong>trillions of sequential steps</strong>—each depending on the last.</p> <p>FlashMD shatters this chain. By directly predicting the system’s state over time steps $1-2$ orders of magnitude higher than the stability limit of traditional integrators, it shifts the paradigm from <strong>simulation to emulation</strong>. Instead of numerically integrating forces at every femtosecond, FlashMD learns to leap forward in time, promising to collapse simulations that once took weeks into hours.</p> <p><strong>But here’s the catch:</strong> Can a learned emulator remain physically stable?</p> <p>Traditional integrators come with mathematical guarantees—energy conservation, time-reversibility, symplectic structure. FlashMD trades these guarantees for speed. It learns dynamics purely from data. In this post, we’ll explore FlashMD’s architecture and ambitions—and then critically examine whether learned emulation can escape the femtosecond prison without losing physical validity. Through an exploratory study, we’ll see where it breaks down and what that reveals about the future of learned simulators.</p> <hr/> <h1 id="molecular-dynamics-fundamentals">Molecular Dynamics Fundamentals</h1> <p>Molecular Dynamics (MD) is the computational engine behind our understanding of matter in motion. At its core, MD follows a straightforward recipe: place $N$ atoms in a box, calculate the forces between them, and step forward in time using Newton’s equations.</p> <p>The challenge lies in the scale. As shown in the workflow below, every MD simulation is built around a <strong>core loop</strong> that must be executed millions to billions of times:</p> <ol> <li><strong>Calculate forces</strong> on every atom (Step 2)</li> <li><strong>Integrate equations of motion</strong> to update positions and velocities (Step 3)</li> <li>Repeat for $10^6$ to $10^9$ steps</li> </ol> <p>[INSERT FIGURE HERE]</p> <p>This repetition is fundamental, not a limitation. To extract meaningful thermodynamic properties—diffusion coefficients, phase transitions, reaction rates—we need trajectories long enough to sample the system’s accessible states. A single nanosecond of physical time typically requires around a million integration steps.</p> <p>This raises a natural question: <strong>which step consumes the most time?</strong></p> <p>XXX Illustration of a MD Simulation witht the steps</p> <h2 id="the-hamiltonian-view-of-atomic-motion">The Hamiltonian View of Atomic Motion</h2> <p>To understand where the computational bottleneck lies, we need to look inside the simulation loop. At each step, the system evolves according to Hamilton’s equations of motion—the fundamental laws governing how atoms move.</p> <p>The core idea is simple: <strong>forces come from energy gradients</strong>. We describe the system’s total energy using the Hamiltonian $H$, which splits into two parts:</p> \[H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}\] <p>where \(\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N\) are atomic positions, \(\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N\) are momenta, and \(V(\mathbf{Q})\) is the potential energy surface (PES).</p> <p>Hamilton’s equations tell us how the system evolves:</p> \[\frac{d\mathbf{q}_i}{dt} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial V}{\partial \mathbf{q}_i}\] <p>The second equation is crucial: <strong>the force on each atom is the negative gradient of the potential energy</strong>. To simulate the system, we need two things:</p> <ol> <li>A way to compute $V(\mathbf{Q})$ and its gradient $\nabla V$.</li> <li>A way to discretize continuous time into finite steps $\Delta t$.</li> </ol> <p><strong>For the second requirement, we use numerical integration.</strong></p> <h3 id="the-velocity-verlet-algorithm">The Velocity Verlet Algorithm</h3> <p>To solve these continuous equations on a computer, we discretize time using the Velocity Verlet integrator:</p> \[\begin{aligned} \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t \\ \mathbf{q}_i &amp;\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\ \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t \end{aligned}\] <p>This algorithm is <strong>symplectic</strong>, meaning it approximately conserves the Hamiltonian even with finite $\Delta t$. But there’s a catch: stability requires $\Delta t \sim 0.5\text{–}1$ femtoseconds<d-cite key="leach2001timestep"></d-cite>. Larger steps cause energy drift and numerical explosions. <strong>This is the femtosecond prison</strong>—the fundamental timestep barrier that limits all classical MD.</p> <p>Every loop iteration requires:</p> <ul> <li><strong>Computing forces</strong> via $\nabla V$ (expensive)</li> <li><strong>Taking a tiny timestep</strong> (limiting)</li> </ul> <p>We’ll address these bottlenecks in sequence, starting with force computation.</p> <h2 id="why-ab-initio-molecular-dynamics-is-expensive">Why <em>ab Initio</em> Molecular Dynamics Is Expensive</h2> <p>The first bottleneck is evaluating $V(\mathbf{Q})$ and its gradient. Historically, this forced a painful compromise:</p> <ul> <li> <p><strong><em>Ab initio</em> methods</strong> (e.g., Density Functional Theory) solve quantum mechanics to compute forces with chemical accuracy. But they require solving the electronic structure problem at every step, with computational cost scaling as $O(N^3)$ or worse—and large constant factors that make even small systems expensive<d-cite key="zhang2018deep"></d-cite>.</p> </li> <li> <p><strong>Classical force fields</strong> use handcrafted functions (harmonic springs, Lennard-Jones potentials, Coulomb terms) that scale linearly with system size. But these predefined functional forms—fixed at design time—cannot adapt to bond breaking, chemical reactions, or complex polarization effects that require quantum mechanical treatment (XXX Source).</p> </li> </ul> <p>For decades, this accuracy-efficiency trade-off defined the field. Quantum accuracy meant tiny systems; large-scale simulations meant sacrificing chemistry.</p> <p>Machine Learning Interatomic Potentials (MLIPs) changed this.</p> <hr/> <h1 id="machine-learned-interatomic-potentials-solving-the-force-bottleneck">Machine-Learned Interatomic Potentials: Solving the Force Bottleneck</h1> <p>We’ve identified two computational bottlenecks in MD. Let’s tackle the first one: <strong>computing forces</strong>.</p> <p>Recall that every timestep requires evaluating $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$—the gradient of the potential energy surface. Historically, this meant choosing between quantum accuracy (expensive) or classical speed (inaccurate). MLIPs broke this trade-off.</p> <h2 id="the-mlip-breakthrough">The MLIP Breakthrough</h2> <p>The core idea is simple: <strong>replace quantum calculations with a learned function</strong>.</p> <p>An MLIP is a neural network that maps atomic positions $\mathbf{Q}$ and atomic numbers $\mathbf{Z}$ directly to potential energy:</p> \[V_\theta(\mathbf{Q}) \approx V_{\text{QM}}(\mathbf{Q})\] <p>Forces are then obtained via automatic differentiation:</p> \[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} V_\theta(\mathbf{Q})\] <p>This bypasses the $O(N^3)$ cost of solving the electronic structure problem at every step.</p> <h3 id="architecture-graph-neural-networks">Architecture: Graph Neural Networks</h3> <p>Modern MLIPs—such as SchNet<d-cite key="schutt2017schnet"></d-cite>, NequIP<d-cite key="batzner2022nequip"></d-cite>, and MACE<d-cite key="batatia2022mace"></d-cite>—use <strong>Graph Neural Networks</strong> (GNNs) that:</p> <ol> <li><strong>Encode molecular structure naturally</strong>: Atoms are nodes, interactions are edges</li> <li><strong>Respect physical symmetries</strong>: Predictions are invariant to translation, rotation, and atom permutation</li> <li><strong>Learn many-body interactions</strong>: Message-passing layers aggregate information from neighboring atoms</li> </ol> <p>The training objective fits both energies and forces jointly:</p> \[\mathcal{L}(\theta) = \lambda_E \|V_\theta - V_{\text{DFT}}\|^2 + \lambda_F \|\mathbf{F}_{\text{pred}} - \mathbf{F}_{\text{DFT}}\|^2\] <p>Training on forces directly improves generalization: force supervision provides richer gradient information and helps the model handle out-of-distribution configurations<d-cite key="chmiela2018sgdml"></d-cite>.</p> <h3 id="impact-quantum-accuracy-at-classical-speed">Impact: Quantum Accuracy at Classical Speed</h3> <p>The speedup is dramatic. Where DFT takes <strong>minutes</strong> per force evaluation, MLIP inference takes <strong>milliseconds</strong>—a <strong>1000× improvement</strong><d-cite key="he2025mlipsbio"></d-cite>. This has enabled:</p> <ul> <li><strong>Larger systems</strong>: Million-atom simulations that were previously impossible</li> <li><strong>Longer timescales</strong>: Microsecond trajectories with quantum accuracy</li> <li><strong>New applications</strong>: Drug discovery, battery materials, catalysis<d-cite key="unke2021spookynet"></d-cite></li> </ul> <p>MLIPs have fundamentally changed what’s computationally feasible in molecular simulation.</p> <h2 id="the-remaining-challenge-the-femtosecond-prison">The Remaining Challenge: The Femtosecond Prison</h2> <p>MLIPs solved the force bottleneck—<strong>Bottleneck #1</strong>. But <strong>Bottleneck #2</strong> remains stubbornly unsolved.</p> <p>As we established earlier, classical integrators require $\Delta t \sim 10^{-15}$ s to maintain numerical stability. This means simulating a microsecond—the timescale of protein folding or molecular recognition—still requires $10^9$ sequential steps, regardless of how fast we can compute forces.</p> <p><strong>Even with instantaneous force predictions, the serial nature of integration makes long-timescale phenomena computationally intractable.</strong></p> <p>To escape the femtosecond prison, we cannot simply accelerate the integrator. We must <strong>bypass it entirely</strong>—replacing step-by-step integration with direct trajectory prediction.</p> <p>This is where FlashMD enters. &lt;!– # Machine-Learned Interatomic Potentials: Solving the Force Bottleneck We have established that we need billions of time steps to simulate meaningful biological or material phenomena. This brings us to the second half of the computational burden: the cost of a single step.</p> <p>As derived in the Hamiltonian framework, every single step requires us to evaluate the gradient of the potential energy surface: $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$.</p> <h2 id="the-mlip-breakthrough-1">The MLIP Breakthrough</h2> <p>At its core, an MLIP is a regression framework that approximates the Potential Energy Surface (PES) by learning a mapping $\mathcal{F}_\theta: (\mathbf{Z}, \mathbf{Q}) \to \mathbb{R}$ from atomic numbers and coordinates directly to the scalar potential energy. This effectively bypasses the $O(N^3)$ cost of solving the electronic structure explicitly.</p> <p>Unlike classical force fields, MLIPs leverage deep neural networks—typically Graph Neural Networks (GNNs) or Message Passing Neural Networks (MPNNs)—to serve as universal approximators of the quantum mechanical interaction <d-cite key="he2025mlipsbio"></d-cite>. This allows them to capture complex, non-local many-body effects that classical approximations inherently miss.</p> <p>Crucially, MLIPs enforce physical consistency by defining atomic forces as the exact negative gradient of the predicted energy with respect to atomic positions via automatic differentiation:</p> \[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} E_{\text{pred}}(\mathbf{Q})\] <p>The training objective is therefore a multi-task learning problem. We optimize the network parameters $\theta$ to minimize a composite loss against ground-truth quantum mechanical labels (typically from DFT):</p> \[\mathcal{L}(\theta) = \lambda_E \|E_{\text{pred}} - E_{\text{DFT}}\|^2 + \lambda_F \|\underbrace{-\nabla_{\mathbf{Q}} E_{\text{pred}}}_{\mathbf{F}_{\text{pred}}} - \mathbf{F}_{\text{DFT}}\|^2\] <p>By training on high-quality snapshots of molecular configurations—generated via random sampling or active learning—MLIPs can capture complex, many-body interactions that classical methods simply cannot see. XXX source</p> <h2 id="why-faster-forces-are-still-not-enough">Why Faster Forces Are Still Not Enough</h2> <p>Machine Learning Interatomic Potentials (MLIPs) have revolutionized the field by reducing the computational cost of force calculation ($F$) by orders of magnitude compared to DFT. However, they leave the fundamental architectural flaw of MD untouched: the integrator bottleneck.</p> <p>Classical integrators like Velocity Verlet face a hard physical speed limit. To maintain numerical stability and energy conservation, the time step $\Delta t$ must resolve the fastest atomic vibrations in the system—typically the oscillation of hydrogen bonds. This confines simulations to the femtosecond scale ($\Delta t \approx 10^{-15} \text{s}$), regardless of how fast the force model is.</p> <p>This creates a massive discrepancy between simulation time and biological reality. To simulate a mere microsecond of physical time—relevant for protein folding or drug binding—we must perform one billion sequential steps:</p> \[N_{\text{steps}} = \frac{10^{-6} \text{ s}}{10^{-15} \text{ s}} = 10^9 \text{ steps}\] <p>We are effectively trapped in a “femtosecond prison.” Even with instant force predictions, this serial dependency makes long-timescale phenomena computationally intractable. To escape this, we cannot simply accelerate the integrator; we must bypass it entirely. –&gt;</p> <hr/> <h1 id="flashmd-escaping-the-femtosecond-prison">FlashMD: Escaping the Femtosecond Prison</h1> <p>FlashMD introduces a transformative approach: rather than incrementally integrating forces like a standard force field, it operates as a direct trajectory predictor.</p> <p>Instead of acting as a “middleman”—predicting energy to derive forces for a classical integrator—FlashMD learns the dynamical map directly from simulation data:</p> \[\mathcal{G}_\theta: (\mathbf{Q}_t, \mathbf{P}_t) \to (\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t})\] <p>This paradigm shift enables the model to predict the system’s next state in a single forward pass, replacing hundreds of small integration steps and allowing for strides 1-2 magnitudes larger than the stability limit of numerical integrators.</p> <p>XXX Here Illustration of Classical MD Loop vs. FlashMD Loop</p> <h2 id="architecture-and-design-principles-of-flashmd">Architecture and Design Principles of FlashMD</h2> <p>To realize the dynamical map $\mathcal{G}_\theta$ defined above, FlashMD implements a flexible deep learning pipeline. While the current implementation defaults to a specific Transformer backbone, the architecture is fundamentally modular.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-flashmd/flashmd_architecture-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/flashmd_architecture-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/flashmd_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-flashmd/flashmd_architecture.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can view FlashMD as a wrapper that prepares atomic data for any powerful graph neural network:</p> <ol> <li><strong>Input: Embedding the Atomic State</strong> The raw atomic state is converted into a graph representation. <ul> <li>The current positions $\mathbf{Q}$ and momenta $\mathbf{P}$ are encoded into node and edge features of a molecular graph.</li> <li><strong>Mass Scaling:</strong> Input momenta are normalized ($\tilde{\mathbf{p}}_i = \mathbf{p}_i / \sqrt{m_i}$) to prevent heavy atoms from dominating the loss, ensuring the model captures fast hydrogen vibrations as accurately as heavy-atom motions.</li> </ul> </li> <li> <p><strong>Backbone: Point-Edge Transformer (PET)</strong> The graph is processed by a message-passing network to extract local geometric features. FlashMD uses the Point-Edge Transformer (PET) by default, which updates edge and node representations via attention mechanisms. At inference time, optional filters can be applied: momentum rescaling for energy conservation, thermostat/barostat integration for ensemble control, and random rotations to mitigate symmetry-breaking artifacts.</p> </li> <li> <p><strong>Output: Multi-Head Prediction</strong> Two separate MLP heads branch from the final node representations to predict the update:</p> <ul> <li>Momentum Head: Predicts $\mathbf{p}_i(t + \Delta t)$.</li> <li>Displacement Head: Predicts $\Delta \mathbf{q}_i(t + \Delta t)$.</li> </ul> </li> </ol> <p>Theoretically, you could swap the backbone for any modern GNN. However, molecular dynamics imposes a strict “non-negotiable” constraint that narrows our choices significantly: E(3) Equivariance.</p> <h2 id="the-symmetry-challenge-e3-equivariance">The Symmetry Challenge: E(3) Equivariance</h2> <p>Imagine simulating a water molecule. If you rotate your entire simulation box by 90 degrees, the physics must remain identical. The potential energy should not change, and the force vectors must rotate by exactly 90 degrees to match the atoms.</p> <p>Standard neural networks see coordinates as simple lists of numbers; they do not inherently “know” that a rotated molecule is the same physical object. If we denote our model as $\mathcal{F}$ and a rotation matrix as $\mathcal{R}$, the model must satisfy:</p> \[\mathcal{F}(\mathcal{R} \cdot \mathbf{Q}) = \mathcal{R} \cdot \mathcal{F}(\mathbf{Q})\] <p>If a model fails this test, it might predict that a molecule flies apart simply because it was rotated to face “North” instead of “East.”</p> <p>There are generally two ways to solve this in Deep Learning:</p> <ol> <li> <p>Hard Constraints (e.g., NequIP<d-cite key="NequIP_Batzner2022"></d-cite>, MACE<d-cite key="MACE_ALLEGRO_leimeroth2025machine"></d-cite>): Bake geometric algebra (spherical harmonics) directly into the network layers. This guarantees exact equivariance but is computationally expensive.</p> </li> <li> <p>Soft Constraints (e.g., SchNet<d-cite key="schutt2017schnet"></d-cite>, PET<d-cite key="PET_pozdnyakov2023smooth"></d-cite>): Use a flexible, standard architecture and “teach” it symmetry through data augmentation or frame averaging.</p> </li> </ol> <h2 id="the-point-edge-transformer-pet">The Point-Edge Transformer (PET)</h2> <ul> <li>is a rotationally unconstrained and transformer-based graph neural network</li> <li>PET maintains feature vectors (or messages) f_l ij for every directed bond between atoms i and j that lie within a specified cutoff radius.</li> <li>These intermediate representations are updated at each message-passing layer by a transformer</li> <li>outputs are subsequently interpreted as the new set of outbound messages from atom i to each neighbor j</li> <li>geometric information and chemical species are also incorporated</li> <li>A feed-forward NN is used to obtain the desired output/target property</li> <li>PET architecture imposes no explicit rotational symmetry constraints, but learns to be equivariant through data augmentation.</li> <li>This unconstrained approach yields high theoretical expressivity: even a single layer of the model acts as a universal approximator featuring virtually unlimited body order and angular resolution</li> </ul> <p>The PET architecture<d-cite key="PET_pozdnyakov2023smooth"></d-cite> reimagines atomic interactions through the lens of modern Transformers. While standard message-passing GNNs aggregate neighbor information via simple summation, PET introduces a richer mechanism that naturally captures <strong>many-body correlations</strong>—the complex ways in which multiple neighbors jointly influence a central atom.</p> <p>(XXX add illustration of PET)</p> <p>The key innovations are:</p> <ol> <li> <p><strong>Tokenization of neighbors.</strong> For each central atom, every neighbor within a cutoff radius $R_c$ is encoded into a distinct <em>abstract token</em> that carries both geometric (relative position) and chemical (species) information. Unlike standard GNNs that collapse neighbor information into a single aggregated vector, PET preserves the identity of each interaction.</p> </li> <li> <p><strong>Self-attention over interactions.</strong> These tokens are processed by a Transformer-style self-attention mechanism. This allows the model to learn that the presence of one neighbor dynamically modifies the effective interaction with another—for example, how a third oxygen atom weakens a hydrogen bond between two water molecules. These <strong>many-body effects</strong> emerge naturally from attention, without requiring hand-crafted descriptors.</p> </li> <li> <p><strong>Computational efficiency.</strong> By avoiding the expensive mathematical machinery of spherical harmonics and Clebsch–Gordan coefficients required by strictly equivariant architectures, PET achieves competitive accuracy at significantly lower computational cost. The price is that rotational symmetry must be learned rather than guaranteed.</p> </li> </ol> <h3 id="enforcing-symmetry-at-runtime">Enforcing Symmetry at Runtime</h3> <p>XXX check section and shorten it Since PET is not intrinsically equivariant, FlashMD must enforce symmetry through two complementary mechanisms.</p> <p><strong>During training</strong>, random rotations are applied to every training sample (<strong>data augmentation</strong>). This teaches the model to produce consistent predictions regardless of molecular orientation—but the resulting equivariance is only approximate, limited by finite training data and model capacity.</p> <p><strong>During inference</strong>, FlashMD adds a second layer of protection: <strong>Stochastic Frame Averaging</strong>. Before each prediction step:</p> <ol> <li>The entire system $(\mathbf{Q}, \mathbf{P})$ is rotated by a random matrix $\mathcal{R}$.</li> <li>The backbone computes the next state in the rotated frame.</li> <li>The output is mapped back to the original frame via $\mathcal{R}^{-1}$.</li> </ol> \[\mathbf{y}_{\text{final}} = \mathcal{R}^{-1} \cdot \mathcal{F}_{\theta}(\mathcal{R} \cdot \mathbf{x})\] <p>Over many rollout steps, this ensures that predictions are <strong>statistically invariant</strong> to orientation—any systematic bias toward a particular direction averages out. It is a pragmatic compromise: cheaper than the full Equivariant Coordinate System Ensemble (ECSE) proposed in the original PET paper, but sufficient for the long rollouts FlashMD targets.</p> <p>The combination of data augmentation and stochastic frame averaging makes PET <em>practically</em> equivariant—not by mathematical proof, but by empirical convergence. Whether this approximation is good enough depends on the application. For the thermostatted benchmarks in the next section, it works remarkably well. For the stricter NVE setting in our <a href="#an-exploratory-study-on-failure-modes">exploratory study</a>, even small symmetry violations may compound.</p> <h2 id="long-stride-predictions-in-practice">Long-Stride Predictions in Practice</h2> <p>To demonstrate what FlashMD is capable of, the authors evaluate it across a diverse set of benchmarks and experiments, designed to answer a simple question: Can we simulate realistic molecular dynamics with much larger time steps – without losing essential physics?</p> <p>FlashMD is explored in two flavors:</p> <ul> <li>A water-specific model, trained only on liquid water</li> <li>A universal model, trained on chemically diverse systems, meant to generalize across molecules and materials</li> </ul> <p>This lets the authors study both ends of the spectrum: maximum accuracy for one system, and broad applicability across many.</p> <h3 id="the-testing-strategy">The Testing Strategy</h3> <p>Before diving into results, it’s worth understanding how you even benchmark a method like FlashMD. A direct, step-by-step comparison of trajectories is impossible: MD is chaotic – tiny differences grow exponentially, so two simulations will quickly diverge even if both are “correct”.</p> <p>Instead, the researchers took a statistical approach:</p> <ol> <li>Generate reference trajectories using conventional MD with a reliable force field (PET-MAD)</li> <li>Run FlashMD simulations under the same conditions</li> <li>Compare statistical properties rather than individual trajectories—things like density, radial distribution functions (how atoms arrange themselves around each other), and phase transition temperatures</li> </ol> <p>As the authors note: “we primarily focus our quantitative analysis on time-independent equilibrium properties, and discuss examples where FlashMD qualitatively captures time-dependent behavior.” In other words: check if the average properties match, and see if the dynamics look qualitatively reasonable.</p> <h3 id="key-results-at-a-glance">Key Results at a Glance</h3> <p><strong>1. Liquid Water:</strong> Water might seem simple, but it’s notoriously tricky to simulate due to its hydrogen bonding network. The team tested both their water-specific and universal models on liquid water at 450 K (above the melting point for their particular model).</p> <p><strong>Key findings:</strong></p> <ul> <li>Temperature control worked well when using appropriate thermostats (Langevin), with deviations typically under 1 K from target temperature</li> <li>Radial distribution functions (which show how oxygen and hydrogen atoms arrange themselves) matched reference MD simulations nearly perfectly</li> <li>Density predictions from constant-pressure simulations were accurate for water-specific models and reasonable for universal models</li> <li>Models could handle strides up to 16 fs—a 64× speedup compared to the 0.25 fs timesteps typically needed</li> </ul> <p><strong>2. Solvated Alanine Dipeptide: Protein-like Dynamics</strong> This system—a small peptide in water—serves as a minimal model for protein flexibility. The critical test: can FlashMD capture the Ramachandran plot, which maps out the backbone conformations proteins can adopt?</p> <p>Remarkably, this works even with strides up to 32× larger than standard MD time steps – a strong indication that FlashMD preserves meaningful molecular motion, not just static snapshots.</p> <p><strong>3. Aluminum Surface: Catching Pre-melting Phenomena</strong> Metal surfaces exhibit fascinating behavior at high temperatures: atoms start becoming mobile before bulk melting occurs, a phenomenon called pre-melting. This requires capturing subtle, layer-specific dynamics.</p> <p><strong>Key findings:</strong></p> <ul> <li>Correctly reproduced the anisotropic softening pattern—surface atoms wiggling more in one direction, second-layer atoms in another</li> <li>Captured dynamic defect formation: temporary creation and migration of surface atoms</li> <li>Achieved this with 64 fs strides (64× faster than the 1 fs baseline), while still showing physically meaningful atomic trajectories</li> </ul> <p>This shows that FlashMD is not limited to molecules, but can handle complex solid-state phenomena.</p> <p><strong>4. Lithium Thiophosphate: Superionic Phase Transitions</strong> Perhaps the most impressive demonstration involved a solid-state battery electrolyte material. At high temperatures, lithium atoms become highly mobile in a “superionic” state—critical for battery performance.</p> <p>The challenge: Predict temperature-dependent lithium conductivity and capture the phase transition.</p> <p><strong>Key findings:</strong></p> <ul> <li>Successfully predicted the superionic transition temperature at 675 K, within the expected range</li> <li>Reproduced the dramatic increase in lithium ion conductivity across the transition</li> <li>Some systematic errors appeared (over/underestimation at low/high temperatures), but the overall behavior was captured with 8× speedup</li> </ul> <p>Here, FlashMD demonstrates that it can capture slow, collective processes, which are traditionally hard to access with MD.</p> <hr/> <h1 id="limitations-and-open-challenges">Limitations and Open Challenges</h1> <p>FlashMD promises incredible speed ($100\times$), but we have to ask: is the physics still real? Neural networks are pattern matchers, not physics engines. They don’t actually understand laws like energy conservation; they only memorize the data they’ve seen. When we push the model beyond its training distribution, cracks begin to appear.</p> <h2 id="learning-dynamics-without-physical-guarantees">Learning Dynamics Without Physical Guarantees</h2> <p>Velocity Verlet comes with mathematical guarantees: energy conservation, symplecticity, time-reversibility. A learned model has no such guarantees. Let’s examine what can go wrong.</p> <h3 id="1-out-of-distribution-drift">1. Out-of-Distribution Drift</h3> <p>FlashMD is trained on equilibrium MD trajectories. But during a long rollout, small errors compound. After 1,000 steps (160 ps with 16 fs strides), the system may drift into configurations never seen during training.</p> <p>Unlike MLIPs—which predict one step ahead—FlashMD’s errors accumulate <strong>autoregressively</strong>. This demands robust uncertainty quantification: the model must know when it doesn’t know.</p> <h3 id="2-chaotic-dynamics">2. Chaotic Dynamics</h3> <p>Molecular systems are chaotic: nearby trajectories diverge exponentially (Lyapunov exponent). This imposes a fundamental limit—even a perfect model cannot predict beyond the system’s decorrelation time (typically picoseconds for liquids, nanoseconds for proteins).</p> <p>This isn’t a bug; it’s physics. FlashMD must capture the <strong>statistical ensemble</strong> of trajectories, not a single deterministic path. This introduces <strong>aleatoric uncertainty</strong>—irreducible randomness from the chaotic dynamics itself.</p> <h3 id="3-energy-conservation">3. Energy Conservation</h3> <p>Here’s the Achilles’ heel. Velocity Verlet conserves energy to machine precision. A neural network has no such constraint—small prediction errors cause energy drift:</p> \[\Delta H = H(\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t}) - H(\mathbf{Q}_t, \mathbf{P}_t)\] <p>FlashMD addresses this two ways:</p> <ol> <li><strong>Training:</strong> Include $|\Delta H|$ in the loss function, encouraging implicit energy conservation</li> <li><strong>Inference:</strong> Rescale momenta post-prediction to enforce $H_{t+\Delta t} = H_t$ exactly</li> </ol> <p>The second approach is aggressive but necessary. However, it modifies the dynamics—we’re no longer solving Hamilton’s equations, but a constrained variant. Does this preserve the correct statistical ensemble? (We’ll return to this question.)</p> <h3 id="4-symplectic-structure">4. Symplectic Structure</h3> <p>In Hamiltonian mechanics, phase space has geometric structure (symplecticity) that preserves volume. Classical integrators respect this; neural networks don’t.</p> <p>Enforcing symplecticity explicitly—via a generating function parameterization—is theoretically possible but computationally prohibitive (it requires computing a 3N × 3N Jacobian). FlashMD takes a pragmatic approach: train on symplectic data (VV trajectories) and hope the model learns the structure implicitly.</p> <p>This is a gamble. Without explicit enforcement, thermodynamic properties (temperature, pressure, free energies) may drift over long timescales.</p> <h3 id="5-rotational-symmetry">5. Rotational Symmetry</h3> <p>Physics is rotationally invariant: if you spin your simulation box, the dynamics shouldn’t change. A standard neural network doesn’t “know” this—it must learn it from data.</p> <p>FlashMD shows it can maintain physical accuracy while taking dramatically larger steps through time, across a diverse range of materials. But as we’ll see next, this performance comes with important caveats—particularly around energy conservation…</p> <p>FlashMD mitigates this via:</p> <ul> <li><strong>Data augmentation:</strong> Random rotations during training</li> <li><strong>Runtime augmentation:</strong> Random rotations at each prediction step</li> </ul> <h1 id="an-exploratory-study-on-failure-modes">An Exploratory Study on Failure Modes</h1> <p>To move beyond theoretical concerns, we conduct a systematic exploratory study on a concrete system: a periodic box of 258 TIP3P water molecules simulated with OpenMM as ground truth. We train FlashMD models under varying conditions and evaluate their ability to conserve energy during NVE rollouts—the most unforgiving test of physical validity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3: Comparison of Maxwell-Boltzmann distributions for different temperatures</figcaption> </figure> <h2 id="experimental-setup">Experimental Setup</h2> <p><strong>Ground Truth Generation.</strong> We generate NVE trajectories at different temperatures (200K–700K, 20K steps) using the TIP3P force field in OpenMM, saving configurations every 0.5 fs for 10 ps each. We verify thermodynamic consistency by comparing the velocity distributions against the Maxwell-Boltzmann distribution at each temperature (<a href="#fig:maxwell">Figure 3</a>).</p> <p><strong>Training Data.</strong> From these trajectories, we construct FlashMD training pairs $(q_t, p_t) \to (q_{t+\Delta t}, p_{t+\Delta t})$ at a prediction stride of $\Delta t = 1\,\text{fs}$. Following the original paper, targets are stored as mass-scaled quantities: $\Delta\tilde{q}_i = \Delta q_i \sqrt{m_i}$ and $\tilde{p}_i = p_i / \sqrt{m_i}$. XXX check again this in code</p> <p><strong>Evaluation Protocol.</strong> Each trained model is deployed in NVE simulation for 50 ps (50,000 steps at 1 fs) starting from a 300 K equilibrated configuration. We track total energy $E_\text{tot}(t)$, temperature $T(t)$, and energy drift rate $\dot{E}$ (eV/ps). A model is considered “exploded” if $T &gt; 1000\,\text{K}$ at any point.</p> <p>XXX before ablation show the trained models are stable but lack the energy conservation leading to huge energy and temperature drifts.</p> <p>The first results of the simulations depict in Figure XX. We trained the models accordingly to the setup above. We also investigated different starting points from the NVE groundtruth to see the bahaviour od the model as well as different seeds to handle stochastic training varaicen. All custom models show that all of the trained models achieve stable NVE simulations. But all of them lack the energy conservation drastically. We see huge drifts in energy and temperature. This indicates us that the model loses some energy somehow. To understand this behaviour we also want to take a look at the momenta distribution of the models compared to the ground truth. Since the momenta relates directly to the potential energy this is traight forward.</p> <p>XXX Image of plots fro the distributions here.</p> <p>As we can see. All of custom models exhibit extensive differences in the momenta of Oxygen atoms. But interestingly we can see that if turn on the rescaling filter we can even better distributions. This leads us to question how could we improve the momentum distrubtion of the model by doing two things:</p> <ul> <li>Loss weighting between positons and momenta</li> <li>Applying mass sacling to the positons and momenta</li> </ul> <h2 id="ablation-1-loss-weighting-between-positions-and-momenta">Ablation 1: Loss Weighting Between Positions and Momenta</h2> <p>The first question is simple: does it matter how much the model cares about getting momenta right vs. positions? We train models with momentum loss weights $w_p \in {0.5, 1.0, 1.5, 2.0, 10.0}$ while keeping the position weight fixed at $w_q = 1.0$, and let them run NVE for 10 ps.</p> <p>The results are… surprising.</p> <table> <thead> <tr> <th>Model</th> <th>$w_p$</th> <th>Drift (eV/ps)</th> <th>$\bar{T}$ (K)</th> <th>$\sigma_T$ (K)</th> <th>Stable until</th> </tr> </thead> <tbody> <tr> <td>$w_p=0.5$</td> <td>0.5</td> <td>2813.95</td> <td>513.8</td> <td>148.9</td> <td>8.1 ps ✗</td> </tr> <tr> <td>$w_p=1.0$ (Baseline)</td> <td>1.0</td> <td>-3.70</td> <td>227.1</td> <td>34.2</td> <td>&gt;10 ps ✓</td> </tr> <tr> <td>$w_p=1.5$</td> <td>1.5</td> <td>-4.80</td> <td>178.0</td> <td>38.8</td> <td>&gt;10 ps ✓</td> </tr> <tr> <td>$w_p=2.0$</td> <td>2.0</td> <td>52.80</td> <td>476.1</td> <td>168.5</td> <td>2.7 ps ✗</td> </tr> <tr> <td>$w_p=10.0$</td> <td>10.0</td> <td>4.39</td> <td>334.9</td> <td>67.5</td> <td>&gt;10 ps ✓</td> </tr> <tr> <td>Ground Truth</td> <td>—</td> <td>-0.0035</td> <td>304.7</td> <td>6.9</td> <td>&gt;10 ps ✓</td> </tr> </tbody> </table> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure X: NVE trajectories for three momentum loss weights and OpenMM ground truth. Top: temperature evolution. Bottom: total energy. The ground truth (black) is essentially flat — none of the models come close.</figcaption> </figure> <p>If you expected “more momentum weight = better momenta = more stable,” you’d be wrong. The relationship is wildly non-monotonic, and we can group the results into three regimes:</p> <p><strong>Too little ($w_p = 0.5$): immediate explosion.</strong> The model doesn’t learn momentum dynamics well enough and blows up within 8 ps. Temperature rockets past 500 K, energy drift is off the charts. Lesson: you <em>need</em> to care about momenta.</p> <p><strong>The “moderate” zone ($w_p = 1.0, 1.5$): stable but freezing.</strong> These models survive the full 10 ps — but look at the temperatures! The baseline cools from 300 K to a mean of 227 K, and $w_p = 1.5$ cools even further to 178 K. The model is systematically predicting momenta that are too small. It’s not exploding, but it’s not doing physics either — it’s slowly bleeding kinetic energy. You can see this clearly in <a href="#fig:ablation1">Figure X</a>: the blue and green lines drift steadily downward while the ground truth holds at ~305 K.</p> <p><strong>Cranked to 10 ($w_p = 10.0$): closest to reality, but noisy.</strong> Here’s the twist: the most extreme weight actually produces the most realistic temperature (335 K, just 10% above target). Looking at <a href="#fig:ablation1">Figure X</a>, the red $w_p=10$ line tracks the ground truth best during the first ~5 ps. But the fluctuations are huge ($\sigma_T = 67$ K vs. ground truth’s 7 K), and by 8 ps it starts slowly heating up.</p> <p>To our suprise, $w_p = 2.0$ explodes at 2.7 ps. Somehow, 2× the baseline is catastrophically unstable while 10× is fine. The loss landscape clearly has some sharp cliffs in this region.</p> <p><strong>The bottom line:</strong> Look at the energy panel in <a href="#fig:ablation1">Figure X</a>. The ground truth (black line) is essentially a flat line. Every single model drifts visibly — the best ones still have energy drift <strong>1000× larger</strong> than OpenMM. Loss weighting can shift the failure mode from cooling to heating, but it can’t bridge that gap.</p> <p>This tells us something important: the problem isn’t <em>how much</em> the model cares about momenta — it’s <em>how</em> it represents them. Which brings us to our next experiment: what if we change the loss to account for the fact that hydrogen and oxygen have very different masses?</p> <p>much* the model cares about momenta, we change <em>how</em> it represents them through mass-scaled loss formulations. –&gt;</p> <h2 id="ablation-2-mass-scaled-loss-functions">Ablation 2: Mass-Scaled Loss Functions</h2> <p>Standard MSE treats all atoms equally in momentum space. But since $p_i = m_i v_i$, oxygen atoms (mass 16) dominate the momentum loss by a factor of $16^2 = 256$ compared to hydrogen (mass 1). This means the model optimizes primarily for oxygen momenta while hydrogen velocity errors remain large.</p> <p>Following the FlashMD paper, we implement a mass-scaled loss:</p> \[\mathcal{L}_{\tilde{p}} = \frac{1}{N} \sum_i \frac{\|p_i^\text{pred} - p_i^\text{true}\|^2}{m_i}, \qquad \mathcal{L}_{\Delta\tilde{q}} = \frac{1}{N} \sum_i \|\Delta q_i^\text{pred} - \Delta q_i^\text{true}\|^2 \cdot m_i\] <p>This is equivalent to computing MSE on the mass-scaled quantities $\tilde{p} = p/\sqrt{m}$ and $\Delta\tilde{q} = \Delta q \cdot \sqrt{m}$, ensuring that velocity errors are weighted equally regardless of atomic mass.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blockpost_flashMD/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-480.webp 480w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-800.webp 800w,/blockpost_flashMD/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blockpost_flashMD/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure X: NVE trajectory comparison between standard MSE and mass-scaled MSE loss. Left column shows the initial drift phase (0–10 ps), right column the full 50 ps trajectory. Both models exhibit systematic cooling relative to the ground truth.</figcaption> </figure> <table> <thead> <tr> <th>Model</th> <th>Loss Type</th> <th>Drift (eV/ps)</th> <th>$\bar{T}$ (K)</th> <th>$\sigma_T$ (K)</th> <th>$\sigma_E$ (kJ/mol)</th> <th>Stable until</th> </tr> </thead> <tbody> <tr> <td>Custom Trained</td> <td>MSE</td> <td>-0.6842</td> <td>207.1</td> <td>23.0</td> <td>591.1</td> <td>&gt;50 ps</td> </tr> <tr> <td>Mass-Scaled Custom Trained</td> <td>mass_scaled_mse</td> <td>-0.6989</td> <td>212.9</td> <td>24.9</td> <td>642.0</td> <td>&gt;50 ps</td> </tr> <tr> <td>Ground Truth (OpenMM)</td> <td>—</td> <td>-0.0003</td> <td>304.2</td> <td>6.9</td> <td>1.2</td> <td>&gt;50 ps</td> </tr> </tbody> </table> <p><strong>Key finding:</strong> Mass-scaling provides a modest improvement in the early drift phase but does not fundamentally alter long-term stability. A time-windowed analysis reveals the nuance:</p> <ul> <li><strong>0–5 ps:</strong> Mass-scaling reduces energy drift by 26% (-3.72 vs. -5.03 eV/ps) and keeps the temperature closer to 300 K (272 K vs. 254 K, $\sigma_T$ halved from 26 to 13 K). The model clearly benefits from balanced treatment of H and O atoms in the initial phase.</li> <li><strong>0–10 ps:</strong> The advantage persists but narrows—drift is reduced by 24% (-2.82 vs. -3.70 eV/ps), mean temperature 252 K vs. 227 K.</li> <li><strong>0–50 ps:</strong> Both models converge to nearly identical behavior—drift rates of -0.68 and -0.70 eV/ps, mean temperatures of 207 K and 213 K. The early advantage is fully washed out.</li> </ul> <p><strong>Physical interpretation:</strong> Mass-scaling correctly addresses the <em>symptom</em>—unequal error weighting across species—but not the <em>disease</em>. Both models systematically cool from 300 K to ~210 K over 50 ps, losing roughly $\frac{1}{3}$ of their kinetic energy. This cooling pattern ($\dot{E} \approx -0.7$ eV/ps, $\sigma_E \sim 600$ kJ/mol vs. ground truth $\sigma_E = 1.2$ kJ/mol) represents a $500\times$ violation of energy conservation that no loss reweighting can fix.</p> <p>The fact that mass-scaling helps <em>early</em> but not <em>late</em> suggests that it improves the model’s initial momentum predictions (reducing the per-species bias), but the accumulated errors from the autoregressive rollout eventually dominate regardless. The fundamental issue is that FlashMD’s single-step prediction errors, while small individually, compound systematically rather than canceling stochastically—a hallmark of non-symplectic integration.</p> <h2 id="summary-of-findings">Summary of Findings</h2> <p>Our exploratory study reveals a consistent pattern across all model variants:</p> <ol> <li> <p><strong>All models exhibit systematic energy drift</strong>, even in the quasi-stable regime. This is expected—the model has no symplectic structure or energy-conserving inductive bias.</p> </li> <li> <p><strong>Loss weighting affects the drift rate</strong> but does not eliminate it. Mass-scaled losses with appropriate momentum weighting achieve the best short-term stability.</p> </li> <li> <p><strong>Failure is always catastrophic once it begins.</strong> There is no graceful degradation—once the system leaves the training distribution, errors compound exponentially.</p> </li> </ol> <p>These findings suggest that improving the loss function alone is insufficient. The fundamental challenge is distributional: the model must either (a) be trained on data hat covers the states it will visit during long rollouts, or (b) incorporate physical constraints that prevent drift toward unphysical regions.</p> <hr/> <h1 id="conclusion">Conclusion</h1> <p>In this post, we traced the two fundamental bottlenecks of molecular dynamics and showed how each has been addressed: MLIPs solved the force bottleneck; FlashMD bypasses the integrator entirely, predicting system evolution at strides one to two orders of magnitude beyond classical stability limits. Across water, proteins, metals, and battery electrolytes, it recovers the correct statistical physics at speedups of 8× to 64×.</p> <p>But our independent exploratory study reveals a sobering counterpoint. When we strip away the thermostats that regulate temperature in standard benchmarks, the learned dynamics fail to conserve energy. Every model we tested—regardless of loss weighting or mass scaling—exhibits systematic energy drift orders of magnitude larger than classical integrators. The problem is structural, not parametric: no amount of loss function tuning resolved it.</p> <p>This does not diminish FlashMD’s contribution—it is the first model to learn meaningful long-stride dynamics across chemically diverse systems. But it makes clear what is still missing: the mathematical guarantees that classical integrators provide for free. Until symplectic architectures, hybrid corrective schemes, or active learning strategies close this gap, FlashMD is best understood as a powerful tool for thermostatted simulations rather than a general-purpose replacement for classical integrators.</p> <p>The question we posed in the introduction—<em>Can a model learn to respect the laws of physics without being explicitly taught to do so?</em>—now has a nuanced answer. For statistical properties sampled under external control, yes. For the strict, unassisted conservation of energy that defines Hamiltonian dynamics, not yet.</p> <h2 id="xxx-propose-a-way-scetch-a-solution-maybe-add-first-experiments-with-handselected-active-learning-">XXX Propose a way, scetch a solution maybe add first experiments with handselected active learning …</h2> <h1 id="conclusion-1">Conclusion</h1> <p><em>(Your text here…)</em></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In 2025, a research group of the COSMO Lab published a new framework for long-stride, universal prediction of molecular dynamics, which they call FLashMD. This new approach addresses one of the biggest challenges in computational science: the trade-off between accuracy and speed in simulating atomic-scale systems. By introducing a novel neural network architecture, FLashMD learns to predict the complex, quantum-mechanical forces governing molecular behavior, enabling simulations that are both accurate and computationally efficient. This post explores the core concepts behind FLashMD, breaks down its innovative architecture, and examines its potential to revolutionize fields from drug discovery to materials science.]]></summary></entry></feed>