<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics | ICLR Blogposts 2026
    
  
</title>
<meta name="author" content="ICLR Blog">
<meta name="description" content="In 2025, a research group of the COSMO Lab published a new framework for long-stride, universal prediction of molecular dynamics, which they call FLashMD. This new approach addresses one of the biggest challenges in computational science: the trade-off between accuracy and speed in simulating atomic-scale systems. By introducing a novel neural network architecture, FLashMD learns to predict the complex, quantum-mechanical forces governing molecular behavior, enabling simulations that are both accurate and computationally efficient. This post explores the core concepts behind FLashMD, breaks down its innovative architecture, and examines its potential to revolutionize fields from drug discovery to materials science.">

  <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f">

<link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/2026/blog/2026/flashmd_old/">


  <!-- Dark Mode -->
  <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>











    <!-- Distill js -->
    <script src="/2026/assets/js/distillpub/template.v2.js"></script>
    <script src="/2026/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics",
            "description": "In 2025, a research group of the COSMO Lab published a new framework for long-stride, universal prediction of molecular dynamics, which they call FLashMD. This new approach addresses one of the biggest challenges in computational science: the trade-off between accuracy and speed in simulating atomic-scale systems. By introducing a novel neural network architecture, FLashMD learns to predict the complex, quantum-mechanical forces governing molecular behavior, enabling simulations that are both accurate and computationally efficient. This post explores the core concepts behind FLashMD, breaks down its innovative architecture, and examines its potential to revolutionize fields from drug discovery to materials science.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/2026/">
          ICLR Blogposts 2026
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/2026/">home
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/2026/about/">about
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/2026/call/">call for blogposts
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/2026/submitting/">submitting
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/2026/reviewing/">reviewing
                    
                  </a>
                </li>
              
            
          
            
              
                
                
                <li class="nav-item dropdown ">
                  <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations
                    
                  </a>
                  <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                    
                      
                        <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a>
                      
                    
                      
                        <div class="dropdown-divider"></div>
                      
                    
                      
                        <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a>
                      
                    
                      
                        <div class="dropdown-divider"></div>
                      
                    
                      
                        <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a>
                      
                    
                      
                        <div class="dropdown-divider"></div>
                      
                    
                      
                        <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a>
                      
                    
                      
                        <div class="dropdown-divider"></div>
                      
                    
                      
                        <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a>
                      
                    
                  </div>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics</h1>
        <p>In 2025, a research group of the COSMO Lab published a new framework for long-stride, universal prediction of molecular dynamics, which they call FLashMD. This new approach addresses one of the biggest challenges in computational science: the trade-off between accuracy and speed in simulating atomic-scale systems. By introducing a novel neural network architecture, FLashMD learns to predict the complex, quantum-mechanical forces governing molecular behavior, enabling simulations that are both accurate and computationally efficient. This post explores the core concepts behind FLashMD, breaks down its innovative architecture, and examines its potential to revolutionize fields from drug discovery to materials science.</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#introduction">Introduction</a>
                </div>
                
              
                <div>
                  <a href="#molecular-dynamics-fundamentals">Molecular Dynamics Fundamentals</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#the-hamiltonian-view-of-atomic-motion">The Hamiltonian View of Atomic Motion</a>
                      </li>
                    
                      <li>
                        <a href="#why-ab-initio-molecular-dynamics-is-expensive">Why ab Initio Molecular Dynamics Is Expensive</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#machine-learned-interatomic-potentials-solving-the-force-bottleneck">Machine-Learned Interatomic Potentials: Solving the Force Bottleneck</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#the-mlip-breakthrough">The MLIP Breakthrough</a>
                      </li>
                    
                      <li>
                        <a href="#why-faster-forces-are-still-not-enough">Why Faster Forces Are Still Not Enough</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#flashmd-escaping-the-femtosecond-time-step">FlashMD: Escaping the Femtosecond Time-Step</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#a-shift-from-forces-to-trajectories">A Shift from Forces to Trajectories</a>
                      </li>
                    
                      <li>
                        <a href="#architecture-and-design-principles-of-flashmd">Architecture and Design Principles of FlashMD</a>
                      </li>
                    
                      <li>
                        <a href="#long-stride-predictions-in-practice">Long-Stride Predictions in Practice</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#limitations-and-open-challenges">Limitations and Open Challenges</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#learning-dynamics-without-physical-guarantees">Learning Dynamics Without Physical Guarantees</a>
                      </li>
                    
                      <li>
                        <a href="#an-exploratory-study-on-failure-modes">An Exploratory Study on Failure Modes</a>
                      </li>
                    
                      <li>
                        <a href="#future-direction-active-learning-strategies">Future Direction: Active Learning Strategies</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#conclusion">Conclusion</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <h1 id="introduction">Introduction</h1>
<p>Molecular Dynamics (MD) is often called the “computational microscope” of modern science. By simulating atoms obeying Newton’s laws, we can watch proteins fold, batteries charge, and materials fracture—all at the atomic scale. If we can simulate the atoms, we can predict the material.</p>

<p>But there’s a brutal trade-off: <strong>accuracy versus time</strong>.</p>

<p>Machine Learning Interatomic Potentials (MLIPs) recently solved the first bottleneck—calculating forces—by replacing expensive quantum calculations with learned approximations. But a second, more stubborn barrier remains: the <strong>femtosecond prison</strong>. Traditional integrators must take tiny time steps ($\sim10^{-15}$ s) to remain stable<d-cite key="leach2001timestep"></d-cite>. To observe a biological process lasting milliseconds requires <strong>trillions of sequential steps</strong>—each depending on the last.</p>

<p>FlashMD shatters this chain. By directly predicting the system’s state over time steps $1-2$ orders of magnitude higher than the stability limit of traditional integrators, it shifts the paradigm from <strong>simulation to emulation</strong>. Instead of numerically integrating forces at every femtosecond, FlashMD learns to leap forward in time, promising to collapse simulations that once took weeks into hours.</p>

<p><strong>But here’s the catch:</strong> Can a learned emulator remain physically stable?</p>

<p>Traditional integrators come with mathematical guarantees—energy conservation, time-reversibility, symplectic structure. FlashMD trades these guarantees for speed. It learns dynamics purely from data. In this post, we’ll explore FlashMD’s architecture and ambitions—and then critically examine whether learned emulation can escape the femtosecond prison without losing physical validity. Through an exploratory study, we’ll see where it breaks down and what that reveals about the future of learned simulators.</p>

<!-- Molecular Dynamics (MD) is often described as the "computational microscope" of modern science. By solving Newton’s equations of motion for atomistic systems, it provides our only window into the dynamic behavior of matter—from the folding of proteins to the diffusion of ions in a battery. If we can simulate the atoms, we can predict the macroscopic properties of the material.

However, a frustrating trade-off has always plagued these simulations: the battle between **accuracy** and **time**.

While the introduction of Machine Learning Interatomic Potentials (MLIPs) successfully bridged the gap between quantum accuracy and classical speed for calculating forces, a second, more stubborn bottleneck remains. Standard simulations are shackled by the stability of their numerical integrators, which require time steps on the order of femtoseconds ($10^{-15}$ s)<d-cite key="leach2001timestep"></d-cite> . To observe biological processes that occur over milliseconds ($10^{-3}$ s), a simulation must execute trillions of sequential steps.

FlashMD is a new machine learning framework that attempts to shatter this barrier. Instead of painstakingly integrating the forces on every atom at every femtosecond, FlashMD uses a deep-learning architecture to "leap" forward in time. By directly predicting the system's state over time steps $1-2$ magnitudes higher than the stability limit of traditional integrators, it aims to shift the paradigm from simulation to emulation.

In this post, we will first explore the fundamental bottlenecks that made FlashMD necessary. We will then dive into its architecture, and finally, offer a critical analysis of its physical validity—specifically regarding the crucial issue of energy conservation. -->

<hr>

<h1 id="molecular-dynamics-fundamentals">Molecular Dynamics Fundamentals</h1>
<p>Molecular Dynamics (MD) is the computational engine behind our understanding of matter in motion. At its core, MD follows a straightforward recipe: place $N$ atoms in a box, calculate the forces between them, and step forward in time using Newton’s equations.</p>

<p>The challenge lies in the scale. As shown in the workflow below, every MD simulation is built around a <strong>core loop</strong> that must be executed millions to billions of times:</p>

<ol>
  <li>
<strong>Calculate forces</strong> on every atom (Step 2)</li>
  <li>
<strong>Integrate equations of motion</strong> to update positions and velocities (Step 3)</li>
  <li>Repeat for $10^6$ to $10^9$ steps</li>
</ol>

<p>[INSERT FIGURE HERE]</p>

<p>This repetition is fundamental, not a limitation. To extract meaningful thermodynamic properties—diffusion coefficients, phase transitions, reaction rates—we need trajectories long enough to sample the system’s accessible states. A single nanosecond of physical time typically requires around a million integration steps.</p>

<p>This raises a natural question: <strong>which step consumes the most time?</strong></p>

<!-- Molecular Dynamics (MD) is often described as a "computational microscope." It allows researchers to observe how atoms and molecules interact over time, providing insights into dynamic behaviors that static images cannot capture.

In many ways, an MD simulation parallels a real laboratory experiment. The workflow typically follows three stages:

1. **Preparation (Equilibration):** We select a model system of $N$ particles and solve Newton's equations until the system settles into a stable state.
2. **Measurement (Production Run):** We evolve the system further to measure macroscopic properties—such as temperature, pressure, or diffusion coefficients
3. **Analysis:** Since instantaneous measurements are noisy, we average these properties over time to obtain statistically significant results. -->

<p>XXX Illustration of a MD Simulation witht the steps</p>

<h2 id="the-hamiltonian-view-of-atomic-motion">The Hamiltonian View of Atomic Motion</h2>

<p>To understand where the computational bottleneck lies, we need to look inside the simulation loop. At each step, the system evolves according to Hamilton’s equations of motion—the fundamental laws governing how atoms move.</p>

<p>The core idea is simple: <strong>forces come from energy gradients</strong>. We describe the system’s total energy using the Hamiltonian $H$, which splits into two parts:</p>

\[H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}\]

<p>where \(\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N\) are atomic positions, \(\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N\) are momenta, and \(V(\mathbf{Q})\) is the potential energy surface (PES).</p>

<p>Hamilton’s equations tell us how the system evolves:</p>

\[\frac{d\mathbf{q}_i}{dt} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial V}{\partial \mathbf{q}_i}\]

<p>The second equation is crucial: <strong>the force on each atom is the negative gradient of the potential energy</strong>. To simulate the system, we need two things:</p>

<ol>
  <li>A way to compute $V(\mathbf{Q})$ and its gradient $\nabla V$.</li>
  <li>A way to discretize continuous time into finite steps $\Delta t$.</li>
</ol>

<p><strong>For the second requirement, we use numerical integration.</strong></p>

<h3 id="the-velocity-verlet-algorithm">The Velocity Verlet Algorithm</h3>

<p>To solve these continuous equations on a computer, we discretize time using the Velocity Verlet integrator:</p>

\[\begin{aligned}
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t \\
    \mathbf{q}_i &amp;\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t
\end{aligned}\]

<p>This algorithm is <strong>symplectic</strong>, meaning it approximately conserves the Hamiltonian even with finite $\Delta t$. But there’s a catch: stability requires $\Delta t \sim 0.5\text{–}1$ femtoseconds<d-cite key="leach2001timestep"></d-cite>. Larger steps cause energy drift and numerical explosions. <strong>This is the femtosecond prison</strong>—the fundamental timestep barrier that limits all classical MD.</p>

<p>Every loop iteration requires:</p>
<ul>
  <li>
<strong>Computing forces</strong> via $\nabla V$ (expensive)</li>
  <li>
<strong>Taking a tiny timestep</strong> (limiting)</li>
</ul>

<p>We’ll address these bottlenecks in sequence, starting with force computation.</p>

<!-- Before navigating the landscape of MLIPs, we must ground ourselves in the Hamiltonian formalism. You can think of the Hamiltonian as the non-negotiable "rulebook" for the system. It is a scalar function that, at any instance, quantifies the total energy of the system. This energy is the sum of two distinct components:

1. **Kinetic Energy ($K$):** The energy of motion, dependent on particle momenta $\mathbf{P}$.
2. **Potential Energy ($V$):** The energy "stored" in atomic configurations and interactions, dependent on particle positions $\mathbf{Q}$.


For an isolated atomistic system with $N$ atoms, masses $m_i$, positions $$\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N$$, and momenta $$\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N$$, the Hamiltonian $H$ takes the following canonical form:

$$
H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}
$$

A fundamental law of physics is that in an isolated system (microcanonical ensemble), this Hamiltonian $H$ is conserved ($\frac{dH}{dt} = 0$). This conservation is the constraint our simulation must respect.

How do the atoms "know" how to move to obey this rule? They follow Hamilton's equations of motion:

$$
\frac{d\mathbf{q}_i}{dt} = \frac{\partial H}{\partial \mathbf{p}_i} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial H}{\partial \mathbf{q}_i}
$$

The first equation simply relates velocity to momentum. The second equation is the physical engine: it states that the time evolution of momentum (the force) is driven strictly by the negative gradient of the potential energy surface (PES), denoted as $V(\mathbf{Q})$. XXX where is V(Q) in the formula?

To solve these continuous equations on a discrete computer, we must turn the infinitely smooth $dt$ into a concrete time step, $\Delta t$. The standard algorithm for this task is the Velocity Verlet (VV) integrator. A single step updates the system as follows:

$$
\begin{aligned}
    \mathbf{p}_i &\leftarrow \mathbf{p}_i - \frac{1}{2} \frac{\partial V}{\partial \mathbf{q}_i} \Delta t \\
    \mathbf{q}_i &\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\
    \mathbf{p}_i &\leftarrow \mathbf{p}_i - \frac{1}{2} \frac{\partial V}{\partial \mathbf{q}_i} \Delta t
\end{aligned}
$$

This loop is the beating heart of molecular dynamics. However, to keep this heart beating, we must repeatedly evaluate the gradient term: $\nabla_i V(\mathbf{Q})$. XXX highlight quickly teh force calculation and small times tep prison -->

<h2 id="why-ab-initio-molecular-dynamics-is-expensive">Why <em>ab Initio</em> Molecular Dynamics Is Expensive</h2>

<p>The first bottleneck is evaluating $V(\mathbf{Q})$ and its gradient. Historically, this forced a painful compromise:</p>

<ul>
  <li>
    <p><strong><em>Ab initio</em> methods</strong> (e.g., Density Functional Theory) solve quantum mechanics to compute forces with chemical accuracy. But they require solving the electronic structure problem at every step, with computational cost scaling as $O(N^3)$ or worse—and large constant factors that make even small systems expensive<d-cite key="zhang2018deep"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Classical force fields</strong> use handcrafted functions (harmonic springs, Lennard-Jones potentials, Coulomb terms) that scale linearly with system size. But these predefined functional forms—fixed at design time—cannot adapt to bond breaking, chemical reactions, or complex polarization effects that require quantum mechanical treatment (XXX Source).</p>
  </li>
</ul>

<p>For decades, this accuracy-efficiency trade-off defined the field. Quantum accuracy meant tiny systems; large-scale simulations meant sacrificing chemistry.</p>

<p>Machine Learning Interatomic Potentials (MLIPs) changed this.</p>

<!-- The central challenge of MD has always been evaluating this potential $V$. Historically, this forced a painful compromise. On one side, ab initio methods like Density Functional Theory (DFT) offer quantum-mechanical accuracy but scale poorly, restricting simulations to tiny systems and picosecond timescales <d-cite key="zhang2018deep"></d-cite>. 

On the other, classical force fields offer linear scaling ($O(N)$) and speed, but rely on rigid, heuristic approximations that often fail to capture complex chemical reactivity and bond breaking. XXX I dont like this sentence

This accuracy-efficiency trade-off defined the field for decades, until Machine Learning Interatomic Potentials (MLIPs) provided a way to bridge the gap. -->

<hr>
<h1 id="machine-learned-interatomic-potentials-solving-the-force-bottleneck">Machine-Learned Interatomic Potentials: Solving the Force Bottleneck</h1>

<p>We’ve identified two computational bottlenecks in MD. Let’s tackle the first one: <strong>computing forces</strong>.</p>

<p>Recall that every timestep requires evaluating $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$—the gradient of the potential energy surface. Historically, this meant choosing between quantum accuracy (expensive) or classical speed (inaccurate). MLIPs broke this trade-off.</p>

<h2 id="the-mlip-breakthrough">The MLIP Breakthrough</h2>

<p>The core idea is simple: <strong>replace quantum calculations with a learned function</strong>.</p>

<p>An MLIP is a neural network that maps atomic positions $\mathbf{Q}$ and atomic numbers $\mathbf{Z}$ directly to potential energy:</p>

\[V_\theta(\mathbf{Q}) \approx V_{\text{QM}}(\mathbf{Q})\]

<p>Forces are then obtained via automatic differentiation:</p>

\[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} V_\theta(\mathbf{Q})\]

<p>This bypasses the $O(N^3)$ cost of solving the electronic structure problem at every step.</p>

<h3 id="architecture-graph-neural-networks">Architecture: Graph Neural Networks</h3>

<p>Modern MLIPs—such as SchNet<d-cite key="schutt2017schnet"></d-cite>, NequIP<d-cite key="batzner2022nequip"></d-cite>, and MACE<d-cite key="batatia2022mace"></d-cite>—use <strong>Graph Neural Networks</strong> (GNNs) that:</p>

<ol>
  <li>
<strong>Encode molecular structure naturally</strong>: Atoms are nodes, interactions are edges</li>
  <li>
<strong>Respect physical symmetries</strong>: Predictions are invariant to translation, rotation, and atom permutation</li>
  <li>
<strong>Learn many-body interactions</strong>: Message-passing layers aggregate information from neighboring atoms</li>
</ol>

<p>The training objective fits both energies and forces jointly:</p>

\[\mathcal{L}(\theta) = \lambda_E \|V_\theta - V_{\text{DFT}}\|^2 + \lambda_F \|\mathbf{F}_{\text{pred}} - \mathbf{F}_{\text{DFT}}\|^2\]

<p>Training on forces directly improves generalization: force supervision provides richer gradient information and helps the model handle out-of-distribution configurations<d-cite key="chmiela2018sgdml"></d-cite>.</p>

<h3 id="impact-quantum-accuracy-at-classical-speed">Impact: Quantum Accuracy at Classical Speed</h3>

<p>The speedup is dramatic. Where DFT takes <strong>minutes</strong> per force evaluation, MLIP inference takes <strong>milliseconds</strong>—a <strong>1000× improvement</strong><d-cite key="he2025mlipsbio"></d-cite>. This has enabled:</p>

<ul>
  <li>
<strong>Larger systems</strong>: Million-atom simulations that were previously impossible</li>
  <li>
<strong>Longer timescales</strong>: Microsecond trajectories with quantum accuracy</li>
  <li>
<strong>New applications</strong>: Drug discovery, battery materials, catalysis<d-cite key="unke2021spookynet"></d-cite>
</li>
</ul>

<p>MLIPs have fundamentally changed what’s computationally feasible in molecular simulation.</p>

<h2 id="the-remaining-challenge-the-femtosecond-prison">The Remaining Challenge: The Femtosecond Prison</h2>

<p>MLIPs solved the force bottleneck—<strong>Bottleneck #1</strong>. But <strong>Bottleneck #2</strong> remains stubbornly unsolved.</p>

<p>As we established earlier, classical integrators require $\Delta t \sim 10^{-15}$ s to maintain numerical stability. This means simulating a microsecond—the timescale of protein folding or molecular recognition—still requires $10^9$ sequential steps, regardless of how fast we can compute forces.</p>

<p><strong>Even with instantaneous force predictions, the serial nature of integration makes long-timescale phenomena computationally intractable.</strong></p>

<p>To escape the femtosecond prison, we cannot simply accelerate the integrator. We must <strong>bypass it entirely</strong>—replacing step-by-step integration with direct trajectory prediction.</p>

<p>This is where FlashMD enters.
&lt;!– # Machine-Learned Interatomic Potentials: Solving the Force Bottleneck
We have established that we need billions of time steps to simulate meaningful biological or material phenomena. This brings us to the second half of the computational burden: the cost of a single step.</p>

<p>As derived in the Hamiltonian framework, every single step requires us to evaluate the gradient of the potential energy surface: $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$.</p>

<h2 id="the-mlip-breakthrough-1">The MLIP Breakthrough</h2>

<p>At its core, an MLIP is a regression framework that approximates the Potential Energy Surface (PES) by learning a mapping $\mathcal{F}_\theta: (\mathbf{Z}, \mathbf{Q}) \to \mathbb{R}$ from atomic numbers and coordinates directly to the scalar potential energy. This effectively bypasses the $O(N^3)$ cost of solving the electronic structure explicitly.</p>

<p>Unlike classical force fields, MLIPs leverage deep neural networks—typically Graph Neural Networks (GNNs) or Message Passing Neural Networks (MPNNs)—to serve as universal approximators of the quantum mechanical interaction <d-cite key="he2025mlipsbio"></d-cite>. This allows them to capture complex, non-local many-body effects that classical approximations inherently miss.</p>

<p>Crucially, MLIPs enforce physical consistency by defining atomic forces as the exact negative gradient of the predicted energy with respect to atomic positions via automatic differentiation:</p>

\[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} E_{\text{pred}}(\mathbf{Q})\]

<p>The training objective is therefore a multi-task learning problem. We optimize the network parameters $\theta$ to minimize a composite loss against ground-truth quantum mechanical labels (typically from DFT):</p>

\[\mathcal{L}(\theta) = \lambda_E \|E_{\text{pred}} - E_{\text{DFT}}\|^2 + \lambda_F \|\underbrace{-\nabla_{\mathbf{Q}} E_{\text{pred}}}_{\mathbf{F}_{\text{pred}}} - \mathbf{F}_{\text{DFT}}\|^2\]

<p>By training on high-quality snapshots of molecular configurations—generated via random sampling or active learning—MLIPs can capture complex, many-body interactions that classical methods simply cannot see. XXX source</p>

<h2 id="why-faster-forces-are-still-not-enough">Why Faster Forces Are Still Not Enough</h2>
<p>Machine Learning Interatomic Potentials (MLIPs) have revolutionized the field by reducing the computational cost of force calculation ($F$) by orders of magnitude compared to DFT. However, they leave the fundamental architectural flaw of MD untouched: the integrator bottleneck.</p>

<p>Classical integrators like Velocity Verlet face a hard physical speed limit. To maintain numerical stability and energy conservation, the time step $\Delta t$ must resolve the fastest atomic vibrations in the system—typically the oscillation of hydrogen bonds. This confines simulations to the femtosecond scale ($\Delta t \approx 10^{-15} \text{s}$), regardless of how fast the force model is.</p>

<p>This creates a massive discrepancy between simulation time and biological reality. To simulate a mere microsecond of physical time—relevant for protein folding or drug binding—we must perform one billion sequential steps:</p>

\[N_{\text{steps}} = \frac{10^{-6} \text{ s}}{10^{-15} \text{ s}} = 10^9 \text{ steps}\]

<p>We are effectively trapped in a “femtosecond prison.” Even with instant force predictions, this serial dependency makes long-timescale phenomena computationally intractable. To escape this, we cannot simply accelerate the integrator; we must bypass it entirely. –&gt;</p>

<hr>

<h1 id="flashmd-escaping-the-femtosecond-prison">FlashMD: Escaping the Femtosecond Prison</h1>

<p>FlashMD introduces a transformative approach: rather than incrementally integrating forces like a standard force field, it operates as a direct trajectory predictor.</p>

<p>Instead of acting as a “middleman”—predicting energy to derive forces for a classical integrator—FlashMD learns the dynamical map directly from simulation data:</p>

\[\mathcal{G}_\theta: (\mathbf{Q}_t, \mathbf{P}_t) \to (\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t})\]

<p>This paradigm shift enables the model to predict the system’s next state in a single forward pass, replacing hundreds of small integration steps and allowing for strides 1-2 magnitudes larger than the stability limit of numerical integrators.</p>

<p>XXX Here Illustration of Classical MD Loop vs. FlashMD Loop</p>

<h2 id="architecture-and-design-principles-of-flashmd">Architecture and Design Principles of FlashMD</h2>
<!-- The FlashMD Architecture -->
<p>To realize the dynamical map $\mathcal{G}_\theta$ defined above, FlashMD implements a flexible deep learning pipeline. While the current implementation defaults to a specific Transformer backbone, the architecture is fundamentally modular.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<p>We can view FlashMD as a wrapper that prepares atomic data for any powerful graph neural network:</p>

<ol>
  <li>
<strong>Input: Embedding the Atomic State</strong> The raw atomic state is converted into a graph representation.
    <ul>
      <li>The current positions $\mathbf{Q}$ and momenta $\mathbf{P}$ are encoded into node and edge features of a molecular graph.</li>
      <li>
<strong>Mass Scaling:</strong> Input momenta are normalized ($\tilde{\mathbf{p}}_i = \mathbf{p}_i / \sqrt{m_i}$) to prevent heavy atoms from dominating the loss, ensuring the model captures fast hydrogen vibrations as accurately as heavy-atom motions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Backbone: Point-Edge Transformer (PET)</strong> The graph is processed by a message-passing network to extract local geometric features. FlashMD uses the Point-Edge Transformer (PET) by default, which updates edge and node representations via attention mechanisms. At inference time, optional filters can be applied: momentum rescaling for energy conservation, thermostat/barostat integration for ensemble control, and random rotations to mitigate symmetry-breaking artifacts.</p>
  </li>
  <li>
    <p><strong>Output: Multi-Head Prediction</strong> Two separate MLP heads branch from the final node representations to predict the update:</p>

    <ul>
      <li>Momentum Head: Predicts $\mathbf{p}_i(t + \Delta t)$.</li>
      <li>Displacement Head: Predicts $\Delta \mathbf{q}_i(t + \Delta t)$.</li>
    </ul>
  </li>
</ol>

<p>Theoretically, you could swap the backbone for any modern GNN. However, molecular dynamics imposes a strict “non-negotiable” constraint that narrows our choices significantly: E(3) Equivariance.</p>

<h2 id="the-symmetry-challenge-e3-equivariance">The Symmetry Challenge: E(3) Equivariance</h2>
<p>Imagine simulating a water molecule. If you rotate your entire simulation box by 90 degrees, the physics must remain identical. The potential energy should not change, and the force vectors must rotate by exactly 90 degrees to match the atoms.</p>

<p>Standard neural networks see coordinates as simple lists of numbers; they do not inherently “know” that a rotated molecule is the same physical object. If we denote our model as $\mathcal{F}$ and a rotation matrix as $\mathcal{R}$, the model must satisfy:</p>

\[\mathcal{F}(\mathcal{R} \cdot \mathbf{Q}) = \mathcal{R} \cdot \mathcal{F}(\mathbf{Q})\]

<p>If a model fails this test, it might predict that a molecule flies apart simply because it was rotated to face “North” instead of “East.”</p>

<p>There are generally two ways to solve this in Deep Learning:</p>

<ol>
  <li>
    <p>Hard Constraints (e.g., NequIP<d-cite key="NequIP_Batzner2022"></d-cite>, MACE<d-cite key="MACE_ALLEGRO_leimeroth2025machine"></d-cite>): Bake geometric algebra (spherical harmonics) directly into the network layers. This guarantees exact equivariance but is computationally expensive.</p>
  </li>
  <li>
    <p>Soft Constraints (e.g., SchNet<d-cite key="schutt2017schnet"></d-cite>, PET<d-cite key="PET_pozdnyakov2023smooth"></d-cite>): Use a flexible, standard architecture and “teach” it symmetry through data augmentation or frame averaging.</p>
  </li>
</ol>

<h2 id="the-point-edge-transformer-pet">The Point-Edge Transformer (PET)</h2>

<ul>
  <li>is a rotationally unconstrained and transformer-based graph neural network</li>
  <li>PET maintains feature vectors (or messages) f_l ij for every directed bond between atoms i and j that lie within a specified cutoff radius.</li>
  <li>These intermediate representations are updated at each message-passing layer by a transformer</li>
  <li>outputs are subsequently interpreted as the new set of outbound messages from atom i to each neighbor j</li>
  <li>geometric information and chemical species are also incorporated</li>
  <li>A feed-forward NN is used to obtain the desired output/target property</li>
  <li>PET architecture imposes no explicit rotational symmetry constraints, but learns to be equivariant through data augmentation.</li>
  <li>This unconstrained approach yields high theoretical expressivity: even a single layer of the model acts as a universal approximator featuring virtually unlimited body order and angular resolution</li>
</ul>

<p>The PET architecture<d-cite key="PET_pozdnyakov2023smooth"></d-cite> reimagines atomic interactions through the lens of modern Transformers. While standard message-passing GNNs aggregate neighbor information via simple summation, PET introduces a richer mechanism that naturally captures <strong>many-body correlations</strong>—the complex ways in which multiple neighbors jointly influence a central atom.</p>

<p>(XXX add illustration of PET)</p>

<p>The key innovations are:</p>

<ol>
  <li>
    <p><strong>Tokenization of neighbors.</strong> For each central atom, every neighbor within a cutoff radius $R_c$ is encoded into a distinct <em>abstract token</em> that carries both geometric (relative position) and chemical (species) information. Unlike standard GNNs that collapse neighbor information into a single aggregated vector, PET preserves the identity of each interaction.</p>
  </li>
  <li>
    <p><strong>Self-attention over interactions.</strong> These tokens are processed by a Transformer-style self-attention mechanism. This allows the model to learn that the presence of one neighbor dynamically modifies the effective interaction with another—for example, how a third oxygen atom weakens a hydrogen bond between two water molecules. These <strong>many-body effects</strong> emerge naturally from attention, without requiring hand-crafted descriptors.</p>
  </li>
  <li>
    <p><strong>Computational efficiency.</strong> By avoiding the expensive mathematical machinery of spherical harmonics and Clebsch–Gordan coefficients required by strictly equivariant architectures, PET achieves competitive accuracy at significantly lower computational cost. The price is that rotational symmetry must be learned rather than guaranteed.</p>
  </li>
</ol>

<h3 id="enforcing-symmetry-at-runtime">Enforcing Symmetry at Runtime</h3>

<p>XXX check section and shorten it
Since PET is not intrinsically equivariant, FlashMD must enforce symmetry through two complementary mechanisms.</p>

<p><strong>During training</strong>, random rotations are applied to every training sample (<strong>data augmentation</strong>). This teaches the model to produce consistent predictions regardless of molecular orientation—but the resulting equivariance is only approximate, limited by finite training data and model capacity.</p>

<p><strong>During inference</strong>, FlashMD adds a second layer of protection: <strong>Stochastic Frame Averaging</strong>. Before each prediction step:</p>

<ol>
  <li>The entire system $(\mathbf{Q}, \mathbf{P})$ is rotated by a random matrix $\mathcal{R}$.</li>
  <li>The backbone computes the next state in the rotated frame.</li>
  <li>The output is mapped back to the original frame via $\mathcal{R}^{-1}$.</li>
</ol>

\[\mathbf{y}_{\text{final}} = \mathcal{R}^{-1} \cdot \mathcal{F}_{\theta}(\mathcal{R} \cdot \mathbf{x})\]

<p>Over many rollout steps, this ensures that predictions are <strong>statistically invariant</strong> to orientation—any systematic bias toward a particular direction averages out. It is a pragmatic compromise: cheaper than the full Equivariant Coordinate System Ensemble (ECSE) proposed in the original PET paper, but sufficient for the long rollouts FlashMD targets.</p>

<p>The combination of data augmentation and stochastic frame averaging makes PET <em>practically</em> equivariant—not by mathematical proof, but by empirical convergence. Whether this approximation is good enough depends on the application. For the thermostatted benchmarks in the next section, it works remarkably well. For the stricter NVE setting in our <a href="#an-exploratory-study-on-failure-modes">exploratory study</a>, even small symmetry violations may compound.</p>

<h2 id="long-stride-predictions-in-practice">Long-Stride Predictions in Practice</h2>
<p>To demonstrate what FlashMD is capable of, the authors evaluate it across a diverse set of benchmarks and experiments, designed to answer a simple question:
Can we simulate realistic molecular dynamics with much larger time steps – without losing essential physics?</p>

<p>FlashMD is explored in two flavors:</p>

<ul>
  <li>A water-specific model, trained only on liquid water</li>
  <li>A universal model, trained on chemically diverse systems, meant to generalize across molecules and materials</li>
</ul>

<p>This lets the authors study both ends of the spectrum: maximum accuracy for one system, and broad applicability across many.</p>

<h3 id="the-testing-strategy">The Testing Strategy</h3>
<p>Before diving into results, it’s worth understanding how you even benchmark a method like FlashMD. A direct, step-by-step comparison of trajectories is impossible: MD is chaotic – tiny differences grow exponentially, so two simulations will quickly diverge even if both are “correct”.</p>

<p>Instead, the researchers took a statistical approach:</p>

<ol>
  <li>Generate reference trajectories using conventional MD with a reliable force field (PET-MAD)</li>
  <li>Run FlashMD simulations under the same conditions</li>
  <li>Compare statistical properties rather than individual trajectories—things like density, radial distribution functions (how atoms arrange themselves around each other), and phase transition temperatures</li>
</ol>

<p>As the authors note: “we primarily focus our quantitative analysis on time-independent equilibrium properties, and discuss examples where FlashMD qualitatively captures time-dependent behavior.” In other words: check if the average properties match, and see if the dynamics look qualitatively reasonable.</p>

<h3 id="key-results-at-a-glance">Key Results at a Glance</h3>

<p><strong>1. Liquid Water:</strong>
Water might seem simple, but it’s notoriously tricky to simulate due to its hydrogen bonding network. The team tested both their water-specific and universal models on liquid water at 450 K (above the melting point for their particular model).</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Temperature control worked well when using appropriate thermostats (Langevin), with deviations typically under 1 K from target temperature</li>
  <li>Radial distribution functions (which show how oxygen and hydrogen atoms arrange themselves) matched reference MD simulations nearly perfectly</li>
  <li>Density predictions from constant-pressure simulations were accurate for water-specific models and reasonable for universal models</li>
  <li>Models could handle strides up to 16 fs—a 64× speedup compared to the 0.25 fs timesteps typically needed</li>
</ul>

<p><strong>2. Solvated Alanine Dipeptide: Protein-like Dynamics</strong>
This system—a small peptide in water—serves as a minimal model for protein flexibility. The critical test: can FlashMD capture the Ramachandran plot, which maps out the backbone conformations proteins can adopt?</p>

<p>Remarkably, this works even with strides up to 32× larger than standard MD time steps – a strong indication that FlashMD preserves meaningful molecular motion, not just static snapshots.</p>

<p><strong>3. Aluminum Surface: Catching Pre-melting Phenomena</strong>
Metal surfaces exhibit fascinating behavior at high temperatures: atoms start becoming mobile before bulk melting occurs, a phenomenon called pre-melting. This requires capturing subtle, layer-specific dynamics.</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Correctly reproduced the anisotropic softening pattern—surface atoms wiggling more in one direction, second-layer atoms in another</li>
  <li>Captured dynamic defect formation: temporary creation and migration of surface atoms</li>
  <li>Achieved this with 64 fs strides (64× faster than the 1 fs baseline), while still showing physically meaningful atomic trajectories</li>
</ul>

<p>This shows that FlashMD is not limited to molecules, but can handle complex solid-state phenomena.</p>

<p><strong>4. Lithium Thiophosphate: Superionic Phase Transitions</strong>
Perhaps the most impressive demonstration involved a solid-state battery electrolyte material. At high temperatures, lithium atoms become highly mobile in a “superionic” state—critical for battery performance.</p>

<p>The challenge: Predict temperature-dependent lithium conductivity and capture the phase transition.</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Successfully predicted the superionic transition temperature at 675 K, within the expected range</li>
  <li>Reproduced the dramatic increase in lithium ion conductivity across the transition</li>
  <li>Some systematic errors appeared (over/underestimation at low/high temperatures), but the overall behavior was captured with 8× speedup</li>
</ul>

<p>Here, FlashMD demonstrates that it can capture slow, collective processes, which are traditionally hard to access with MD.</p>

<hr>

<h1 id="limitations-and-open-challenges">Limitations and Open Challenges</h1>
<p>FlashMD promises incredible speed ($100\times$), but we have to ask: is the physics still real? Neural networks are pattern matchers, not physics engines. They don’t actually understand laws like energy conservation; they only memorize the data they’ve seen. When we push the model beyond its training distribution, cracks begin to appear.</p>

<h2 id="learning-dynamics-without-physical-guarantees">Learning Dynamics Without Physical Guarantees</h2>

<p>Velocity Verlet comes with mathematical guarantees: energy conservation, symplecticity, time-reversibility. A learned model has no such guarantees. Let’s examine what can go wrong.</p>

<h3 id="1-out-of-distribution-drift">1. Out-of-Distribution Drift</h3>

<p>FlashMD is trained on equilibrium MD trajectories. But during a long rollout, small errors compound. After 1,000 steps (160 ps with 16 fs strides), the system may drift into configurations never seen during training.</p>

<p>Unlike MLIPs—which predict one step ahead—FlashMD’s errors accumulate <strong>autoregressively</strong>. This demands robust uncertainty quantification: the model must know when it doesn’t know.</p>

<h3 id="2-chaotic-dynamics">2. Chaotic Dynamics</h3>

<p>Molecular systems are chaotic: nearby trajectories diverge exponentially (Lyapunov exponent). This imposes a fundamental limit—even a perfect model cannot predict beyond the system’s decorrelation time (typically picoseconds for liquids, nanoseconds for proteins).</p>

<p>This isn’t a bug; it’s physics. FlashMD must capture the <strong>statistical ensemble</strong> of trajectories, not a single deterministic path. This introduces <strong>aleatoric uncertainty</strong>—irreducible randomness from the chaotic dynamics itself.</p>

<h3 id="3-energy-conservation">3. Energy Conservation</h3>

<p>Here’s the Achilles’ heel. Velocity Verlet conserves energy to machine precision. A neural network has no such constraint—small prediction errors cause energy drift:</p>

\[\Delta H = H(\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t}) - H(\mathbf{Q}_t, \mathbf{P}_t)\]

<p>FlashMD addresses this two ways:</p>

<ol>
  <li>
<strong>Training:</strong> Include $|\Delta H|$ in the loss function, encouraging implicit energy conservation</li>
  <li>
<strong>Inference:</strong> Rescale momenta post-prediction to enforce $H_{t+\Delta t} = H_t$ exactly</li>
</ol>

<p>The second approach is aggressive but necessary. However, it modifies the dynamics—we’re no longer solving Hamilton’s equations, but a constrained variant. Does this preserve the correct statistical ensemble? (We’ll return to this question.)</p>

<h3 id="4-symplectic-structure">4. Symplectic Structure</h3>

<p>In Hamiltonian mechanics, phase space has geometric structure (symplecticity) that preserves volume. Classical integrators respect this; neural networks don’t.</p>

<p>Enforcing symplecticity explicitly—via a generating function parameterization—is theoretically possible but computationally prohibitive (it requires computing a 3N × 3N Jacobian). FlashMD takes a pragmatic approach: train on symplectic data (VV trajectories) and hope the model learns the structure implicitly.</p>

<p>This is a gamble. Without explicit enforcement, thermodynamic properties (temperature, pressure, free energies) may drift over long timescales.</p>

<h3 id="5-rotational-symmetry">5. Rotational Symmetry</h3>

<p>Physics is rotationally invariant: if you spin your simulation box, the dynamics shouldn’t change. A standard neural network doesn’t “know” this—it must learn it from data.</p>

<p>FlashMD shows it can maintain physical accuracy while taking dramatically larger steps through time, across a diverse range of materials. But as we’ll see next, this performance comes with important caveats—particularly around energy conservation…</p>

<p>FlashMD mitigates this via:</p>
<ul>
  <li>
<strong>Data augmentation:</strong> Random rotations during training</li>
  <li>
<strong>Runtime augmentation:</strong> Random rotations at each prediction step</li>
</ul>

<h1 id="an-exploratory-study-on-failure-modes">An Exploratory Study on Failure Modes</h1>

<p>To move beyond theoretical concerns, we conduct a systematic exploratory study on a concrete system: a periodic box of 258 TIP3P water molecules simulated with OpenMM as ground truth. We train FlashMD models under varying conditions and evaluate their ability to conserve energy during NVE rollouts—the most unforgiving test of physical validity.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption">Figure 3: Comparison of Maxwell-Boltzmann distributions for different temperatures</figcaption>
  
</figure>

<h2 id="experimental-setup">Experimental Setup</h2>

<p><strong>Ground Truth Generation.</strong> We generate NVE trajectories at different temperatures (200K–700K, 20K steps) using the TIP3P force field in OpenMM, saving configurations every 0.5 fs for 10 ps each. We verify thermodynamic consistency by comparing the velocity distributions against the Maxwell-Boltzmann distribution at each temperature (<a href="#fig:maxwell">Figure 3</a>).</p>

<p><strong>Training Data.</strong> From these trajectories, we construct FlashMD training pairs $(q_t, p_t) \to (q_{t+\Delta t}, p_{t+\Delta t})$ at a prediction stride of $\Delta t = 1\,\text{fs}$. Following the original paper, targets are stored as mass-scaled quantities: $\Delta\tilde{q}_i = \Delta q_i \sqrt{m_i}$ and $\tilde{p}_i = p_i / \sqrt{m_i}$. XXX check again this in code</p>

<p><strong>Evaluation Protocol.</strong> Each trained model is deployed in NVE simulation for 50 ps (50,000 steps at 1 fs) starting from a 300 K equilibrated configuration. We track total energy $E_\text{tot}(t)$, temperature $T(t)$, and energy drift rate $\dot{E}$ (eV/ps). A model is considered “exploded” if $T &gt; 1000\,\text{K}$ at any point.</p>

<p>XXX before ablation show the trained models are stable but lack the energy conservation leading to huge energy and temperature drifts.</p>

<p>The first results of the simulations depict in Figure XX. We trained the models accordingly to the setup above. We also investigated different starting points from the NVE groundtruth to see the bahaviour od the model as well as different seeds to handle stochastic training varaicen. All custom models show that all of the trained models achieve stable NVE simulations. But all of them lack the energy conservation drastically. We see huge drifts in energy and temperature. This indicates us that the model loses some energy somehow. To understand this behaviour we also want to take a look at the momenta distribution of the models compared to the ground truth. Since the momenta relates directly to the potential energy this is traight forward.</p>

<p>XXX Image of plots fro the distributions here.</p>

<p>As we can see. All of custom models exhibit extensive differences in the momenta of Oxygen atoms. But interestingly we can see that if turn on the rescaling filter we can even better distributions. This leads us to question how could we improve the momentum distrubtion of the model by doing two things:</p>
<ul>
  <li>Loss weighting between positons and momenta</li>
  <li>Applying mass sacling to the positons and momenta</li>
</ul>

<h2 id="ablation-1-loss-weighting-between-positions-and-momenta">Ablation 1: Loss Weighting Between Positions and Momenta</h2>

<p>The first question is simple: does it matter how much the model cares about getting momenta right vs. positions? We train models with momentum loss weights $w_p \in {0.5, 1.0, 1.5, 2.0, 10.0}$ while keeping the position weight fixed at $w_q = 1.0$, and let them run NVE for 10 ps.</p>

<p>The results are… surprising.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>$w_p$</th>
      <th>Drift (eV/ps)</th>
      <th>$\bar{T}$ (K)</th>
      <th>$\sigma_T$ (K)</th>
      <th>Stable until</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$w_p=0.5$</td>
      <td>0.5</td>
      <td>2813.95</td>
      <td>513.8</td>
      <td>148.9</td>
      <td>8.1 ps ✗</td>
    </tr>
    <tr>
      <td>$w_p=1.0$ (Baseline)</td>
      <td>1.0</td>
      <td>-3.70</td>
      <td>227.1</td>
      <td>34.2</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>$w_p=1.5$</td>
      <td>1.5</td>
      <td>-4.80</td>
      <td>178.0</td>
      <td>38.8</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>$w_p=2.0$</td>
      <td>2.0</td>
      <td>52.80</td>
      <td>476.1</td>
      <td>168.5</td>
      <td>2.7 ps ✗</td>
    </tr>
    <tr>
      <td>$w_p=10.0$</td>
      <td>10.0</td>
      <td>4.39</td>
      <td>334.9</td>
      <td>67.5</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>Ground Truth</td>
      <td>—</td>
      <td>-0.0035</td>
      <td>304.7</td>
      <td>6.9</td>
      <td>&gt;10 ps ✓</td>
    </tr>
  </tbody>
</table>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectories for three momentum loss weights and OpenMM ground truth. Top: temperature evolution. Bottom: total energy. The ground truth (black) is essentially flat — none of the models come close.</figcaption>
  
</figure>

<p>If you expected “more momentum weight = better momenta = more stable,” you’d be wrong. The relationship is wildly non-monotonic, and we can group the results into three regimes:</p>

<p><strong>Too little ($w_p = 0.5$): immediate explosion.</strong> The model doesn’t learn momentum dynamics well enough and blows up within 8 ps. Temperature rockets past 500 K, energy drift is off the charts. Lesson: you <em>need</em> to care about momenta.</p>

<p><strong>The “moderate” zone ($w_p = 1.0, 1.5$): stable but freezing.</strong> These models survive the full 10 ps — but look at the temperatures! The baseline cools from 300 K to a mean of 227 K, and $w_p = 1.5$ cools even further to 178 K. The model is systematically predicting momenta that are too small. It’s not exploding, but it’s not doing physics either — it’s slowly bleeding kinetic energy. You can see this clearly in <a href="#fig:ablation1">Figure X</a>: the blue and green lines drift steadily downward while the ground truth holds at ~305 K.</p>

<p><strong>Cranked to 10 ($w_p = 10.0$): closest to reality, but noisy.</strong> Here’s the twist: the most extreme weight actually produces the most realistic temperature (335 K, just 10% above target). Looking at <a href="#fig:ablation1">Figure X</a>, the red $w_p=10$ line tracks the ground truth best during the first ~5 ps. But the fluctuations are huge ($\sigma_T = 67$ K vs. ground truth’s 7 K), and by 8 ps it starts slowly heating up.</p>

<p>To our suprise, $w_p = 2.0$ explodes at 2.7 ps. Somehow, 2× the baseline is catastrophically unstable while 10× is fine. The loss landscape clearly has some sharp cliffs in this region.</p>

<p><strong>The bottom line:</strong> Look at the energy panel in <a href="#fig:ablation1">Figure X</a>. The ground truth (black line) is essentially a flat line. Every single model drifts visibly — the best ones still have energy drift <strong>1000× larger</strong> than OpenMM. Loss weighting can shift the failure mode from cooling to heating, but it can’t bridge that gap.</p>

<p>This tells us something important: the problem isn’t <em>how much</em> the model cares about momenta — it’s <em>how</em> it represents them. Which brings us to our next experiment: what if we change the loss to account for the fact that hydrogen and oxygen have very different masses?</p>

<p>much* the model cares about momenta, we change <em>how</em> it represents them through mass-scaled loss formulations. –&gt;</p>

<h2 id="ablation-2-mass-scaled-loss-functions">Ablation 2: Mass-Scaled Loss Functions</h2>
<p>Standard MSE treats all atoms equally in momentum space. But since $p_i = m_i v_i$, oxygen atoms (mass 16) dominate the momentum loss by a factor of $16^2 = 256$ compared to hydrogen (mass 1). This means the model optimizes primarily for oxygen momenta while hydrogen velocity errors remain large.</p>

<p>Following the FlashMD paper, we implement a mass-scaled loss:</p>

\[\mathcal{L}_{\tilde{p}} = \frac{1}{N} \sum_i \frac{\|p_i^\text{pred} - p_i^\text{true}\|^2}{m_i}, \qquad \mathcal{L}_{\Delta\tilde{q}} = \frac{1}{N} \sum_i \|\Delta q_i^\text{pred} - \Delta q_i^\text{true}\|^2 \cdot m_i\]

<p>This is equivalent to computing MSE on the mass-scaled quantities $\tilde{p} = p/\sqrt{m}$ and $\Delta\tilde{q} = \Delta q \cdot \sqrt{m}$, ensuring that velocity errors are weighted equally regardless of atomic mass.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectory comparison between standard MSE and mass-scaled MSE loss. Left column shows the initial drift phase (0–10 ps), right column the full 50 ps trajectory. Both models exhibit systematic cooling relative to the ground truth.</figcaption>
  
</figure>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Loss Type</th>
      <th>Drift (eV/ps)</th>
      <th>$\bar{T}$ (K)</th>
      <th>$\sigma_T$ (K)</th>
      <th>$\sigma_E$ (kJ/mol)</th>
      <th>Stable until</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Custom Trained</td>
      <td>MSE</td>
      <td>-0.6842</td>
      <td>207.1</td>
      <td>23.0</td>
      <td>591.1</td>
      <td>&gt;50 ps</td>
    </tr>
    <tr>
      <td>Mass-Scaled Custom Trained</td>
      <td>mass_scaled_mse</td>
      <td>-0.6989</td>
      <td>212.9</td>
      <td>24.9</td>
      <td>642.0</td>
      <td>&gt;50 ps</td>
    </tr>
    <tr>
      <td>Ground Truth (OpenMM)</td>
      <td>—</td>
      <td>-0.0003</td>
      <td>304.2</td>
      <td>6.9</td>
      <td>1.2</td>
      <td>&gt;50 ps</td>
    </tr>
  </tbody>
</table>

<p><strong>Key finding:</strong> Mass-scaling provides a modest improvement in the early drift phase but does not fundamentally alter long-term stability. A time-windowed analysis reveals the nuance:</p>

<ul>
  <li>
<strong>0–5 ps:</strong> Mass-scaling reduces energy drift by 26% (-3.72 vs. -5.03 eV/ps) and keeps the temperature closer to 300 K (272 K vs. 254 K, $\sigma_T$ halved from 26 to 13 K). The model clearly benefits from balanced treatment of H and O atoms in the initial phase.</li>
  <li>
<strong>0–10 ps:</strong> The advantage persists but narrows—drift is reduced by 24% (-2.82 vs. -3.70 eV/ps), mean temperature 252 K vs. 227 K.</li>
  <li>
<strong>0–50 ps:</strong> Both models converge to nearly identical behavior—drift rates of -0.68 and -0.70 eV/ps, mean temperatures of 207 K and 213 K. The early advantage is fully washed out.</li>
</ul>

<p><strong>Physical interpretation:</strong> Mass-scaling correctly addresses the <em>symptom</em>—unequal error weighting across species—but not the <em>disease</em>. Both models systematically cool from 300 K to ~210 K over 50 ps, losing roughly $\frac{1}{3}$ of their kinetic energy. This cooling pattern ($\dot{E} \approx -0.7$ eV/ps, $\sigma_E \sim 600$ kJ/mol vs. ground truth $\sigma_E = 1.2$ kJ/mol) represents a $500\times$ violation of energy conservation that no loss reweighting can fix.</p>

<p>The fact that mass-scaling helps <em>early</em> but not <em>late</em> suggests that it improves the model’s initial momentum predictions (reducing the per-species bias), but the accumulated errors from the autoregressive rollout eventually dominate regardless. The fundamental issue is that FlashMD’s single-step prediction errors, while small individually, compound systematically rather than canceling stochastically—a hallmark of non-symplectic integration.</p>

<h2 id="summary-of-findings">Summary of Findings</h2>

<p>Our exploratory study reveals a consistent pattern across all model variants:</p>

<ol>
  <li>
    <p><strong>All models exhibit systematic energy drift</strong>, even in the quasi-stable 
regime. This is expected—the model has no symplectic structure or 
energy-conserving inductive bias.</p>
  </li>
  <li>
    <p><strong>Loss weighting affects the drift rate</strong> but does not eliminate it. 
Mass-scaled losses with appropriate momentum weighting achieve the 
best short-term stability.</p>
  </li>
  <li>
    <p><strong>Failure is always catastrophic once it begins.</strong> There is no graceful 
degradation—once the system leaves the training distribution, errors 
compound exponentially.</p>
  </li>
</ol>

<p>These findings suggest that improving the loss function alone is insufficient. The fundamental challenge is distributional: the model must either (a) be trained on data hat covers the states it will visit during long rollouts, or (b) incorporate physical constraints that prevent drift toward unphysical regions.</p>

<hr>

<h1 id="conclusion">Conclusion</h1>

<p>In this post, we traced the two fundamental bottlenecks of molecular dynamics and showed how each has been addressed: MLIPs solved the force bottleneck; FlashMD bypasses the integrator entirely, predicting system evolution at strides one to two orders of magnitude beyond classical stability limits. Across water, proteins, metals, and battery electrolytes, it recovers the correct statistical physics at speedups of 8× to 64×.</p>

<p>But our independent exploratory study reveals a sobering counterpoint. When we strip away the thermostats that regulate temperature in standard benchmarks, the learned dynamics fail to conserve energy. Every model we tested—regardless of loss weighting or mass scaling—exhibits systematic energy drift orders of magnitude larger than classical integrators. The problem is structural, not parametric: no amount of loss function tuning resolved it.</p>

<p>This does not diminish FlashMD’s contribution—it is the first model to learn meaningful long-stride dynamics across chemically diverse systems. But it makes clear what is still missing: the mathematical guarantees that classical integrators provide for free. Until symplectic architectures, hybrid corrective schemes, or active learning strategies close this gap, FlashMD is best understood as a powerful tool for thermostatted simulations rather than a general-purpose replacement for classical integrators.</p>

<p>The question we posed in the introduction—<em>Can a model learn to respect the laws of physics without being explicitly taught to do so?</em>—now has a nuanced answer. For statistical properties sampled under external control, yes. For the strict, unassisted conservation of energy that defines Hamiltonian dynamics, not yet.</p>

<h2 id="xxx-propose-a-way-scetch-a-solution-maybe-add-first-experiments-with-handselected-active-learning-">XXX Propose a way, scetch a solution maybe add first experiments with handselected active learning …</h2>

<h1 id="conclusion-1">Conclusion</h1>
<p><em>(Your text here…)</em></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/2026/assets/bibliography/2026-04-27-flashmd.bib"></d-bibliography>

      <d-article>
        
          
            

  
    
    
      <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    

    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    
      <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/distill-example/">Sample Blog Post</a>
    
  </li>

  

  <li class="my-2">
    
      <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/breaking-the-femtosecond-prison-promise-and-limits-of-flashmd/">Breaking the Femtosecond Prison: Promise and Limits of FlashMD</a>
    
  </li>



          
        
        <br>
        <br>
        
        
      </d-article>
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  © Copyright 2026
  ICLR
  
  Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

  
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/2026/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

<!-- Custom overrides -->
<script src="/2026/assets/js/distillpub/overrides.js"></script>






















  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Removed Badges -->


  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
  <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  <!-- Removed Pseudocode -->









  <!-- Scrolling Progress Bar -->
  <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/2026/assets/js/search-data.js"></script>
  <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>


  
</body>
</html>
