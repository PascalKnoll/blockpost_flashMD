<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://0.0.0.0:8080/2026/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:8080/2026/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-02-15T22:56:00+00:00</updated><id>http://0.0.0.0:8080/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Sample Blog Post</title><link href="http://0.0.0.0:8080/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post" /><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>http://0.0.0.0:8080/2026/blog/2026/distill-example</id><content type="html" xml:base="http://0.0.0.0:8080/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a>
that brought a significant improvement to the loading and rendering speed, which is now
<a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<h2 id="images-and-figures">Images and Figures</h2>

<p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
To include images in your submission in this way, you must do something like the following:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div>

<p>which results in the following image:</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

<p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory
<code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p>

<p>Please avoid using the direct markdown method of embedding images; they may not be properly resized.
Some more complex ways to load images (note the different styles of the shapes/shadows):</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h3 id="interactive-figures">Interactive Figures</h3>

<p>Here’s how you could embed interactive figures that have been exported as HTML files.
Note that we will be using plotly for this demo, but anything built off of HTML should work
(<strong>no extra javascript is allowed!</strong>).
All that’s required is for you to export your figure into HTML format, and make sure that the file
exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory.
To embed it into any page, simply insert the following code anywhere into your page.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div>

<p>For example, the following code can be used to generate the figure underneath it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>And then include it with the following:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div>

<p>Voila!</p>

<div class="l-page">
  <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe>
</div>

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <a-cite key="gregor2015draw">&lt;/d-cite&gt; (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</a-cite></p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
It supports more than 100 languages.
This example is in C++.
All you have to do is wrap your code in a liquid tag:</p>

<p>{% highlight c++ linenos %} <br /> code code code <br /> {% endhighlight %}</p>

<p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="diagrams">Diagrams</h2>

<p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly.
Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p>

<p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div>

<pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre>

<hr />

<h2 id="tweets">Tweets</h2>

<p>An example of displaying a tweet:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p>An example of pulling from a timeline:</p>
<div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p>

<hr />

<h2 id="blockquotes">Blockquotes</h2>

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item</li>
</ol>

<ul>
  <li>Unordered sub-list.</li>
</ul>

<ol>
  <li>Actual numbers don’t matter, just that it’s a number
    <ol>
      <li>Ordered sub-list</li>
    </ol>
  </li>
  <li>
    <p>And another item.</p>

    <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

    <p>To have a line break without a paragraph, you will need to use two trailing spaces.
Note that this line is separate, but within the same paragraph.
(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p>
  </li>
</ol>

<ul>
  <li>
    <p>Unordered lists can use asterisks</p>
  </li>
  <li>
    <p>Or minuses</p>
  </li>
  <li>
    <p>Or pluses</p>
  </li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics</title><link href="http://0.0.0.0:8080/2026/blog/2026/flashmd_old/" rel="alternate" type="text/html" title="OLD: FlashMD - Bypassing the Integrator for Long-Timescale Dynamics" /><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>http://0.0.0.0:8080/2026/blog/2026/flashmd_old</id><content type="html" xml:base="http://0.0.0.0:8080/2026/blog/2026/flashmd_old/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Molecular Dynamics (MD) is often called the “computational microscope” of modern science. By simulating atoms obeying Newton’s laws, we can watch proteins fold, batteries charge, and materials fracture—all at the atomic scale. If we can simulate the atoms, we can predict the material.</p>

<p>But there’s a brutal trade-off: <strong>accuracy versus time</strong>.</p>

<p>Machine Learning Interatomic Potentials (MLIPs) recently solved the first bottleneck—calculating forces—by replacing expensive quantum calculations with learned approximations. But a second, more stubborn barrier remains: the <strong>femtosecond prison</strong>. Traditional integrators must take tiny time steps ($\sim10^{-15}$ s) to remain stable<d-cite key="leach2001timestep"></d-cite>. To observe a biological process lasting milliseconds requires <strong>trillions of sequential steps</strong>—each depending on the last.</p>

<p>FlashMD shatters this chain. By directly predicting the system’s state over time steps $1-2$ orders of magnitude higher than the stability limit of traditional integrators, it shifts the paradigm from <strong>simulation to emulation</strong>. Instead of numerically integrating forces at every femtosecond, FlashMD learns to leap forward in time, promising to collapse simulations that once took weeks into hours.</p>

<p><strong>But here’s the catch:</strong> Can a learned emulator remain physically stable?</p>

<p>Traditional integrators come with mathematical guarantees—energy conservation, time-reversibility, symplectic structure. FlashMD trades these guarantees for speed. It learns dynamics purely from data. In this post, we’ll explore FlashMD’s architecture and ambitions—and then critically examine whether learned emulation can escape the femtosecond prison without losing physical validity. Through an exploratory study, we’ll see where it breaks down and what that reveals about the future of learned simulators.</p>

<!-- Molecular Dynamics (MD) is often described as the "computational microscope" of modern science. By solving Newton’s equations of motion for atomistic systems, it provides our only window into the dynamic behavior of matter—from the folding of proteins to the diffusion of ions in a battery. If we can simulate the atoms, we can predict the macroscopic properties of the material.

However, a frustrating trade-off has always plagued these simulations: the battle between **accuracy** and **time**.

While the introduction of Machine Learning Interatomic Potentials (MLIPs) successfully bridged the gap between quantum accuracy and classical speed for calculating forces, a second, more stubborn bottleneck remains. Standard simulations are shackled by the stability of their numerical integrators, which require time steps on the order of femtoseconds ($10^{-15}$ s)<d-cite key="leach2001timestep"></d-cite> . To observe biological processes that occur over milliseconds ($10^{-3}$ s), a simulation must execute trillions of sequential steps.

FlashMD is a new machine learning framework that attempts to shatter this barrier. Instead of painstakingly integrating the forces on every atom at every femtosecond, FlashMD uses a deep-learning architecture to "leap" forward in time. By directly predicting the system's state over time steps $1-2$ magnitudes higher than the stability limit of traditional integrators, it aims to shift the paradigm from simulation to emulation.

In this post, we will first explore the fundamental bottlenecks that made FlashMD necessary. We will then dive into its architecture, and finally, offer a critical analysis of its physical validity—specifically regarding the crucial issue of energy conservation. -->

<hr />

<h1 id="molecular-dynamics-fundamentals">Molecular Dynamics Fundamentals</h1>
<p>Molecular Dynamics (MD) is the computational engine behind our understanding of matter in motion. At its core, MD follows a straightforward recipe: place $N$ atoms in a box, calculate the forces between them, and step forward in time using Newton’s equations.</p>

<p>The challenge lies in the scale. As shown in the workflow below, every MD simulation is built around a <strong>core loop</strong> that must be executed millions to billions of times:</p>

<ol>
  <li><strong>Calculate forces</strong> on every atom (Step 2)</li>
  <li><strong>Integrate equations of motion</strong> to update positions and velocities (Step 3)</li>
  <li>Repeat for $10^6$ to $10^9$ steps</li>
</ol>

<p>[INSERT FIGURE HERE]</p>

<p>This repetition is fundamental, not a limitation. To extract meaningful thermodynamic properties—diffusion coefficients, phase transitions, reaction rates—we need trajectories long enough to sample the system’s accessible states. A single nanosecond of physical time typically requires around a million integration steps.</p>

<p>This raises a natural question: <strong>which step consumes the most time?</strong></p>

<!-- Molecular Dynamics (MD) is often described as a "computational microscope." It allows researchers to observe how atoms and molecules interact over time, providing insights into dynamic behaviors that static images cannot capture.

In many ways, an MD simulation parallels a real laboratory experiment. The workflow typically follows three stages:

1. **Preparation (Equilibration):** We select a model system of $N$ particles and solve Newton's equations until the system settles into a stable state.
2. **Measurement (Production Run):** We evolve the system further to measure macroscopic properties—such as temperature, pressure, or diffusion coefficients
3. **Analysis:** Since instantaneous measurements are noisy, we average these properties over time to obtain statistically significant results. -->

<p>XXX Illustration of a MD Simulation witht the steps</p>

<h2 id="the-hamiltonian-view-of-atomic-motion">The Hamiltonian View of Atomic Motion</h2>

<p>To understand where the computational bottleneck lies, we need to look inside the simulation loop. At each step, the system evolves according to Hamilton’s equations of motion—the fundamental laws governing how atoms move.</p>

<p>The core idea is simple: <strong>forces come from energy gradients</strong>. We describe the system’s total energy using the Hamiltonian $H$, which splits into two parts:</p>

\[H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}\]

<p>where \(\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N\) are atomic positions, \(\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N\) are momenta, and \(V(\mathbf{Q})\) is the potential energy surface (PES).</p>

<p>Hamilton’s equations tell us how the system evolves:</p>

\[\frac{d\mathbf{q}_i}{dt} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial V}{\partial \mathbf{q}_i}\]

<p>The second equation is crucial: <strong>the force on each atom is the negative gradient of the potential energy</strong>. To simulate the system, we need two things:</p>

<ol>
  <li>A way to compute $V(\mathbf{Q})$ and its gradient $\nabla V$.</li>
  <li>A way to discretize continuous time into finite steps $\Delta t$.</li>
</ol>

<p><strong>For the second requirement, we use numerical integration.</strong></p>

<h3 id="the-velocity-verlet-algorithm">The Velocity Verlet Algorithm</h3>

<p>To solve these continuous equations on a computer, we discretize time using the Velocity Verlet integrator:</p>

\[\begin{aligned}
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t \\
    \mathbf{q}_i &amp;\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t
\end{aligned}\]

<p>This algorithm is <strong>symplectic</strong>, meaning it approximately conserves the Hamiltonian even with finite $\Delta t$. But there’s a catch: stability requires $\Delta t \sim 0.5\text{–}1$ femtoseconds<d-cite key="leach2001timestep"></d-cite>. Larger steps cause energy drift and numerical explosions. <strong>This is the femtosecond prison</strong>—the fundamental timestep barrier that limits all classical MD.</p>

<p>Every loop iteration requires:</p>
<ul>
  <li><strong>Computing forces</strong> via $\nabla V$ (expensive)</li>
  <li><strong>Taking a tiny timestep</strong> (limiting)</li>
</ul>

<p>We’ll address these bottlenecks in sequence, starting with force computation.</p>

<!-- Before navigating the landscape of MLIPs, we must ground ourselves in the Hamiltonian formalism. You can think of the Hamiltonian as the non-negotiable "rulebook" for the system. It is a scalar function that, at any instance, quantifies the total energy of the system. This energy is the sum of two distinct components:

1. **Kinetic Energy ($K$):** The energy of motion, dependent on particle momenta $\mathbf{P}$.
2. **Potential Energy ($V$):** The energy "stored" in atomic configurations and interactions, dependent on particle positions $\mathbf{Q}$.


For an isolated atomistic system with $N$ atoms, masses $m_i$, positions $$\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N$$, and momenta $$\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N$$, the Hamiltonian $H$ takes the following canonical form:

$$
H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}
$$

A fundamental law of physics is that in an isolated system (microcanonical ensemble), this Hamiltonian $H$ is conserved ($\frac{dH}{dt} = 0$). This conservation is the constraint our simulation must respect.

How do the atoms "know" how to move to obey this rule? They follow Hamilton's equations of motion:

$$
\frac{d\mathbf{q}_i}{dt} = \frac{\partial H}{\partial \mathbf{p}_i} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial H}{\partial \mathbf{q}_i}
$$

The first equation simply relates velocity to momentum. The second equation is the physical engine: it states that the time evolution of momentum (the force) is driven strictly by the negative gradient of the potential energy surface (PES), denoted as $V(\mathbf{Q})$. XXX where is V(Q) in the formula?

To solve these continuous equations on a discrete computer, we must turn the infinitely smooth $dt$ into a concrete time step, $\Delta t$. The standard algorithm for this task is the Velocity Verlet (VV) integrator. A single step updates the system as follows:

$$
\begin{aligned}
    \mathbf{p}_i &\leftarrow \mathbf{p}_i - \frac{1}{2} \frac{\partial V}{\partial \mathbf{q}_i} \Delta t \\
    \mathbf{q}_i &\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\
    \mathbf{p}_i &\leftarrow \mathbf{p}_i - \frac{1}{2} \frac{\partial V}{\partial \mathbf{q}_i} \Delta t
\end{aligned}
$$

This loop is the beating heart of molecular dynamics. However, to keep this heart beating, we must repeatedly evaluate the gradient term: $\nabla_i V(\mathbf{Q})$. XXX highlight quickly teh force calculation and small times tep prison -->

<h2 id="why-ab-initio-molecular-dynamics-is-expensive">Why <em>ab Initio</em> Molecular Dynamics Is Expensive</h2>

<p>The first bottleneck is evaluating $V(\mathbf{Q})$ and its gradient. Historically, this forced a painful compromise:</p>

<ul>
  <li>
    <p><strong><em>Ab initio</em> methods</strong> (e.g., Density Functional Theory) solve quantum mechanics to compute forces with chemical accuracy. But they require solving the electronic structure problem at every step, with computational cost scaling as $O(N^3)$ or worse—and large constant factors that make even small systems expensive<d-cite key="zhang2018deep"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Classical force fields</strong> use handcrafted functions (harmonic springs, Lennard-Jones potentials, Coulomb terms) that scale linearly with system size. But these predefined functional forms—fixed at design time—cannot adapt to bond breaking, chemical reactions, or complex polarization effects that require quantum mechanical treatment (XXX Source).</p>
  </li>
</ul>

<p>For decades, this accuracy-efficiency trade-off defined the field. Quantum accuracy meant tiny systems; large-scale simulations meant sacrificing chemistry.</p>

<p>Machine Learning Interatomic Potentials (MLIPs) changed this.</p>

<!-- The central challenge of MD has always been evaluating this potential $V$. Historically, this forced a painful compromise. On one side, ab initio methods like Density Functional Theory (DFT) offer quantum-mechanical accuracy but scale poorly, restricting simulations to tiny systems and picosecond timescales <d-cite key="zhang2018deep"></d-cite>. 

On the other, classical force fields offer linear scaling ($O(N)$) and speed, but rely on rigid, heuristic approximations that often fail to capture complex chemical reactivity and bond breaking. XXX I dont like this sentence

This accuracy-efficiency trade-off defined the field for decades, until Machine Learning Interatomic Potentials (MLIPs) provided a way to bridge the gap. -->

<hr />
<h1 id="machine-learned-interatomic-potentials-solving-the-force-bottleneck">Machine-Learned Interatomic Potentials: Solving the Force Bottleneck</h1>

<p>We’ve identified two computational bottlenecks in MD. Let’s tackle the first one: <strong>computing forces</strong>.</p>

<p>Recall that every timestep requires evaluating $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$—the gradient of the potential energy surface. Historically, this meant choosing between quantum accuracy (expensive) or classical speed (inaccurate). MLIPs broke this trade-off.</p>

<h2 id="the-mlip-breakthrough">The MLIP Breakthrough</h2>

<p>The core idea is simple: <strong>replace quantum calculations with a learned function</strong>.</p>

<p>An MLIP is a neural network that maps atomic positions $\mathbf{Q}$ and atomic numbers $\mathbf{Z}$ directly to potential energy:</p>

\[V_\theta(\mathbf{Q}) \approx V_{\text{QM}}(\mathbf{Q})\]

<p>Forces are then obtained via automatic differentiation:</p>

\[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} V_\theta(\mathbf{Q})\]

<p>This bypasses the $O(N^3)$ cost of solving the electronic structure problem at every step.</p>

<h3 id="architecture-graph-neural-networks">Architecture: Graph Neural Networks</h3>

<p>Modern MLIPs—such as SchNet<d-cite key="schutt2017schnet"></d-cite>, NequIP<d-cite key="batzner2022nequip"></d-cite>, and MACE<d-cite key="batatia2022mace"></d-cite>—use <strong>Graph Neural Networks</strong> (GNNs) that:</p>

<ol>
  <li><strong>Encode molecular structure naturally</strong>: Atoms are nodes, interactions are edges</li>
  <li><strong>Respect physical symmetries</strong>: Predictions are invariant to translation, rotation, and atom permutation</li>
  <li><strong>Learn many-body interactions</strong>: Message-passing layers aggregate information from neighboring atoms</li>
</ol>

<p>The training objective fits both energies and forces jointly:</p>

\[\mathcal{L}(\theta) = \lambda_E \|V_\theta - V_{\text{DFT}}\|^2 + \lambda_F \|\mathbf{F}_{\text{pred}} - \mathbf{F}_{\text{DFT}}\|^2\]

<p>Training on forces directly improves generalization: force supervision provides richer gradient information and helps the model handle out-of-distribution configurations<d-cite key="chmiela2018sgdml"></d-cite>.</p>

<h3 id="impact-quantum-accuracy-at-classical-speed">Impact: Quantum Accuracy at Classical Speed</h3>

<p>The speedup is dramatic. Where DFT takes <strong>minutes</strong> per force evaluation, MLIP inference takes <strong>milliseconds</strong>—a <strong>1000× improvement</strong><d-cite key="he2025mlipsbio"></d-cite>. This has enabled:</p>

<ul>
  <li><strong>Larger systems</strong>: Million-atom simulations that were previously impossible</li>
  <li><strong>Longer timescales</strong>: Microsecond trajectories with quantum accuracy</li>
  <li><strong>New applications</strong>: Drug discovery, battery materials, catalysis<d-cite key="unke2021spookynet"></d-cite></li>
</ul>

<p>MLIPs have fundamentally changed what’s computationally feasible in molecular simulation.</p>

<h2 id="the-remaining-challenge-the-femtosecond-prison">The Remaining Challenge: The Femtosecond Prison</h2>

<p>MLIPs solved the force bottleneck—<strong>Bottleneck #1</strong>. But <strong>Bottleneck #2</strong> remains stubbornly unsolved.</p>

<p>As we established earlier, classical integrators require $\Delta t \sim 10^{-15}$ s to maintain numerical stability. This means simulating a microsecond—the timescale of protein folding or molecular recognition—still requires $10^9$ sequential steps, regardless of how fast we can compute forces.</p>

<p><strong>Even with instantaneous force predictions, the serial nature of integration makes long-timescale phenomena computationally intractable.</strong></p>

<p>To escape the femtosecond prison, we cannot simply accelerate the integrator. We must <strong>bypass it entirely</strong>—replacing step-by-step integration with direct trajectory prediction.</p>

<p>This is where FlashMD enters.
&lt;!– # Machine-Learned Interatomic Potentials: Solving the Force Bottleneck
We have established that we need billions of time steps to simulate meaningful biological or material phenomena. This brings us to the second half of the computational burden: the cost of a single step.</p>

<p>As derived in the Hamiltonian framework, every single step requires us to evaluate the gradient of the potential energy surface: $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$.</p>

<h2 id="the-mlip-breakthrough-1">The MLIP Breakthrough</h2>

<p>At its core, an MLIP is a regression framework that approximates the Potential Energy Surface (PES) by learning a mapping $\mathcal{F}_\theta: (\mathbf{Z}, \mathbf{Q}) \to \mathbb{R}$ from atomic numbers and coordinates directly to the scalar potential energy. This effectively bypasses the $O(N^3)$ cost of solving the electronic structure explicitly.</p>

<p>Unlike classical force fields, MLIPs leverage deep neural networks—typically Graph Neural Networks (GNNs) or Message Passing Neural Networks (MPNNs)—to serve as universal approximators of the quantum mechanical interaction <d-cite key="he2025mlipsbio"></d-cite>. This allows them to capture complex, non-local many-body effects that classical approximations inherently miss.</p>

<p>Crucially, MLIPs enforce physical consistency by defining atomic forces as the exact negative gradient of the predicted energy with respect to atomic positions via automatic differentiation:</p>

\[\mathbf{F}_{\text{pred}} = -\nabla_{\mathbf{Q}} E_{\text{pred}}(\mathbf{Q})\]

<p>The training objective is therefore a multi-task learning problem. We optimize the network parameters $\theta$ to minimize a composite loss against ground-truth quantum mechanical labels (typically from DFT):</p>

\[\mathcal{L}(\theta) = \lambda_E \|E_{\text{pred}} - E_{\text{DFT}}\|^2 + \lambda_F \|\underbrace{-\nabla_{\mathbf{Q}} E_{\text{pred}}}_{\mathbf{F}_{\text{pred}}} - \mathbf{F}_{\text{DFT}}\|^2\]

<p>By training on high-quality snapshots of molecular configurations—generated via random sampling or active learning—MLIPs can capture complex, many-body interactions that classical methods simply cannot see. XXX source</p>

<h2 id="why-faster-forces-are-still-not-enough">Why Faster Forces Are Still Not Enough</h2>
<p>Machine Learning Interatomic Potentials (MLIPs) have revolutionized the field by reducing the computational cost of force calculation ($F$) by orders of magnitude compared to DFT. However, they leave the fundamental architectural flaw of MD untouched: the integrator bottleneck.</p>

<p>Classical integrators like Velocity Verlet face a hard physical speed limit. To maintain numerical stability and energy conservation, the time step $\Delta t$ must resolve the fastest atomic vibrations in the system—typically the oscillation of hydrogen bonds. This confines simulations to the femtosecond scale ($\Delta t \approx 10^{-15} \text{s}$), regardless of how fast the force model is.</p>

<p>This creates a massive discrepancy between simulation time and biological reality. To simulate a mere microsecond of physical time—relevant for protein folding or drug binding—we must perform one billion sequential steps:</p>

\[N_{\text{steps}} = \frac{10^{-6} \text{ s}}{10^{-15} \text{ s}} = 10^9 \text{ steps}\]

<p>We are effectively trapped in a “femtosecond prison.” Even with instant force predictions, this serial dependency makes long-timescale phenomena computationally intractable. To escape this, we cannot simply accelerate the integrator; we must bypass it entirely. –&gt;</p>

<hr />

<h1 id="flashmd-escaping-the-femtosecond-prison">FlashMD: Escaping the Femtosecond Prison</h1>

<p>FlashMD introduces a transformative approach: rather than incrementally integrating forces like a standard force field, it operates as a direct trajectory predictor.</p>

<p>Instead of acting as a “middleman”—predicting energy to derive forces for a classical integrator—FlashMD learns the dynamical map directly from simulation data:</p>

\[\mathcal{G}_\theta: (\mathbf{Q}_t, \mathbf{P}_t) \to (\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t})\]

<p>This paradigm shift enables the model to predict the system’s next state in a single forward pass, replacing hundreds of small integration steps and allowing for strides 1-2 magnitudes larger than the stability limit of numerical integrators.</p>

<p>XXX Here Illustration of Classical MD Loop vs. FlashMD Loop</p>

<h2 id="architecture-and-design-principles-of-flashmd">Architecture and Design Principles of FlashMD</h2>
<!-- The FlashMD Architecture -->
<p>To realize the dynamical map $\mathcal{G}_\theta$ defined above, FlashMD implements a flexible deep learning pipeline. While the current implementation defaults to a specific Transformer backbone, the architecture is fundamentally modular.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

<p>We can view FlashMD as a wrapper that prepares atomic data for any powerful graph neural network:</p>

<ol>
  <li><strong>Input: Embedding the Atomic State</strong> The raw atomic state is converted into a graph representation.
    <ul>
      <li>The current positions $\mathbf{Q}$ and momenta $\mathbf{P}$ are encoded into node and edge features of a molecular graph.</li>
      <li><strong>Mass Scaling:</strong> Input momenta are normalized ($\tilde{\mathbf{p}}_i = \mathbf{p}_i / \sqrt{m_i}$) to prevent heavy atoms from dominating the loss, ensuring the model captures fast hydrogen vibrations as accurately as heavy-atom motions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Backbone: Point-Edge Transformer (PET)</strong> The graph is processed by a message-passing network to extract local geometric features. FlashMD uses the Point-Edge Transformer (PET) by default, which updates edge and node representations via attention mechanisms. At inference time, optional filters can be applied: momentum rescaling for energy conservation, thermostat/barostat integration for ensemble control, and random rotations to mitigate symmetry-breaking artifacts.</p>
  </li>
  <li>
    <p><strong>Output: Multi-Head Prediction</strong> Two separate MLP heads branch from the final node representations to predict the update:</p>

    <ul>
      <li>Momentum Head: Predicts $\mathbf{p}_i(t + \Delta t)$.</li>
      <li>Displacement Head: Predicts $\Delta \mathbf{q}_i(t + \Delta t)$.</li>
    </ul>
  </li>
</ol>

<p>Theoretically, you could swap the backbone for any modern GNN. However, molecular dynamics imposes a strict “non-negotiable” constraint that narrows our choices significantly: E(3) Equivariance.</p>

<h2 id="the-symmetry-challenge-e3-equivariance">The Symmetry Challenge: E(3) Equivariance</h2>
<p>Imagine simulating a water molecule. If you rotate your entire simulation box by 90 degrees, the physics must remain identical. The potential energy should not change, and the force vectors must rotate by exactly 90 degrees to match the atoms.</p>

<p>Standard neural networks see coordinates as simple lists of numbers; they do not inherently “know” that a rotated molecule is the same physical object. If we denote our model as $\mathcal{F}$ and a rotation matrix as $\mathcal{R}$, the model must satisfy:</p>

\[\mathcal{F}(\mathcal{R} \cdot \mathbf{Q}) = \mathcal{R} \cdot \mathcal{F}(\mathbf{Q})\]

<p>If a model fails this test, it might predict that a molecule flies apart simply because it was rotated to face “North” instead of “East.”</p>

<p>There are generally two ways to solve this in Deep Learning:</p>

<ol>
  <li>
    <p>Hard Constraints (e.g., NequIP<d-cite key="NequIP_Batzner2022"></d-cite>, MACE<d-cite key="MACE_ALLEGRO_leimeroth2025machine"></d-cite>): Bake geometric algebra (spherical harmonics) directly into the network layers. This guarantees exact equivariance but is computationally expensive.</p>
  </li>
  <li>
    <p>Soft Constraints (e.g., SchNet<d-cite key="schutt2017schnet"></d-cite>, PET<d-cite key="PET_pozdnyakov2023smooth"></d-cite>): Use a flexible, standard architecture and “teach” it symmetry through data augmentation or frame averaging.</p>
  </li>
</ol>

<h2 id="the-point-edge-transformer-pet">The Point-Edge Transformer (PET)</h2>

<ul>
  <li>is a rotationally unconstrained and transformer-based graph neural network</li>
  <li>PET maintains feature vectors (or messages) f_l ij for every directed bond between atoms i and j that lie within a specified cutoff radius.</li>
  <li>These intermediate representations are updated at each message-passing layer by a transformer</li>
  <li>outputs are subsequently interpreted as the new set of outbound messages from atom i to each neighbor j</li>
  <li>geometric information and chemical species are also incorporated</li>
  <li>A feed-forward NN is used to obtain the desired output/target property</li>
  <li>PET architecture imposes no explicit rotational symmetry constraints, but learns to be equivariant through data augmentation.</li>
  <li>This unconstrained approach yields high theoretical expressivity: even a single layer of the model acts as a universal approximator featuring virtually unlimited body order and angular resolution</li>
</ul>

<p>The PET architecture<d-cite key="PET_pozdnyakov2023smooth"></d-cite> reimagines atomic interactions through the lens of modern Transformers. While standard message-passing GNNs aggregate neighbor information via simple summation, PET introduces a richer mechanism that naturally captures <strong>many-body correlations</strong>—the complex ways in which multiple neighbors jointly influence a central atom.</p>

<p>(XXX add illustration of PET)</p>

<p>The key innovations are:</p>

<ol>
  <li>
    <p><strong>Tokenization of neighbors.</strong> For each central atom, every neighbor within a cutoff radius $R_c$ is encoded into a distinct <em>abstract token</em> that carries both geometric (relative position) and chemical (species) information. Unlike standard GNNs that collapse neighbor information into a single aggregated vector, PET preserves the identity of each interaction.</p>
  </li>
  <li>
    <p><strong>Self-attention over interactions.</strong> These tokens are processed by a Transformer-style self-attention mechanism. This allows the model to learn that the presence of one neighbor dynamically modifies the effective interaction with another—for example, how a third oxygen atom weakens a hydrogen bond between two water molecules. These <strong>many-body effects</strong> emerge naturally from attention, without requiring hand-crafted descriptors.</p>
  </li>
  <li>
    <p><strong>Computational efficiency.</strong> By avoiding the expensive mathematical machinery of spherical harmonics and Clebsch–Gordan coefficients required by strictly equivariant architectures, PET achieves competitive accuracy at significantly lower computational cost. The price is that rotational symmetry must be learned rather than guaranteed.</p>
  </li>
</ol>

<h3 id="enforcing-symmetry-at-runtime">Enforcing Symmetry at Runtime</h3>

<p>XXX check section and shorten it
Since PET is not intrinsically equivariant, FlashMD must enforce symmetry through two complementary mechanisms.</p>

<p><strong>During training</strong>, random rotations are applied to every training sample (<strong>data augmentation</strong>). This teaches the model to produce consistent predictions regardless of molecular orientation—but the resulting equivariance is only approximate, limited by finite training data and model capacity.</p>

<p><strong>During inference</strong>, FlashMD adds a second layer of protection: <strong>Stochastic Frame Averaging</strong>. Before each prediction step:</p>

<ol>
  <li>The entire system $(\mathbf{Q}, \mathbf{P})$ is rotated by a random matrix $\mathcal{R}$.</li>
  <li>The backbone computes the next state in the rotated frame.</li>
  <li>The output is mapped back to the original frame via $\mathcal{R}^{-1}$.</li>
</ol>

\[\mathbf{y}_{\text{final}} = \mathcal{R}^{-1} \cdot \mathcal{F}_{\theta}(\mathcal{R} \cdot \mathbf{x})\]

<p>Over many rollout steps, this ensures that predictions are <strong>statistically invariant</strong> to orientation—any systematic bias toward a particular direction averages out. It is a pragmatic compromise: cheaper than the full Equivariant Coordinate System Ensemble (ECSE) proposed in the original PET paper, but sufficient for the long rollouts FlashMD targets.</p>

<p>The combination of data augmentation and stochastic frame averaging makes PET <em>practically</em> equivariant—not by mathematical proof, but by empirical convergence. Whether this approximation is good enough depends on the application. For the thermostatted benchmarks in the next section, it works remarkably well. For the stricter NVE setting in our <a href="#an-exploratory-study-on-failure-modes">exploratory study</a>, even small symmetry violations may compound.</p>

<h2 id="long-stride-predictions-in-practice">Long-Stride Predictions in Practice</h2>
<p>To demonstrate what FlashMD is capable of, the authors evaluate it across a diverse set of benchmarks and experiments, designed to answer a simple question:
Can we simulate realistic molecular dynamics with much larger time steps – without losing essential physics?</p>

<p>FlashMD is explored in two flavors:</p>

<ul>
  <li>A water-specific model, trained only on liquid water</li>
  <li>A universal model, trained on chemically diverse systems, meant to generalize across molecules and materials</li>
</ul>

<p>This lets the authors study both ends of the spectrum: maximum accuracy for one system, and broad applicability across many.</p>

<h3 id="the-testing-strategy">The Testing Strategy</h3>
<p>Before diving into results, it’s worth understanding how you even benchmark a method like FlashMD. A direct, step-by-step comparison of trajectories is impossible: MD is chaotic – tiny differences grow exponentially, so two simulations will quickly diverge even if both are “correct”.</p>

<p>Instead, the researchers took a statistical approach:</p>

<ol>
  <li>Generate reference trajectories using conventional MD with a reliable force field (PET-MAD)</li>
  <li>Run FlashMD simulations under the same conditions</li>
  <li>Compare statistical properties rather than individual trajectories—things like density, radial distribution functions (how atoms arrange themselves around each other), and phase transition temperatures</li>
</ol>

<p>As the authors note: “we primarily focus our quantitative analysis on time-independent equilibrium properties, and discuss examples where FlashMD qualitatively captures time-dependent behavior.” In other words: check if the average properties match, and see if the dynamics look qualitatively reasonable.</p>

<h3 id="key-results-at-a-glance">Key Results at a Glance</h3>

<p><strong>1. Liquid Water:</strong>
Water might seem simple, but it’s notoriously tricky to simulate due to its hydrogen bonding network. The team tested both their water-specific and universal models on liquid water at 450 K (above the melting point for their particular model).</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Temperature control worked well when using appropriate thermostats (Langevin), with deviations typically under 1 K from target temperature</li>
  <li>Radial distribution functions (which show how oxygen and hydrogen atoms arrange themselves) matched reference MD simulations nearly perfectly</li>
  <li>Density predictions from constant-pressure simulations were accurate for water-specific models and reasonable for universal models</li>
  <li>Models could handle strides up to 16 fs—a 64× speedup compared to the 0.25 fs timesteps typically needed</li>
</ul>

<p><strong>2. Solvated Alanine Dipeptide: Protein-like Dynamics</strong>
This system—a small peptide in water—serves as a minimal model for protein flexibility. The critical test: can FlashMD capture the Ramachandran plot, which maps out the backbone conformations proteins can adopt?</p>

<p>Remarkably, this works even with strides up to 32× larger than standard MD time steps – a strong indication that FlashMD preserves meaningful molecular motion, not just static snapshots.</p>

<p><strong>3. Aluminum Surface: Catching Pre-melting Phenomena</strong>
Metal surfaces exhibit fascinating behavior at high temperatures: atoms start becoming mobile before bulk melting occurs, a phenomenon called pre-melting. This requires capturing subtle, layer-specific dynamics.</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Correctly reproduced the anisotropic softening pattern—surface atoms wiggling more in one direction, second-layer atoms in another</li>
  <li>Captured dynamic defect formation: temporary creation and migration of surface atoms</li>
  <li>Achieved this with 64 fs strides (64× faster than the 1 fs baseline), while still showing physically meaningful atomic trajectories</li>
</ul>

<p>This shows that FlashMD is not limited to molecules, but can handle complex solid-state phenomena.</p>

<p><strong>4. Lithium Thiophosphate: Superionic Phase Transitions</strong>
Perhaps the most impressive demonstration involved a solid-state battery electrolyte material. At high temperatures, lithium atoms become highly mobile in a “superionic” state—critical for battery performance.</p>

<p>The challenge: Predict temperature-dependent lithium conductivity and capture the phase transition.</p>

<p><strong>Key findings:</strong></p>
<ul>
  <li>Successfully predicted the superionic transition temperature at 675 K, within the expected range</li>
  <li>Reproduced the dramatic increase in lithium ion conductivity across the transition</li>
  <li>Some systematic errors appeared (over/underestimation at low/high temperatures), but the overall behavior was captured with 8× speedup</li>
</ul>

<p>Here, FlashMD demonstrates that it can capture slow, collective processes, which are traditionally hard to access with MD.</p>

<hr />

<h1 id="limitations-and-open-challenges">Limitations and Open Challenges</h1>
<p>FlashMD promises incredible speed ($100\times$), but we have to ask: is the physics still real? Neural networks are pattern matchers, not physics engines. They don’t actually understand laws like energy conservation; they only memorize the data they’ve seen. When we push the model beyond its training distribution, cracks begin to appear.</p>

<h2 id="learning-dynamics-without-physical-guarantees">Learning Dynamics Without Physical Guarantees</h2>

<p>Velocity Verlet comes with mathematical guarantees: energy conservation, symplecticity, time-reversibility. A learned model has no such guarantees. Let’s examine what can go wrong.</p>

<h3 id="1-out-of-distribution-drift">1. Out-of-Distribution Drift</h3>

<p>FlashMD is trained on equilibrium MD trajectories. But during a long rollout, small errors compound. After 1,000 steps (160 ps with 16 fs strides), the system may drift into configurations never seen during training.</p>

<p>Unlike MLIPs—which predict one step ahead—FlashMD’s errors accumulate <strong>autoregressively</strong>. This demands robust uncertainty quantification: the model must know when it doesn’t know.</p>

<h3 id="2-chaotic-dynamics">2. Chaotic Dynamics</h3>

<p>Molecular systems are chaotic: nearby trajectories diverge exponentially (Lyapunov exponent). This imposes a fundamental limit—even a perfect model cannot predict beyond the system’s decorrelation time (typically picoseconds for liquids, nanoseconds for proteins).</p>

<p>This isn’t a bug; it’s physics. FlashMD must capture the <strong>statistical ensemble</strong> of trajectories, not a single deterministic path. This introduces <strong>aleatoric uncertainty</strong>—irreducible randomness from the chaotic dynamics itself.</p>

<h3 id="3-energy-conservation">3. Energy Conservation</h3>

<p>Here’s the Achilles’ heel. Velocity Verlet conserves energy to machine precision. A neural network has no such constraint—small prediction errors cause energy drift:</p>

\[\Delta H = H(\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t}) - H(\mathbf{Q}_t, \mathbf{P}_t)\]

<p>FlashMD addresses this two ways:</p>

<ol>
  <li><strong>Training:</strong> Include $|\Delta H|$ in the loss function, encouraging implicit energy conservation</li>
  <li><strong>Inference:</strong> Rescale momenta post-prediction to enforce $H_{t+\Delta t} = H_t$ exactly</li>
</ol>

<p>The second approach is aggressive but necessary. However, it modifies the dynamics—we’re no longer solving Hamilton’s equations, but a constrained variant. Does this preserve the correct statistical ensemble? (We’ll return to this question.)</p>

<h3 id="4-symplectic-structure">4. Symplectic Structure</h3>

<p>In Hamiltonian mechanics, phase space has geometric structure (symplecticity) that preserves volume. Classical integrators respect this; neural networks don’t.</p>

<p>Enforcing symplecticity explicitly—via a generating function parameterization—is theoretically possible but computationally prohibitive (it requires computing a 3N × 3N Jacobian). FlashMD takes a pragmatic approach: train on symplectic data (VV trajectories) and hope the model learns the structure implicitly.</p>

<p>This is a gamble. Without explicit enforcement, thermodynamic properties (temperature, pressure, free energies) may drift over long timescales.</p>

<h3 id="5-rotational-symmetry">5. Rotational Symmetry</h3>

<p>Physics is rotationally invariant: if you spin your simulation box, the dynamics shouldn’t change. A standard neural network doesn’t “know” this—it must learn it from data.</p>

<p>FlashMD shows it can maintain physical accuracy while taking dramatically larger steps through time, across a diverse range of materials. But as we’ll see next, this performance comes with important caveats—particularly around energy conservation…</p>

<p>FlashMD mitigates this via:</p>
<ul>
  <li><strong>Data augmentation:</strong> Random rotations during training</li>
  <li><strong>Runtime augmentation:</strong> Random rotations at each prediction step</li>
</ul>

<h1 id="an-exploratory-study-on-failure-modes">An Exploratory Study on Failure Modes</h1>

<p>To move beyond theoretical concerns, we conduct a systematic exploratory study on a concrete system: a periodic box of 258 TIP3P water molecules simulated with OpenMM as ground truth. We train FlashMD models under varying conditions and evaluate their ability to conserve energy during NVE rollouts—the most unforgiving test of physical validity.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/tip3p_maxwell_boltzmann_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 3: Comparison of Maxwell-Boltzmann distributions for different temperatures</figcaption>
  
</figure>

<h2 id="experimental-setup">Experimental Setup</h2>

<p><strong>Ground Truth Generation.</strong> We generate NVE trajectories at different temperatures (200K–700K, 20K steps) using the TIP3P force field in OpenMM, saving configurations every 0.5 fs for 10 ps each. We verify thermodynamic consistency by comparing the velocity distributions against the Maxwell-Boltzmann distribution at each temperature (<a href="#fig:maxwell">Figure 3</a>).</p>

<p><strong>Training Data.</strong> From these trajectories, we construct FlashMD training pairs $(q_t, p_t) \to (q_{t+\Delta t}, p_{t+\Delta t})$ at a prediction stride of $\Delta t = 1\,\text{fs}$. Following the original paper, targets are stored as mass-scaled quantities: $\Delta\tilde{q}_i = \Delta q_i \sqrt{m_i}$ and $\tilde{p}_i = p_i / \sqrt{m_i}$. XXX check again this in code</p>

<p><strong>Evaluation Protocol.</strong> Each trained model is deployed in NVE simulation for 50 ps (50,000 steps at 1 fs) starting from a 300 K equilibrated configuration. We track total energy $E_\text{tot}(t)$, temperature $T(t)$, and energy drift rate $\dot{E}$ (eV/ps). A model is considered “exploded” if $T &gt; 1000\,\text{K}$ at any point.</p>

<p>XXX before ablation show the trained models are stable but lack the energy conservation leading to huge energy and temperature drifts.</p>

<p>The first results of the simulations depict in Figure XX. We trained the models accordingly to the setup above. We also investigated different starting points from the NVE groundtruth to see the bahaviour od the model as well as different seeds to handle stochastic training varaicen. All custom models show that all of the trained models achieve stable NVE simulations. But all of them lack the energy conservation drastically. We see huge drifts in energy and temperature. This indicates us that the model loses some energy somehow. To understand this behaviour we also want to take a look at the momenta distribution of the models compared to the ground truth. Since the momenta relates directly to the potential energy this is traight forward.</p>

<p>XXX Image of plots fro the distributions here.</p>

<p>As we can see. All of custom models exhibit extensive differences in the momenta of Oxygen atoms. But interestingly we can see that if turn on the rescaling filter we can even better distributions. This leads us to question how could we improve the momentum distrubtion of the model by doing two things:</p>
<ul>
  <li>Loss weighting between positons and momenta</li>
  <li>Applying mass sacling to the positons and momenta</li>
</ul>

<h2 id="ablation-1-loss-weighting-between-positions-and-momenta">Ablation 1: Loss Weighting Between Positions and Momenta</h2>

<p>The first question is simple: does it matter how much the model cares about getting momenta right vs. positions? We train models with momentum loss weights $w_p \in {0.5, 1.0, 1.5, 2.0, 10.0}$ while keeping the position weight fixed at $w_q = 1.0$, and let them run NVE for 10 ps.</p>

<p>The results are… surprising.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>$w_p$</th>
      <th>Drift (eV/ps)</th>
      <th>$\bar{T}$ (K)</th>
      <th>$\sigma_T$ (K)</th>
      <th>Stable until</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$w_p=0.5$</td>
      <td>0.5</td>
      <td>2813.95</td>
      <td>513.8</td>
      <td>148.9</td>
      <td>8.1 ps ✗</td>
    </tr>
    <tr>
      <td>$w_p=1.0$ (Baseline)</td>
      <td>1.0</td>
      <td>-3.70</td>
      <td>227.1</td>
      <td>34.2</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>$w_p=1.5$</td>
      <td>1.5</td>
      <td>-4.80</td>
      <td>178.0</td>
      <td>38.8</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>$w_p=2.0$</td>
      <td>2.0</td>
      <td>52.80</td>
      <td>476.1</td>
      <td>168.5</td>
      <td>2.7 ps ✗</td>
    </tr>
    <tr>
      <td>$w_p=10.0$</td>
      <td>10.0</td>
      <td>4.39</td>
      <td>334.9</td>
      <td>67.5</td>
      <td>&gt;10 ps ✓</td>
    </tr>
    <tr>
      <td>Ground Truth</td>
      <td>—</td>
      <td>-0.0035</td>
      <td>304.7</td>
      <td>6.9</td>
      <td>&gt;10 ps ✓</td>
    </tr>
  </tbody>
</table>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectories for three momentum loss weights and OpenMM ground truth. Top: temperature evolution. Bottom: total energy. The ground truth (black) is essentially flat — none of the models come close.</figcaption>
  
</figure>

<p>If you expected “more momentum weight = better momenta = more stable,” you’d be wrong. The relationship is wildly non-monotonic, and we can group the results into three regimes:</p>

<p><strong>Too little ($w_p = 0.5$): immediate explosion.</strong> The model doesn’t learn momentum dynamics well enough and blows up within 8 ps. Temperature rockets past 500 K, energy drift is off the charts. Lesson: you <em>need</em> to care about momenta.</p>

<p><strong>The “moderate” zone ($w_p = 1.0, 1.5$): stable but freezing.</strong> These models survive the full 10 ps — but look at the temperatures! The baseline cools from 300 K to a mean of 227 K, and $w_p = 1.5$ cools even further to 178 K. The model is systematically predicting momenta that are too small. It’s not exploding, but it’s not doing physics either — it’s slowly bleeding kinetic energy. You can see this clearly in <a href="#fig:ablation1">Figure X</a>: the blue and green lines drift steadily downward while the ground truth holds at ~305 K.</p>

<p><strong>Cranked to 10 ($w_p = 10.0$): closest to reality, but noisy.</strong> Here’s the twist: the most extreme weight actually produces the most realistic temperature (335 K, just 10% above target). Looking at <a href="#fig:ablation1">Figure X</a>, the red $w_p=10$ line tracks the ground truth best during the first ~5 ps. But the fluctuations are huge ($\sigma_T = 67$ K vs. ground truth’s 7 K), and by 8 ps it starts slowly heating up.</p>

<p>To our suprise, $w_p = 2.0$ explodes at 2.7 ps. Somehow, 2× the baseline is catastrophically unstable while 10× is fine. The loss landscape clearly has some sharp cliffs in this region.</p>

<p><strong>The bottom line:</strong> Look at the energy panel in <a href="#fig:ablation1">Figure X</a>. The ground truth (black line) is essentially a flat line. Every single model drifts visibly — the best ones still have energy drift <strong>1000× larger</strong> than OpenMM. Loss weighting can shift the failure mode from cooling to heating, but it can’t bridge that gap.</p>

<p>This tells us something important: the problem isn’t <em>how much</em> the model cares about momenta — it’s <em>how</em> it represents them. Which brings us to our next experiment: what if we change the loss to account for the fact that hydrogen and oxygen have very different masses?</p>

<p>much* the model cares about momenta, we change <em>how</em> it represents them through mass-scaled loss formulations. –&gt;</p>

<h2 id="ablation-2-mass-scaled-loss-functions">Ablation 2: Mass-Scaled Loss Functions</h2>
<p>Standard MSE treats all atoms equally in momentum space. But since $p_i = m_i v_i$, oxygen atoms (mass 16) dominate the momentum loss by a factor of $16^2 = 256$ compared to hydrogen (mass 1). This means the model optimizes primarily for oxygen momenta while hydrogen velocity errors remain large.</p>

<p>Following the FlashMD paper, we implement a mass-scaled loss:</p>

\[\mathcal{L}_{\tilde{p}} = \frac{1}{N} \sum_i \frac{\|p_i^\text{pred} - p_i^\text{true}\|^2}{m_i}, \qquad \mathcal{L}_{\Delta\tilde{q}} = \frac{1}{N} \sum_i \|\Delta q_i^\text{pred} - \Delta q_i^\text{true}\|^2 \cdot m_i\]

<p>This is equivalent to computing MSE on the mass-scaled quantities $\tilde{p} = p/\sqrt{m}$ and $\Delta\tilde{q} = \Delta q \cdot \sqrt{m}$, ensuring that velocity errors are weighted equally regardless of atomic mass.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectory comparison between standard MSE and mass-scaled MSE loss. Left column shows the initial drift phase (0–10 ps), right column the full 50 ps trajectory. Both models exhibit systematic cooling relative to the ground truth.</figcaption>
  
</figure>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Loss Type</th>
      <th>Drift (eV/ps)</th>
      <th>$\bar{T}$ (K)</th>
      <th>$\sigma_T$ (K)</th>
      <th>$\sigma_E$ (kJ/mol)</th>
      <th>Stable until</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Custom Trained</td>
      <td>MSE</td>
      <td>-0.6842</td>
      <td>207.1</td>
      <td>23.0</td>
      <td>591.1</td>
      <td>&gt;50 ps</td>
    </tr>
    <tr>
      <td>Mass-Scaled Custom Trained</td>
      <td>mass_scaled_mse</td>
      <td>-0.6989</td>
      <td>212.9</td>
      <td>24.9</td>
      <td>642.0</td>
      <td>&gt;50 ps</td>
    </tr>
    <tr>
      <td>Ground Truth (OpenMM)</td>
      <td>—</td>
      <td>-0.0003</td>
      <td>304.2</td>
      <td>6.9</td>
      <td>1.2</td>
      <td>&gt;50 ps</td>
    </tr>
  </tbody>
</table>

<p><strong>Key finding:</strong> Mass-scaling provides a modest improvement in the early drift phase but does not fundamentally alter long-term stability. A time-windowed analysis reveals the nuance:</p>

<ul>
  <li><strong>0–5 ps:</strong> Mass-scaling reduces energy drift by 26% (-3.72 vs. -5.03 eV/ps) and keeps the temperature closer to 300 K (272 K vs. 254 K, $\sigma_T$ halved from 26 to 13 K). The model clearly benefits from balanced treatment of H and O atoms in the initial phase.</li>
  <li><strong>0–10 ps:</strong> The advantage persists but narrows—drift is reduced by 24% (-2.82 vs. -3.70 eV/ps), mean temperature 252 K vs. 227 K.</li>
  <li><strong>0–50 ps:</strong> Both models converge to nearly identical behavior—drift rates of -0.68 and -0.70 eV/ps, mean temperatures of 207 K and 213 K. The early advantage is fully washed out.</li>
</ul>

<p><strong>Physical interpretation:</strong> Mass-scaling correctly addresses the <em>symptom</em>—unequal error weighting across species—but not the <em>disease</em>. Both models systematically cool from 300 K to ~210 K over 50 ps, losing roughly $\frac{1}{3}$ of their kinetic energy. This cooling pattern ($\dot{E} \approx -0.7$ eV/ps, $\sigma_E \sim 600$ kJ/mol vs. ground truth $\sigma_E = 1.2$ kJ/mol) represents a $500\times$ violation of energy conservation that no loss reweighting can fix.</p>

<p>The fact that mass-scaling helps <em>early</em> but not <em>late</em> suggests that it improves the model’s initial momentum predictions (reducing the per-species bias), but the accumulated errors from the autoregressive rollout eventually dominate regardless. The fundamental issue is that FlashMD’s single-step prediction errors, while small individually, compound systematically rather than canceling stochastically—a hallmark of non-symplectic integration.</p>

<h2 id="summary-of-findings">Summary of Findings</h2>

<p>Our exploratory study reveals a consistent pattern across all model variants:</p>

<ol>
  <li>
    <p><strong>All models exhibit systematic energy drift</strong>, even in the quasi-stable 
regime. This is expected—the model has no symplectic structure or 
energy-conserving inductive bias.</p>
  </li>
  <li>
    <p><strong>Loss weighting affects the drift rate</strong> but does not eliminate it. 
Mass-scaled losses with appropriate momentum weighting achieve the 
best short-term stability.</p>
  </li>
  <li>
    <p><strong>Failure is always catastrophic once it begins.</strong> There is no graceful 
degradation—once the system leaves the training distribution, errors 
compound exponentially.</p>
  </li>
</ol>

<p>These findings suggest that improving the loss function alone is insufficient. The fundamental challenge is distributional: the model must either (a) be trained on data hat covers the states it will visit during long rollouts, or (b) incorporate physical constraints that prevent drift toward unphysical regions.</p>

<hr />

<h1 id="conclusion">Conclusion</h1>

<p>In this post, we traced the two fundamental bottlenecks of molecular dynamics and showed how each has been addressed: MLIPs solved the force bottleneck; FlashMD bypasses the integrator entirely, predicting system evolution at strides one to two orders of magnitude beyond classical stability limits. Across water, proteins, metals, and battery electrolytes, it recovers the correct statistical physics at speedups of 8× to 64×.</p>

<p>But our independent exploratory study reveals a sobering counterpoint. When we strip away the thermostats that regulate temperature in standard benchmarks, the learned dynamics fail to conserve energy. Every model we tested—regardless of loss weighting or mass scaling—exhibits systematic energy drift orders of magnitude larger than classical integrators. The problem is structural, not parametric: no amount of loss function tuning resolved it.</p>

<p>This does not diminish FlashMD’s contribution—it is the first model to learn meaningful long-stride dynamics across chemically diverse systems. But it makes clear what is still missing: the mathematical guarantees that classical integrators provide for free. Until symplectic architectures, hybrid corrective schemes, or active learning strategies close this gap, FlashMD is best understood as a powerful tool for thermostatted simulations rather than a general-purpose replacement for classical integrators.</p>

<p>The question we posed in the introduction—<em>Can a model learn to respect the laws of physics without being explicitly taught to do so?</em>—now has a nuanced answer. For statistical properties sampled under external control, yes. For the strict, unassisted conservation of energy that defines Hamiltonian dynamics, not yet.</p>

<h2 id="xxx-propose-a-way-scetch-a-solution-maybe-add-first-experiments-with-handselected-active-learning-">XXX Propose a way, scetch a solution maybe add first experiments with handselected active learning …</h2>

<h1 id="conclusion-1">Conclusion</h1>
<p><em>(Your text here…)</em></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In 2025, a research group of the COSMO Lab published a new framework for long-stride, universal prediction of molecular dynamics, which they call FLashMD. This new approach addresses one of the biggest challenges in computational science: the trade-off between accuracy and speed in simulating atomic-scale systems. By introducing a novel neural network architecture, FLashMD learns to predict the complex, quantum-mechanical forces governing molecular behavior, enabling simulations that are both accurate and computationally efficient. This post explores the core concepts behind FLashMD, breaks down its innovative architecture, and examines its potential to revolutionize fields from drug discovery to materials science.]]></summary></entry><entry><title type="html">Breaking the Femtosecond Prison: Promise and Limits of FlashMD</title><link href="http://0.0.0.0:8080/2026/blog/2026/breaking-the-femtosecond-prison-promise-and-limits-of-flashmd/" rel="alternate" type="text/html" title="Breaking the Femtosecond Prison: Promise and Limits of FlashMD" /><published>2026-02-15T00:00:00+00:00</published><updated>2026-02-15T00:00:00+00:00</updated><id>http://0.0.0.0:8080/2026/blog/2026/breaking-the-femtosecond-prison-promise-and-limits-of-flashmd</id><content type="html" xml:base="http://0.0.0.0:8080/2026/blog/2026/breaking-the-femtosecond-prison-promise-and-limits-of-flashmd/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>As Richard Feynman once said, everything that living things do can be understood in terms of the jigglings and wigglings of atoms<d-cite key="feynman1964relation"></d-cite> . The computational microscope used to examine this behavior is called Molecular Dynamics (MD). MD enables us to simulate atoms according to Newton’s laws and observe processes such as protein folding, battery charging, and material fracture—all at the atomic scale.</p>

<p>However, the MD workflow has long suffered from two significant bottlenecks:</p>

<ol>
  <li><strong>Calculating the forces</strong> between atoms at each timestep</li>
  <li><strong>The timestep stability limit</strong> of numerical integrators</li>
</ol>

<p>With the advent of Machine Learning Interatomic Potentials (MLIPs), the first bottleneck has been largely addressed—replacing costly quantum mechanical calculations with learned approximations. But the second bottleneck remains: we can now compute forces cheaply and accurately, yet we are still forced to take tiny time steps ($\sim10^{-15}$ s) to maintain numerical stability<d-cite key="leach2001timestep"></d-cite>. Observing biological processes that unfold over milliseconds requires <strong>trillions of sequential steps</strong>—each dependent on the previous one. No amount of faster force evaluation can fix this.</p>

<p>FlashMD<d-cite key="bigi2025flashmd"></d-cite> tackles this second bottleneck head-on. Rather than following the traditional MD loop—calculating forces and integrating numerically with small time steps—FlashMD predicts the system’s evolution directly over significantly larger time intervals, often extending one to two orders of magnitude beyond the limits of classical integrators. Across water, proteins, metals, and battery materials, it recovers the correct statistical physics at speedups of 8× to 64×.</p>

<p>However, standard deep learning models are fundamentally agnostic to physical laws. Classical integrators come with mathematical guarantees: they approximately conserve energy, preserve time-reversibility, and maintain long-term stability. A learned model offers none of these.</p>

<p>This raises a fundamental question: <strong>Can a model learn to respect the laws of physics without being explicitly taught to do so?</strong> That is precisely what FlashMD attempts—learning the physics and dynamical behavior of atomic systems entirely from data.</p>

<p>In this post, we build up from first principles—starting with the physics of molecular dynamics, through the MLIP revolution, to FlashMD’s architecture and its impressive empirical results. We then go beyond the original paper: in an independent exploratory study, we stress-test FlashMD in the microcanonical (NVE) ensemble, where energy must be conserved solely by the dynamics—no thermostats, no safety nets. The results reveal that all tested models exhibit systematic energy drift and eventually diverge catastrophically—exposing the gap between statistical accuracy and physical validity.</p>

<p>The question is not simply whether FlashMD is fast. It is whether learned dynamics can be trusted—and our results suggest they cannot yet.</p>

<hr />

<h1 id="molecular-dynamics-fundamentals">Molecular Dynamics Fundamentals</h1>
<p>Molecular Dynamics (MD) is the computational engine behind our understanding of matter in motion. At its core, MD follows a straightforward recipe: place $N$ atoms in a box, calculate the forces between them, and step forward in time using Newton’s equations.</p>

<p>The challenge lies in the scale. As shown in the workflow below, every MD simulation is built around a <strong>core loop</strong> that must be executed millions to billions of times:</p>

<ol>
  <li><strong>Calculate forces</strong> on every atom (Step 2)</li>
  <li><strong>Integrate equations of motion</strong> to update positions and velocities (Step 3)</li>
  <li>Repeat for $10^6$ to $10^9$ steps</li>
</ol>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/MD_simulation_workflow-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/MD_simulation_workflow-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/MD_simulation_workflow-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/MD_simulation_workflow.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 1: Overview of a Standard MD Simulation Pipeline. The workflow involves system setup, an iterative loop of force calculation and position integration, and a final analysis phase to compute thermodynamic properties.</figcaption>
  
</figure>

<p>This repetition is fundamental, not a limitation. To extract meaningful thermodynamic properties—diffusion coefficients, phase transitions, reaction rates—we need trajectories long enough to sample the system’s accessible states. A single nanosecond of physical time typically requires around a million integration steps.</p>

<p>This raises a natural question: <strong>which step consumes the most time?</strong></p>

<h2 id="the-hamiltonian-view-of-atomic-motion">The Hamiltonian View of Atomic Motion</h2>
<p>To understand where the computational bottleneck lies, we need to look inside the simulation loop. At each step, the system evolves according to Hamilton’s equations of motion—the fundamental laws governing how atoms move.</p>

<p>The core idea is simple: <strong>forces come from energy gradients</strong>. We describe the system’s total energy using the Hamiltonian $H$, which splits into two parts:</p>

\[H(\mathbf{P}, \mathbf{Q}) = \underbrace{\sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i}}_{\text{Kinetic Energy } K(\mathbf{P})} + \underbrace{V(\mathbf{Q})}_{\text{Potential Energy}}\]

<p>where \(\mathbf{Q} = \{\mathbf{q}_i\}_{i=1}^N\) are atomic positions, \(\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^N\) are momenta, and \(V(\mathbf{Q})\) is the potential energy surface (PES).</p>

<p>Hamilton’s equations tell us how the system evolves:</p>

\[\frac{d\mathbf{q}_i}{dt} = \frac{\mathbf{p}_i}{m_i} \quad , \quad \frac{d\mathbf{p}_i}{dt} = -\frac{\partial V}{\partial \mathbf{q}_i}\]

<p>The first equation is the easy part: the velocity is simply momentum divided by mass, which makes the kinetic energy computationally cheap. The second equation is crucial: <strong>the force on each atom is the negative gradient of the potential energy with respect to its position</strong>.</p>

<p>In principle, if we could solve these coupled differential equations exactly, we would obtain the full trajectory of the system. In practice, however, the Hamiltonian of realistic molecular systems is far too complex to admit an analytical solution. As a result, molecular dynamics always relies on approximations.</p>

<p>To simulate the system, we need two things:</p>

<ol>
  <li>A way to compute $V(\mathbf{Q})$ and its gradient $\nabla V$.</li>
  <li>A way to discretize continuous time into finite steps $\Delta t$.</li>
</ol>

<p><strong>For the second requirement, we use numerical integration.</strong></p>

<h3 id="the-velocity-verlet-algorithm">The Velocity Verlet Algorithm</h3>

<p>There are many numerical integration schemes out there, but Velocity Verlet is one of the most commonly used in molecular dynamics. Thanks to its simplicity and efficiency, we’ll use it here as our example.</p>

<p>To solve these continuous equations on a computer, we discretize time using the Velocity Verlet integrator:</p>

\[\begin{aligned}
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t \\
    \mathbf{q}_i &amp;\leftarrow \mathbf{q}_i + \frac{\mathbf{p}_i}{m_i} \Delta t \\
    \mathbf{p}_i &amp;\leftarrow \mathbf{p}_i - \frac{1}{2} \nabla_{\mathbf{q}_i} V \cdot \Delta t
\end{aligned}\]

<p>This algorithm is <strong>symplectic</strong>, meaning it preserves the geometric structure of phase space. This is crucial: symplectic integrators approximately conserve the Hamiltonian (total energy) even with finite $\Delta t$, preventing the systematic energy drift that would cause non-symplectic methods to fail over long simulations<d-cite key="hairer2006geometric"></d-cite>. But there’s a catch: stability requires $\Delta t \sim 0.5\text{–}1$ femtoseconds<d-cite key="leach2001timestep"></d-cite>. Larger steps cause energy drift and numerical explosions. <strong>This is the femtosecond prison</strong>—the fundamental timestep barrier that limits all classical MD.</p>

<p>Every loop iteration requires:</p>
<ul>
  <li><strong>Computing forces</strong> via $\nabla V$ (expensive)</li>
  <li><strong>Taking a tiny timestep</strong> (limiting)</li>
</ul>

<p>We’ll address these bottlenecks in sequence, starting with force computation.</p>

<h2 id="why-ab-initio-molecular-dynamics-is-expensive">Why <em>ab Initio</em> Molecular Dynamics Is Expensive</h2>

<p>The first bottleneck is evaluating $V(\mathbf{Q})$ and its gradient. Historically, this forced a painful compromise:</p>

<ul>
  <li>
    <p><strong><em>Ab initio</em> methods</strong> (e.g., Density Functional Theory) solve quantum mechanics to compute forces with chemical accuracy. But they require solving the electronic structure problem at every step, with computational cost scaling as $O(N^3)$ or worse—and large constant factors that make even small systems expensive<d-cite key="zhang2018deep"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Classical force fields</strong> use handcrafted functions (harmonic springs, Lennard-Jones potentials, Coulomb terms) that scale linearly with system size. But these predefined functional forms—fixed at design time—cannot adapt to bond breaking, chemical reactions, or complex polarization effects that require quantum mechanical treatment (XXX Source).</p>
  </li>
</ul>

<p>For decades, this accuracy-efficiency trade-off defined the field. Quantum accuracy meant tiny systems; large-scale simulations meant sacrificing chemistry.</p>

<p><strong>Machine Learning Interatomic Potentials (MLIPs)</strong> changed this.</p>

<hr />
<h1 id="machine-learned-interatomic-potentials-solving-the-force-bottleneck">Machine-Learned Interatomic Potentials: Solving the Force Bottleneck</h1>
<p>Recall that every timestep requires evaluating $\mathbf{F}_i = -\nabla_i V(\mathbf{Q})$—the gradient of the potential energy surface. Historically, this forced a compromise: precise but computationally prohibitive Quantum Mechanics (scaling $\mathcal{O}(N^3)$), or fast but inaccurate classical force fields.</p>

<p>MLIPs resolve this by learning a surrogate map from atomic configurations to potential energy, reducing the complexity to $\mathcal{O}(N)$. (XXX Source)</p>

<h2 id="the-mlip-breakthrough">The MLIP Breakthrough</h2>

<p>Instead of regressing forces directly, MLIPs parameterize the scalar potential energy \(V_\theta(\mathbf{Q})\). Forces are obtained via automatic differentiation with respect to atomic positions:</p>

\[\mathbf{F}_{\text{pred}} = -\frac{\partial V_\theta(\mathbf{Q})}{\partial \mathbf{Q}}\]

<p>This approach is critical for two reasons:</p>
<ol>
  <li><strong>Physical Validity:</strong> It guarantees a conservative force field ($\nabla \times \mathbf{F} = 0$), ensuring energy conservation.</li>
  <li><strong>Data Efficiency:</strong> Training on force labels provides $3N$ constraints per data point (vectors) compared to a single scalar energy constraint, significantly stabilizing the gradient descent.</li>
</ol>

<h3 id="architecture-graph-neural-networks">Architecture: Graph Neural Networks</h3>

<p>Molecular systems are not grids; they are continuous 3D graphs governed by physical symmetries. Modern MLIPs (e.g., SchNet<d-cite key="schutt2017schnet"></d-cite>, NequIP<d-cite key="NequIP_Batzner2022"></d-cite>, MACE<d-cite key="MACE_ALLEGRO_leimeroth2025machine"></d-cite>) leverage Geometric Deep Learning to encode these priors:</p>

<ul>
  <li><strong>Locality:</strong> Message passing operations approximate quantum interactions, which decay with distance.</li>
  <li><strong>Symmetry:</strong> Architectures are designed to be invariant to rotation and translation ($E(3)$ symmetry). Rotating the molecule does not change the predicted energy $V_\theta$.The resulting objective function minimizes errors in both energy and forces:</li>
</ul>

\[\mathcal{L}(\theta) = \lambda_E \|V_\theta - V_{\text{DFT}}\|^2 + \lambda_F \|\mathbf{F}_{\text{pred}} - \mathbf{F}_{\text{DFT}}\|^2\]

<p>Training on forces directly improves generalization: force supervision provides richer gradient information and helps the model handle out-of-distribution configurations<d-cite key="chmiela2018towards"></d-cite>.</p>

<h2 id="the-remaining-challenge-the-femtosecond-prison">The Remaining Challenge: The Femtosecond Prison</h2>

<p>MLIPs successfully decoupled accuracy from computational cost. They allow us to run simulations with ab initio accuracy at millisecond inference speeds.</p>

<p>While MLIPs accelerate the function evaluation, they are still bound to the Velocity Verlet integrator. To maintain numerical stability, we are still forced to take femtosecond steps ($\Delta t \approx 10^{-15}s$).</p>

<p>Simulating long-timescale phenomena (microseconds to seconds) remains computationally intractable, not because force calculation is slow, but because the integration process is inherently serial and granular.</p>

<p>To escape the femtosecond prison, we cannot simply accelerate the integrator. We must <strong>bypass it entirely</strong>—replacing step-by-step integration with direct trajectory prediction.</p>

<p>Now that we’ve solved force calculation, how do we escape the femtosecond prison?</p>

<hr />

<h1 id="flashmd-escaping-the-femtosecond-prison">FlashMD: Escaping the Femtosecond Prison</h1>

<p>MLIPs solved the force bottleneck—but they left the integration bottleneck untouched. No matter how fast we evaluate $V(\mathbf{Q})$, we are still chained to femtosecond steps by the stability requirements of Velocity Verlet.</p>

<p>FlashMD<d-cite key="bigi2025flashmd"></d-cite> proposes a radical alternative: <strong>bypass both force calculation and numerical integration entirely</strong>. Instead of learning a potential energy surface and feeding its gradients into a classical integrator, FlashMD learns the <strong>dynamical map</strong> itself—a neural network that directly predicts how the system evolves over a large time interval:</p>

\[\mathcal{G}_\theta: (\mathbf{Q}_t, \mathbf{P}_t) \longrightarrow (\mathbf{Q}_{t+\Delta t}, \mathbf{P}_{t+\Delta t})\]

<p>A single forward pass through $\mathcal{G}_\theta$ replaces hundreds of sequential Velocity Verlet steps. This enables prediction strides of <strong>1–2 orders of magnitude</strong> beyond the stability limit of classical integrators—turning trillions of steps into millions.</p>

<p>But this paradigm shift comes at a cost. Classical integrators are backed by mathematical guarantees: symplecticity, time-reversibility, and approximate energy conservation. A learned dynamical map offers none of these. Whether the resulting trajectories remain physically meaningful is an open question—one we will investigate directly in our <a href="#an-exploratory-study-on-failure-modes">exploratory study</a>.</p>

<p>First, let us understand how FlashMD is built.</p>

<p>XXX Here Illustration of Classical MD Loop vs. FlashMD Loop</p>

<h2 id="architecture-and-design-principles">Architecture and Design Principles</h2>

<p>FlashMD is designed as a modular pipeline with three stages: input embedding, a graph neural network backbone, and multi-head output prediction. This modularity is deliberate—the backbone can be swapped for any sufficiently expressive GNN, as long as it respects the symmetries of physics.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/flashmd_architecture-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/flashmd_architecture.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 2: The FlashMD architecture taken from the main paper. Atomic positions and momenta are embedded into a molecular graph, processed by a GNN backbone (here: PET), and decoded into displacement and momentum predictions via separate MLP heads.</figcaption>
  
</figure>

<p><strong>Stage 1: Input Embedding.</strong>
The current positions $\mathbf{Q}$ and momenta $\mathbf{P}$ are encoded into node and edge features of a molecular graph. A critical preprocessing step is <strong>mass scaling</strong>: input momenta are normalized as $\tilde{\mathbf{p}}_i = \mathbf{p}_i / \sqrt{m_i}$. Without this, heavy atoms (e.g., gold at 197 amu) would dominate the training loss by a factor of $\sim 200^2$ compared to hydrogen (1 amu), causing the model to neglect fast hydrogen vibrations entirely.</p>

<p><strong>Stage 2: GNN Backbone.</strong>
The molecular graph is processed by a message-passing neural network that extracts local geometric features and propagates information between neighboring atoms. The default choice is the Point-Edge Transformer (PET)<d-cite key="PET_pozdnyakov2023smooth"></d-cite>, but any architecture that can operate on atomic graphs is a valid candidate—provided it handles a fundamental physical constraint we discuss next.</p>

<p><strong>Stage 3: Multi-Head Output.</strong>
Two separate MLP heads decode the final node representations into predictions:</p>
<ul>
  <li><strong>Displacement head:</strong> predicts $\Delta \mathbf{q}_i = \mathbf{q}_i(t+\Delta t) - \mathbf{q}_i(t)$</li>
  <li><strong>Momentum head:</strong> predicts $\mathbf{p}_i(t+\Delta t)$</li>
</ul>

<p>At inference time, optional post-processing filters can be applied: momentum rescaling for energy conservation, thermostat/barostat coupling for ensemble control, and random rotations to enforce symmetry—a point we return to shortly.</p>

<h2 id="the-non-negotiable-constraint-e3-equivariance">The Non-Negotiable Constraint: E(3) Equivariance</h2>

<p>Regardless of which GNN backbone we choose, one physical requirement is absolute: the model must respect the symmetries of Euclidean space.</p>

<p>Consider a water molecule. If we rotate the entire simulation box by 90°, the physics does not change. Energies remain identical, and force vectors rotate by exactly the same 90° to follow the atoms. Formally, for any rotation matrix $\mathcal{R}$, a physically valid model $\mathcal{F}$ must satisfy:</p>

\[\mathcal{F}(\mathcal{R} \cdot \mathbf{Q}) = \mathcal{R} \cdot \mathcal{F}(\mathbf{Q})\]

<p>This property—<strong>E(3) equivariance</strong>—is trivially satisfied by classical force fields (which are derived from physics) but is <em>not</em> automatic for neural networks. A standard MLP sees atomic coordinates as plain numbers; it has no notion that a rotated molecule represents the same physical system. If equivariance is violated, the model might predict that a molecule flies apart simply because it was oriented “North” instead of “East.”</p>

<p>Two fundamentally different strategies exist to address this:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Examples</th>
      <th>Mechanism</th>
      <th>Trade-off</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Hard constraints</strong></td>
      <td>NequIP<d-cite key="NequIP_Batzner2022"></d-cite>, MACE<d-cite key="MACE_ALLEGRO_leimeroth2025machine"></d-cite></td>
      <td>Spherical harmonics baked into network layers</td>
      <td>Exact equivariance, but computationally expensive</td>
    </tr>
    <tr>
      <td><strong>Soft constraints</strong></td>
      <td>SchNet<d-cite key="schutt2017schnet"></d-cite>, PET<d-cite key="PET_pozdnyakov2023smooth"></d-cite></td>
      <td>Flexible architecture + data augmentation</td>
      <td>Fast and expressive, but equivariance is approximate</td>
    </tr>
  </tbody>
</table>

<p>Which strategy is preferable depends on the application. Hard-constrained models provide mathematical guarantees and are ideal when exactness matters (e.g., computing free energy differences). Soft-constrained models trade guaranteed exactness for speed and architectural flexibility—an attractive bargain when the goal is long-timescale dynamics with large strides.</p>

<p>FlashMD adopts the soft-constraint approach, using the Point-Edge Transformer as its default backbone.</p>

<h2 id="the-point-edge-transformer-pet">The Point-Edge Transformer (PET)</h2>

<p>The Point-Edge Transformer (PET)<d-cite key="PET_pozdnyakov2023smooth"></d-cite> is FlashMD’s default backbone—a rotationally unconstrained, Transformer-based graph neural network that trades exact symmetry for raw expressivity.</p>

<p>In a standard message-passing GNN, each atom aggregates information from its neighbors into a single summary vector—discarding which neighbor contributed what. PET instead preserves the identity of every interaction by maintaining a separate feature vector $\mathbf{f}_{ij}^{(l)}$ for each directed bond between atoms $i$ and $j$ within a cutoff radius $R_c$. These per-bond representations are the fundamental currency of the architecture.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/pet_illustration-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/pet_illustration-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/pet_illustration-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/pet_illustration.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: The PET message-passing mechanism. For each central atom, neighbor interactions are encoded as distinct tokens and processed by Transformer self-attention, enabling many-body correlations without explicit angular descriptors.</figcaption>
  
</figure>

<p><strong>How it works, step by step:</strong></p>

<ol>
  <li>
    <p><strong>Tokenization.</strong> For each central atom $i$, every neighbor $j$ within $R_c$ is encoded into a distinct token. Each token fuses geometric information (the relative displacement $\mathbf{q}_j - \mathbf{q}_i$) with chemical identity (atomic species of $i$ and $j$). The result is a set of neighbor tokens—one per bond—that the model can reason over individually.</p>
  </li>
  <li>
    <p><strong>Transformer self-attention.</strong> These tokens are fed into a standard Transformer self-attention layer. This is where PET’s power lies: attention allows the model to learn that the <em>combination</em> of neighbors matters, not just each neighbor in isolation. For example, it can discover that a third oxygen atom nearby weakens a hydrogen bond between two water molecules—a genuine <strong>many-body effect</strong> that emerges from attention weights, without any hand-crafted angular descriptor.</p>
  </li>
  <li>
    <p><strong>Message update.</strong> The Transformer outputs are reinterpreted as updated bond messages $\mathbf{f}_{ij}^{(l+1)}$, which become the inputs for the next message-passing layer. After $L$ layers, the accumulated per-atom representations are passed through a feed-forward network to predict the target property (in FlashMD’s case: displacements and momenta).</p>
  </li>
</ol>

<p><strong>Why this matters for FlashMD.</strong> The unconstrained design gives PET a remarkable theoretical property: even a single message-passing layer acts as a <strong>universal approximator</strong> with virtually unlimited body order and angular resolution<d-cite key="PET_pozdnyakov2023smooth"></d-cite>. This means PET can, in principle, represent arbitrarily complex atomic interactions—exactly the kind of expressivity needed to learn a dynamical map that replaces hundreds of Velocity Verlet steps.</p>

<p>The price is clear: PET imposes <strong>no explicit rotational symmetry constraints</strong>. It must learn equivariance from data rather than guaranteeing it by construction. This is where the runtime symmetrization described below becomes essential.</p>

<h3 id="enforcing-symmetry-at-runtime">Enforcing Symmetry at Runtime</h3>
<p>Since the PET backbone is not intrinsically equivariant, FlashMD must learn physical symmetries from the data itself.</p>

<p>To achieve this, the authors employ Data Augmentation. During training, every molecular configuration is randomly rotated before being fed into the model. This forces the network to learn that the physics of a molecule is independent of its orientation in space.</p>

<p>While this does not provide the strict mathematical guarantees of equivariant architectures (like NequIP or MACE), it allows FlashMD to retain the raw expressivity of the PET backbone. For the bulk systems studied in the paper, this approximate equivariance proves sufficient: the model recovers correct radial distribution functions and diffusion coefficients without needing expensive geometric algebra operations.</p>

<p>However, as we will see in our <a href="#an-exploratory-study-on-failure-modes">exploratory study</a>, relying on the model to “learn” symmetry rather than enforcing it by construction may have consequences when operating in strict energy-conserving ensembles.</p>

<h2 id="long-stride-predictions-in-practice">Long-Stride Predictions in Practice</h2>

<p>With the architecture in place, the central question is: <strong>does it actually work?</strong> Can FlashMD simulate realistic molecular dynamics with dramatically larger time steps—without losing the physics that matters?</p>

<h3 id="benchmarking-strategy">Benchmarking Strategy</h3>

<p>Evaluating a learned dynamics model is subtle. Molecular dynamics is <strong>chaotic</strong>: two simulations starting from nearly identical states will diverge exponentially within picoseconds, even if both are perfectly correct. Comparing trajectories point-by-point is therefore meaningless.</p>

<p>Instead, the authors adopt a <strong>statistical evaluation</strong>: generate reference trajectories with a trusted conventional force field (PET-MAD), run FlashMD under identical conditions, and compare <strong>ensemble-averaged properties</strong>—density, radial distribution functions, phase transition temperatures, and diffusion coefficients. If these statistical fingerprints match, the dynamics are physically meaningful, regardless of whether individual trajectories agree.</p>

<p>FlashMD is evaluated in two configurations: a <strong>water-specific model</strong> (trained exclusively on liquid water, optimized for maximum accuracy) and a <strong>universal model</strong> (trained on chemically diverse systems, designed for broad generalization). This lets the authors probe both extremes of the accuracy–generality trade-off.</p>

<h3 id="key-results">Key Results</h3>

<p><strong>Liquid Water.</strong>
Despite its apparent simplicity, water is notoriously difficult to simulate due to its fluctuating hydrogen-bond network. At 450 K, the water-specific model reproduces radial distribution functions—the spatial arrangement of oxygen and hydrogen atoms—nearly perfectly. Temperature control via a Langevin thermostat maintains deviations under 1 K. Strides of up to <strong>16 fs</strong> are stable, representing a <strong>64× speedup</strong> over the 0.25 fs baseline typically required for rigid water models (see <a href="#fig:water">Figure XX</a>).</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/water_experiment_flashmd-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/water_experiment_flashmd-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/water_experiment_flashmd-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/water_experiment_flashmd.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: Water Experiments FlashMD</figcaption>
  
</figure>

<p><strong>Solvated Alanine Dipeptide.</strong>
This small peptide in water serves as a minimal proxy for protein flexibility. The critical benchmark is the <strong>Ramachandran plot</strong> (<a href="#fig:universal">Figure XX</a> a), which maps the accessible backbone conformations. FlashMD recovers the correct distribution of dihedral angles even at strides <strong>32× larger</strong> than standard MD—strong evidence that it captures meaningful conformational dynamics, not merely static snapshots.</p>

<p><strong>Aluminum Surface Pre-melting.</strong>
Metal surfaces exhibit subtle, layer-dependent dynamics at high temperatures: surface atoms become mobile before the bulk melts. FlashMD reproduces the characteristic <strong>anisotropic softening pattern</strong> (different vibration amplitudes along different crystal axes) and captures the formation and migration of dynamic surface defects—all at <strong>64 fs strides</strong> (64× speedup) (<a href="#fig:universal">Figure XX</a> b).</p>

<p><strong>Lithium Thiophosphate: Superionic Transition.</strong>
Perhaps the most striking result involves a solid-state battery electrolyte. At elevated temperatures, lithium ions become highly mobile in a “superionic” phase—a collective transition critical for battery performance (<a href="#fig:universal">Figure XX</a> c). FlashMD predicts the transition temperature at 675 K (within the expected range) and reproduces the dramatic jump in lithium conductivity, achieving an <strong>8× speedup</strong>. Some systematic errors appear at extreme temperatures, but the qualitative physics is captured.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/universal_experiments_flashmd-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/universal_experiments_flashmd-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/universal_experiments_flashmd-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/universal_experiments_flashmd.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: Universal Experiments FlashMD</figcaption>
  
</figure>

<h3 id="what-these-results-show">What These Results Show</h3>

<p>Across liquid, biomolecular, metallic, and ionic systems, FlashMD consistently recovers the correct statistical physics while operating at strides far beyond classical stability limits. The speedups range from 8× to 64× depending on the system, with the universal model showing remarkable transferability across chemically distinct materials.</p>

<p>But statistical agreement with reference simulations is not the whole story. These benchmarks were conducted with thermostats and barostats that actively regulate temperature and pressure—external controls that can mask underlying issues with the learned dynamics.</p>

<p><strong>What happens when we remove these safety nets?</strong></p>

<p>In the next section, we strip away the thermostats and run FlashMD in the most unforgiving setting: the microcanonical (NVE) ensemble, where energy must be conserved by the dynamics alone. This reveals fundamental limitations of the learned approach—and points toward what must be solved before FlashMD can be trusted as a general-purpose simulator.</p>

<h1 id="an-exploratory-study-on-failure-modes">An Exploratory Study on Failure Modes</h1>

<p>To move beyond theoretical concerns, we conduct a systematic exploratory study on a concrete system: a periodic box of 258 TIP3P water molecules simulated with OpenMM as ground truth. We then trained FlashMD models under a variety of settings and asked a blunt question: can they conserve energy in NVE rollouts—the most unforgiving test of physical correctness?</p>

<h2 id="experimental-setup">Experimental Setup</h2>

<p><strong>Ground Truth Generation.</strong> We generated short NVE trajectories across a wide temperature range (200–700 K) using the TIP3P force field in OpenMM, saving configurations every 0.5 fs over 10 ps.</p>

<p><strong>Training Data.</strong> From these trajectories, we build training pairs of the form $(q_t, p_t) \rightarrow (q_{t+1}, p_{t+1})$, corresponding to a prediction stride of 1 fs.</p>

<p><strong>Evaluation.</strong> Each trained model is rolled out for 50 ps in an NVE simulation, starting from a 300 K equilibrated configuration.</p>

<p>Before tackle the ablation studies, we first asked a profound question: do the trained models even behave sensibly?</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/comparison_detailed_analysis-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/comparison_detailed_analysis-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/comparison_detailed_analysis-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/comparison_detailed_analysis.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 2: Comparison of temperature and energy stability across various model setups against the OpenMM ground truth.</figcaption>
  
</figure>

<p>The first results are shown in <a href="#fig:basecomparison">Figure 2</a>. Across different random seeds and different starting configurations, the picture is consistent. All trained models produce trajectories that look stable—no immediate explosions, no obvious numerical blow-ups.</p>

<p>But none of them conserve energy. Total energy drifts steadily, and temperature drops far below the target value. In other words, the models are quietly bleeding kinetic energy.</p>

<p>To understand where this energy is going, we looked at the momentum distributions predicted by the models and compared them to ground truth. Since kinetic energy is directly determined by momenta, any systematic bias here would immediately explain the observed cooling.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/velocity_momentum_species_grid-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/velocity_momentum_species_grid-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/velocity_momentum_species_grid-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/velocity_momentum_species_grid.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 3: Momentum distribution</figcaption>
  
</figure>

<p>The picture is surprisingly consistent across models (<a href="#fig:base_mom_dist">Figure 3</a>). Hydrogen velocities look roughly reasonable, but oxygen momenta are systematically shifted toward lower values compared to ground truth. Since oxygen atoms carry most of the system’s kinetic energy, even a small bias here translates into a large temperature drop.</p>

<p>This leads us to the question: can we fix this with better loss design?</p>

<p>We tried two straightforward ideas:</p>

<ol>
  <li>Change how strongly the model is penalized for momentum errors relative to position errors.</li>
  <li>Change how momentum errors are measured, so hydrogen and oxygen atoms are treated more equally.</li>
</ol>

<h2 id="ablation-1-loss-weighting-between-positions-and-momenta">Ablation 1: Loss Weighting Between Positions and Momenta</h2>

<p>The first question is simple: does it matter how much the model cares about getting momenta right vs. positions? We train models with momentum loss weights $w_p \in {0.5, 1.0, 1.5, 2.0, 10.0}$ while keeping the position weight fixed at $w_q = 1.0$, and let them run NVE for 10 ps.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/nve_comparison_4models_10ps_temp_energy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectories for three momentum loss weights and OpenMM ground truth. Top: temperature evolution. Bottom: total energy. The ground truth (black) is essentially flat — none of the models come close.</figcaption>
  
</figure>

<p>What we found is that changing the weight mostly changes how the model fails:</p>
<ul>
  <li>With too little momentum weight, models become unstable and quickly blow up.</li>
  <li>With moderate weighting, models remain numerically stable but steadily cool.</li>
  <li>With very large weighting, temperatures look closer to correct early on, but fluctuations become large and the system eventually starts drifting again.</li>
</ul>

<p>We also notice something interesting at the species level: when we crank the momentum weight up to $w_p = 10$, the oxygen velocity and momentum distributions move noticeably closer to the ground truth, while hydrogen barely changes (see <a href="#fig:mom10">Figure 6</a>). That hints at a mass-dependent effect in how momentum errors show up. Still, the big picture doesn’t change — energy continues to drift, which reinforces that loss reweighting alone isn’t enough to recover physically consistent dynamics.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/mom_10_velocity_momentum_species_grid-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/mom_10_velocity_momentum_species_grid-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/mom_10_velocity_momentum_species_grid-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/mom_10_velocity_momentum_species_grid.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 6: Species-wise velocity and momentum distributions under large momentum loss weighting ($w_p=10$). Oxygen improves; hydrogen does not.</figcaption>
  
</figure>

<p>In other words, loss weighting shifts the failure mode—from exploding, to freezing, to noisy drifting—but it never produces true energy conservation. None of the settings come close to the flat energy profile of the ground-truth simulation.</p>

<p>So cranking the momentum weight alone clearly isn’t the answer. We don’t actually know what the <em>“right”</em> fix is yet — but the FlashMD paper points to one reasonable idea: change how momentum errors are represented by taking atomic masses into account. Naturally, we tried that next.</p>

<h2 id="ablation-2-mass-scaled-loss-functions">Ablation 2: Mass-Scaled Loss Functions</h2>
<p>With a standard MSE on momenta, all atoms are treated equally — even though their masses differ drastically. Because $p_i = m_i v_i$, oxygen (mass 16) contributes about $16^2 = 256$ times more to the momentum loss than hydrogen. In principle, this should bias training toward oxygen. Yet we actually observe the opposite: oxygen velocities and momenta are poorly reproduced. This hints that the problem isn’t just weighting, but how momenta are represented in the loss in the first place.</p>

<p>In other words, even when oxygen dominates the loss numerically, the model still fails to capture its dynamics correctly — highlighting the need for a mass-aware reformulation. Inspired by the FlashMD paper, we address this by implementing a mass-scaled loss:</p>

\[\mathcal{L}_{\tilde{p}} = \frac{1}{N} \sum_i \frac{\|p_i^\text{pred} - p_i^\text{true}\|^2}{m_i}, \qquad \mathcal{L}_{\Delta\tilde{q}} = \frac{1}{N} \sum_i \|\Delta q_i^\text{pred} - \Delta q_i^\text{true}\|^2 \cdot m_i\]

<p>This is equivalent to computing MSE on the mass-scaled quantities $\tilde{p} = p/\sqrt{m}$ and $\Delta\tilde{q} = \Delta q \cdot \sqrt{m}$, ensuring that velocity errors are weighted equally regardless of atomic mass.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-480.webp 480w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-800.webp 800w,/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/2026/assets/img/2026-04-27-flashmd/mass_scaling_nve_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure X: NVE trajectory comparison between standard MSE and mass-scaled MSE loss. Left column shows the initial drift phase (0–10 ps), right column the full 50 ps trajectory. Both models exhibit systematic cooling relative to the ground truth.</figcaption>
  
</figure>

<p>The fact that mass-scaling helps <em>early</em> but not <em>late</em> suggests that it improves the model’s initial momentum predictions (reducing the per-species bias), but the accumulated errors from the autoregressive rollout eventually dominate regardless. The fundamental issue is that FlashMD’s single-step prediction errors, while small individually, compound systematically rather than canceling stochastically—a hallmark of non-symplectic integration.</p>

<h2 id="the-takeaway">The Takeaway</h2>
<p>Whether we reweight momenta, mass-scale the loss, or tune coefficients, FlashMD still exhibits systematic energy drift in NVE. The problem is not simply how much the model cares about momenta, nor how errors are weighted across atoms.</p>

<p>The deeper issue is structural: FlashMD performs unconstrained autoregressive prediction with no mechanism enforcing conservation laws. Small per-step biases accumulate in the same direction rather than canceling out. Over thousands of steps, that inevitably turns into macroscopic energy drift.</p>

<hr />

<h1 id="summary">Summary</h1>
<p>We have seen that FlashMD introduces a fundamentally new way to think about MD simulation: bypassing both force evaluation and numerical integration by directly learning the evolution of the system. In doing so, it addresses the two central bottlenecks of classical MD—the cost of force calculations and the femtosecond timestep stability limit—and demonstrates impressive speedups.</p>

<p>Our independent exploratory study reveals a clear limitation. When external controls such as thermostats are removed and the model is asked to operate in the microcanonical (NVE) ensemble, FlashMD does not conserve energy. This failure persists across loss reweighting and mass-scaled formulations, indicating a structural issue rather than a tuning problem.</p>

<p>None of this undermines the importance of the work. FlashMD is an impressive step toward longer-stride learned dynamics. But it does make one thing clear: what is still missing are the mathematical guarantees that classical integrators provide by construction.</p>

<p>The question we posed in the introduction—Can a model learn to respect the laws of physics without being explicitly taught to do so?—now has a nuanced answer. For statistical properties sampled under external control, yes. For the strict, unassisted conservation of energy that defines Hamiltonian dynamics, not yet.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Can a neural network learn to respect the laws of physics without being explicitly taught? We explore this question through FlashMD, a new framework that bypasses the timestep stability limit (XX femtosecond prison) of classical integrators to predict molecular evolution directly. This post guides you from the basics of MD bottlenecks to the cutting edge of learned dynamics. We conclude with an exclusive exploratory study revealing a hidden cost to this speed: when safety nets are removed, FlashMD struggles to conserve energy, highlighting the gap between statistical accuracy and physical validity.]]></summary></entry></feed>